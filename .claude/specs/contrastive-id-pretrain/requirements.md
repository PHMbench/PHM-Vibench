# 需求文档：对比学习ID预训练任务

## 概述

本规范定义了在 PHM-Vibench 中实现长工业信号对比学习预训练任务的需求，基于 ID_task 架构实现内存高效的时序数据处理。

## 业务背景

### 问题陈述
PHM-Vibench 当前缺乏针对长工业信号的对比学习预训练方法。现有预训练任务主要基于掩码重建，未充分利用长时序数据（8192-16384+ 采样点）内的时序依赖关系和信号关联性。

### 业务价值
- **研究推进**：为工业信号提供先进的对比学习预训练能力
- **性能提升**：下游故障诊断任务预期提升 5-15% F1 分数
- **内存效率**：相比全信号加载方法节省 50%+ 内存
- **框架兼容**：与现有 PHM-Vibench 生态系统无缝集成

## 利益相关者

### 主要用户
- **机器学习研究员**：进行工业故障诊断和预测性维护实验
- **数据科学家**：在工业环境中开发和部署故障诊断模型
- **PHM-Vibench 开发者**：维护和扩展基准测试平台

### 次要用户
- **学术研究者**：使用 PHM-Vibench 发表可复现的研究成果
- **工业工程师**：将预训练模型应用于实际设备监测

## 用户故事

### US-001：对比学习预训练执行
**作为** 机器学习研究员  
**我希望** 在长工业信号上训练对比学习模型  
**以便** 为下游故障诊断任务创建更好的特征表示  

**验收标准：**
- 当我配置对比ID预训练任务时
- 并且我指定数据集和模型参数
- 那么系统应该从同一信号ID生成正样本对
- 并且系统应该使用不同信号ID作为负样本
- 并且训练应该在100个epoch内使用InfoNCE损失收敛

### US-002：内存高效处理
**作为** 处理大型工业数据集的数据科学家  
**我希望** 无需将整个数据集加载到内存即可处理信号  
**以便** 训练超出可用RAM容量的数据集  

**验收标准：**
- 当我训练包含超过16K采样点的信号数据集时
- 那么系统应该通过H5DataDict使用延迟加载
- 并且内存使用相比全量加载减少至少50%
- 并且处理应该在不同信号长度间保持一致的吞吐量

### US-003：灵活的窗口采样
**作为** 研究时序模式的研究员  
**我希望** 配置不同的窗口策略来生成正样本  
**以便** 实验对比学习中的各种时序关系  

**验收标准：**
- 当我配置窗口采样策略为"random"时
- 那么系统应该从不同时间位置生成2个不重叠窗口
- 当我配置窗口大小和步长参数时
- 那么系统应该在窗口生成中遵守这些参数
- 并且窗口应该保持信号完整性和预处理一致性

### US-004：与现有架构集成
**作为** PHM-Vibench 开发者  
**我希望** 扩展BaseIDTask而不修改现有组件  
**以便** 新功能与当前框架无缝集成  

**验收标准：**
- 当我实现继承自BaseIDTask的ContrastiveIDTask时
- 那么它应该自动获得窗口化、预处理和延迟加载能力
- 并且ID_dataset应该无需修改即可支持对比学习
- 并且任务应该正确注册到工厂模式系统
- 并且现有管道配置应该保持功能正常

### US-005：配置驱动的实验
**作为** 进行系统实验的研究员  
**我希望** 通过YAML配置控制所有对比学习参数  
**以便** 高效地复现实验和进行参数扫描  

**验收标准：**
- 当我在配置中指定温度、学习率和批量大小时
- 那么这些参数应该在训练中正确应用
- 当我修改配置中的window_size或采样策略时
- 那么窗口行为应该相应改变
- 并且所有实验应该使用相同配置可复现

## 功能需求

### FR-001：核心对比学习实现
系统应实现用于对比学习的InfoNCE损失：
- 使用不同时间窗口从同一信号ID生成正样本对
- 使用每批中不同ID的信号作为负样本
- 在相似度计算前对特征向量应用L2归一化
- 支持可配置的温度参数用于相似度缩放

### FR-002：BaseIDTask集成
系统应扩展BaseIDTask以利用现有基础设施：
- 继承窗口化功能（create_windows方法）
- 利用现有预处理管道（process_sample方法）
- 通过H5DataDict集成保持延迟加载
- 保留所有数据验证和错误处理机制

### FR-003：批处理准备逻辑
系统应为对比学习实现高效的批处理准备：
- 在每批中并行处理多个信号ID
- 为正样本对每个信号ID生成正好2个窗口
- 通过填充或截断处理可变长度信号
- 为GPU处理保持一致的张量形状

### FR-004：训练循环集成
系统应与PyTorch Lightning训练框架集成：
- 实现用于训练、验证和测试的_shared_step方法
- 支持损失和准确率指标的自动记录
- 启用梯度裁剪和混合精度训练
- 提供适当的优化器和调度器配置

### FR-005：配置模式
系统应通过YAML支持全面配置：
- 数据参数：batch_size、window_size、num_window、sampling_strategy
- 模型参数：backbone选择、嵌入维度
- 任务参数：learning_rate、temperature、weight_decay
- 训练参数：epochs、gradient_clipping、early_stopping

## 非功能需求

### NFR-001：性能需求
- **内存效率**：相比全信号加载减少50%+内存使用
- **训练速度**：CWRU数据集在单GPU上2小时内完成50个训练epoch
- **吞吐量**：每批至少处理32个样本而不内存溢出
- **可扩展性**：支持高达32K采样点长度的信号而不性能下降

### NFR-002：兼容性需求
- **Python版本**：兼容Python 3.8+
- **PyTorch版本**：兼容PyTorch 2.6.0
- **PHM-Vibench集成**：与现有工厂模式无缝集成
- **硬件**：支持CPU和GPU训练环境

### NFR-003：代码质量需求
- **可维护性**：遵循PHM-Vibench编码标准和模式
- **文档**：所有公共方法的完整文档字符串
- **测试**：单元和集成测试最低80%代码覆盖率
- **错误处理**：优雅处理边缘情况和无效输入

### NFR-004：可靠性需求
- **鲁棒性**：优雅处理损坏或不完整的信号数据
- **确定性**：使用固定随机种子产生一致结果
- **恢复**：尽管个别样本处理失败仍继续训练
- **验证**：全面的输入验证和早期错误检测

## 技术约束

### TC-001：架构约束
- 必须继承自BaseIDTask而不破坏现有功能
- 必须使用现有ID_dataset而不需要修改
- 必须使用工厂模式注册（@register_task装饰器）
- 必须遵循PyTorch Lightning模块结构

### TC-002：数据处理约束
- 必须支持1K到32K+采样点的可变长度信号
- 必须保持与H5DataDict延迟加载的兼容性
- 必须处理具有灵活通道数的多通道信号
- 必须在不同采样策略间保持数据预处理一致性

### TC-003：内存约束
- 必须在典型GPU内存限制（8-16GB）内运行
- 必须避免同时将整个数据集加载到内存
- 必须使用高效的张量操作以最小化内存开销
- 必须支持梯度累积以实现有效的大批量训练

### TC-004：集成约束
- 必须与现有PHM-Vibench管道工作（Pipeline_01、Pipeline_02、Pipeline_ID）
- 必须保持与当前模型架构的兼容性（ISFM、PatchTST）
- 必须支持现有训练器配置和回调
- 必须与当前日志和实验跟踪系统集成

## 成功标准

### 验收测试
1. **功能验证**：所有用户故事通过验收标准测试
2. **性能基准**：内存使用和训练时间达到指定目标
3. **集成测试**：在现有PHM-Vibench工作流中成功运行
4. **可重现性**：使用固定种子在多次运行中获得相同结果

### 质量关卡
1. **代码审查**：由PHM-Vibench核心开发者批准
2. **测试覆盖**：综合测试套件最低80%覆盖率
3. **文档**：完整的API文档和使用示例
4. **性能**：达到或超过指定的内存和速度要求

### 完成定义
- [ ] ContrastiveIDTask实现完成并测试
- [ ] 配置文件创建并验证
- [ ] 与BaseIDTask的集成验证
- [ ] 达到内存效率目标
- [ ] 满足性能基准
- [ ] 文档完成
- [ ] 所有测试通过
- [ ] 代码审查批准

## 假设和依赖

### 假设
- BaseIDTask提供稳定的窗口化和预处理API
- ID_dataset保持一致的数据格式和元数据结构
- H5DataDict继续支持大信号的延迟加载
- 现有模型架构（ISFM、PatchTST）保持兼容

### 依赖
- PyTorch 2.6.0 带CUDA支持
- PyTorch Lightning训练框架
- H5py用于高效数据存储和检索
- NumPy用于数值计算
- 现有PHM-Vibench工厂和配置系统

## 风险和缓解

### Risk-001：正样本质量
**风险**：随机窗口可能生成非常相似或相同的窗口  
**影响**：中等 - 可能降低对比学习效果  
**缓解**：实现窗口位置之间的最小距离约束

### Risk-002：负样本不平衡
**风险**：批内负样本有限影响对比学习  
**影响**：中等 - 小批量大小可能影响学习质量  
**缓解**：支持批量大小调整和潜在的跨批负样本采样

### Risk-003：内存使用扩展
**风险**：非常长的信号或大批量仍可能导致内存增长  
**影响**：高 - 可能阻止在资源受限系统上训练  
**缓解**：实现动态批量大小和梯度累积策略

### Risk-004：集成复杂性
**风险**：BaseIDTask修改可能破坏兼容性  
**影响**：低 - 明确定义的继承策略最小化风险  
**缓解**：全面测试和对基础功能的最小更改