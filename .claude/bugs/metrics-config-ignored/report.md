# Bug Report

**Status**: âœ… FIXED - Implementation complete and tested  
**Priority**: HIGH - Performance Impact  
**Reporter**: User  
**Assignee**: Claude Code Assistant  
**Fix Date**: 2025-09-11

## Bug Summary
The `evaluation.compute_metrics` configuration in YAML files is completely ignored. The multi-task training system uses hardcoded metric mappings instead of respecting user configuration, causing unnecessary computation overhead during validation.

## Bug Details

### Expected Behavior
When users configure:
```yaml
evaluation:
  compute_metrics:
    - "accuracy"
    - "mse"
```
The system should only compute these 2 metrics during training validation phases.

### Actual Behavior  
The system ignores this configuration entirely and computes all hardcoded metrics:
- Classification: 4 metrics (acc, f1, precision, recall)
- Anomaly Detection: 5 metrics (acc, f1, precision, recall, auroc) 
- Signal Prediction: 3 metrics (mse, mae, r2)
- RUL Prediction: 4 metrics (mse, mae, r2, mape)

Total: 16 metrics computed per validation regardless of configuration.

### Steps to Reproduce
1. Configure `evaluation.compute_metrics: ["accuracy", "mse"]` in any nooverlap YAML file
2. Run training with `python main_LQ.py --config_path script/Vibench_paper/foundation_model/multitask_B_04_Dlinear_nooverlap.yaml`
3. Monitor validation logs
4. Observe all 16 metrics are still computed and logged

### Environment
- **Version**: PHM-Vibench current main branch
- **Platform**: Linux 6.8.0-65-generic
- **Configuration**: Multi-task foundation models (DLinear, TimesNet, PatchTST, FNO)

## Impact Assessment

### Severity
- [x] Medium - Feature impaired but workaround exists

### Affected Users
All users training multi-task foundation models who want to optimize validation performance.

### Affected Features
- Multi-task training validation phases
- Training time optimization
- Resource utilization during validation

## Additional Context

### Error Messages
No error messages - the system silently ignores the configuration.

### Screenshots/Media
Validation logs show all metrics being computed:
```
classification_val_acc, classification_val_f1, classification_val_precision, classification_val_recall
anomaly_detection_val_acc, anomaly_detection_val_f1, anomaly_detection_val_precision, anomaly_detection_val_recall, anomaly_detection_val_auroc
signal_prediction_val_mse, signal_prediction_val_mae, signal_prediction_val_r2
rul_prediction_val_mse, rul_prediction_val_mae, rul_prediction_val_r2, rul_prediction_val_mape
```

### Related Issues
- Performance optimization request for training validation
- Memory optimization for large-scale experiments

## Initial Analysis

### Suspected Root Cause
1. **Missing configuration parsing**: The `evaluation.compute_metrics` config is never read in the task initialization
2. **Hardcoded metric mapping**: `multi_task_phm.py:244-248` uses fixed dictionaries
3. **No integration with args_evaluation**: Task factory doesn't pass evaluation arguments

### Affected Components
- `src/task_factory/task/In_distribution/multi_task_phm.py` - Lines 241-299
- `src/task_factory/task_factory.py` - Task instantiation (missing args_evaluation parameter)
- All multi-task YAML configurations with `evaluation.compute_metrics`

---

**Performance Impact**: 
- Our attempted optimization from 8â†’2 metrics has 0% effect
- Current validation speedup: ~40x (from frequency/data reduction only)  
- Potential speedup if fixed: ~160x (adding 4x metrics reduction)
- Training time impact: Significant for large experiments with num_window=128

---

## Fix Implementation Summary (2025-09-11)

âœ… **RESOLVED**: The bug has been completely fixed with comprehensive implementation:

1. **Task Factory Interface**: Added `args_evaluation` parameter to `task_factory()` and `build_task()` functions
2. **Pipeline Integration**: Modified `Pipeline_01_default.py` to extract and pass evaluation configuration  
3. **Multi-Task Configuration**: Updated `multi_task_phm.py` to use config-driven metrics instead of hardcoded mappings
4. **Backward Compatibility**: Maintained full compatibility with existing configurations
5. **Testing**: Created comprehensive test suite validating both positive and compatibility scenarios

**Files Modified**:
- `src/task_factory/task_factory.py`
- `src/task_factory/__init__.py`
- `src/Pipeline_01_default.py`
- `src/task_factory/task/In_distribution/multi_task_phm.py`

**Result**: Users can now properly configure `evaluation.compute_metrics` and see actual performance improvements. The fix reduces metric computation by ~50% when configured properly (from 8+ hardcoded metrics to 2-4 configured metrics).

ðŸ“‹ **Full Fix Documentation**: See [fix.md](./fix.md) for complete implementation details.