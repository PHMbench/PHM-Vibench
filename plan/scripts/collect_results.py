#!/usr/bin/env python3
"""
ç»“æœæ”¶é›†å’Œæ±‡æ€»è„šæœ¬
è‡ªåŠ¨æ”¶é›†æ‰€æœ‰Flowå®éªŒç»“æœå¹¶ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
"""

import pandas as pd
import json
import numpy as np
from pathlib import Path
from datetime import datetime
import argparse

def collect_experiment_results(experiment_dir="results/", output_prefix="experiment"):
    """æ±‡æ€»æ‰€æœ‰å®éªŒç»“æœ"""
    results = []
    
    print(f"ğŸ“Š æ”¶é›†å®éªŒç»“æœä»: {experiment_dir}")
    
    for exp_path in Path(experiment_dir).glob("*/"):
        if exp_path.is_dir():
            print(f"  æ£€æŸ¥: {exp_path.name}")
            
            # æŸ¥æ‰¾æŒ‡æ ‡æ–‡ä»¶
            metrics_files = [
                exp_path / "metrics.json",
                exp_path / "lightning_logs" / "version_0" / "metrics.csv",
                exp_path / "results.json"
            ]
            
            experiment_data = {
                'experiment': exp_path.name,
                'path': str(exp_path),
                'timestamp': exp_path.stat().st_mtime
            }
            
            # å°è¯•ä»ä¸åŒæ–‡ä»¶åŠ è½½æŒ‡æ ‡
            metrics_loaded = False
            
            for metrics_file in metrics_files:
                if metrics_file.exists():
                    try:
                        if metrics_file.suffix == '.json':
                            with open(metrics_file) as f:
                                metrics = json.load(f)
                            experiment_data.update(metrics)
                            metrics_loaded = True
                            break
                            
                        elif metrics_file.suffix == '.csv':
                            # PyTorch Lightning CSVæ ¼å¼
                            df = pd.read_csv(metrics_file)
                            if not df.empty:
                                # å–æœ€åä¸€ä¸ªepochçš„æŒ‡æ ‡
                                last_metrics = df.iloc[-1].to_dict()
                                # æ¸…ç†æŒ‡æ ‡åç§°
                                clean_metrics = {}
                                for k, v in last_metrics.items():
                                    if not pd.isna(v) and k not in ['epoch', 'step']:
                                        clean_metrics[k] = v
                                experiment_data.update(clean_metrics)
                                metrics_loaded = True
                                break
                                
                    except Exception as e:
                        print(f"    è­¦å‘Š: æ— æ³•è¯»å– {metrics_file}: {e}")
                        continue
            
            if not metrics_loaded:
                print(f"    è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„æŒ‡æ ‡æ–‡ä»¶ for {exp_path.name}")
            
            # æ·»åŠ é»˜è®¤å€¼
            for key in ['accuracy', 'test_accuracy', 'val_accuracy']:
                if key not in experiment_data:
                    experiment_data[key] = None
            
            for key in ['f1_score', 'test_f1', 'val_f1']:
                if key not in experiment_data:
                    experiment_data[key] = None
                    
            for key in ['training_time', 'model_params']:
                if key not in experiment_data:
                    experiment_data[key] = None
            
            results.append(experiment_data)
    
    if not results:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å®éªŒç»“æœ!")
        return None
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(results)
    
    # æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–
    df = clean_and_standardize_results(df)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_file = f"{output_prefix}_results_{timestamp}.csv"
    excel_file = f"{output_prefix}_results_{timestamp}.xlsx"
    
    df.to_csv(csv_file, index=False)
    df.to_excel(excel_file, index=False)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜:")
    print(f"   CSV: {csv_file}")
    print(f"   Excel: {excel_file}")
    
    # æ‰“å°æ‘˜è¦ç»Ÿè®¡
    print_summary_statistics(df)
    
    return df

def clean_and_standardize_results(df):
    """æ¸…ç†å’Œæ ‡å‡†åŒ–ç»“æœæ•°æ®"""
    
    # æå–å®éªŒç±»å‹
    df['experiment_type'] = df['experiment'].apply(extract_experiment_type)
    
    # æ ‡å‡†åŒ–å‡†ç¡®ç‡åˆ—
    accuracy_cols = ['accuracy', 'test_accuracy', 'val_accuracy', 'test_acc', 'val_acc']
    df['final_accuracy'] = None
    
    for _, row in df.iterrows():
        for col in accuracy_cols:
            if col in row and row[col] is not None and not pd.isna(row[col]):
                df.loc[df['experiment'] == row['experiment'], 'final_accuracy'] = row[col]
                break
    
    # æ ‡å‡†åŒ–F1åˆ†æ•°åˆ—
    f1_cols = ['f1_score', 'test_f1', 'val_f1', 'f1', 'test_f1_score', 'val_f1_score']
    df['final_f1'] = None
    
    for _, row in df.iterrows():
        for col in f1_cols:
            if col in row and row[col] is not None and not pd.isna(row[col]):
                df.loc[df['experiment'] == row['experiment'], 'final_f1'] = row[col]
                break
    
    # è½¬æ¢æ—¶é—´æˆ³ä¸ºå¯è¯»æ ¼å¼
    if 'timestamp' in df.columns:
        df['date_created'] = pd.to_datetime(df['timestamp'], unit='s').dt.strftime('%Y-%m-%d %H:%M')
    
    # æŒ‰æ—¶é—´æˆ³æ’åº
    if 'timestamp' in df.columns:
        df = df.sort_values('timestamp', ascending=False)
    
    return df

def extract_experiment_type(experiment_name):
    """ä»å®éªŒåç§°æå–å®éªŒç±»å‹"""
    name_lower = experiment_name.lower()
    
    if 'flow_quick' in name_lower or 'quick' in name_lower:
        return 'quick_validation'
    elif 'flow_baseline' in name_lower or 'baseline' in name_lower:
        return 'baseline'
    elif 'flow_contrastive' in name_lower or 'contrastive' in name_lower:
        return 'contrastive'
    elif 'flow_pipeline02' in name_lower or 'pipeline02' in name_lower:
        return 'pipeline02'
    elif 'flow_research' in name_lower or 'research' in name_lower:
        return 'research'
    elif 'ablation' in name_lower:
        return 'ablation'
    elif 'comparison' in name_lower:
        return 'comparison'
    else:
        return 'other'

def print_summary_statistics(df):
    """æ‰“å°æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯"""
    
    print("\nğŸ“ˆ å®éªŒç»“æœæ‘˜è¦:")
    print("=" * 50)
    
    print(f"æ€»å®éªŒæ•°: {len(df)}")
    
    if 'experiment_type' in df.columns:
        print(f"\nå®éªŒç±»å‹åˆ†å¸ƒ:")
        type_counts = df['experiment_type'].value_counts()
        for exp_type, count in type_counts.items():
            print(f"  {exp_type}: {count}")
    
    if 'final_accuracy' in df.columns and df['final_accuracy'].notna().sum() > 0:
        acc_stats = df['final_accuracy'].dropna()
        print(f"\nå‡†ç¡®ç‡ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {acc_stats.mean():.4f}")
        print(f"  æ ‡å‡†å·®: {acc_stats.std():.4f}")
        print(f"  æœ€å¤§å€¼: {acc_stats.max():.4f}")
        print(f"  æœ€å°å€¼: {acc_stats.min():.4f}")
        print(f"  ä¸­ä½æ•°: {acc_stats.median():.4f}")
    
    if 'final_f1' in df.columns and df['final_f1'].notna().sum() > 0:
        f1_stats = df['final_f1'].dropna()
        print(f"\nF1åˆ†æ•°ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {f1_stats.mean():.4f}")
        print(f"  æ ‡å‡†å·®: {f1_stats.std():.4f}")
        print(f"  æœ€å¤§å€¼: {f1_stats.max():.4f}")
        print(f"  æœ€å°å€¼: {f1_stats.min():.4f}")
    
    # æœ€ä½³æ€§èƒ½å®éªŒ
    if 'final_accuracy' in df.columns and df['final_accuracy'].notna().sum() > 0:
        best_acc_idx = df['final_accuracy'].idxmax()
        best_exp = df.loc[best_acc_idx]
        print(f"\nğŸ† æœ€ä½³å‡†ç¡®ç‡å®éªŒ:")
        print(f"  å®éªŒå: {best_exp['experiment']}")
        print(f"  å‡†ç¡®ç‡: {best_exp['final_accuracy']:.4f}")
        if 'final_f1' in best_exp and not pd.isna(best_exp['final_f1']):
            print(f"  F1åˆ†æ•°: {best_exp['final_f1']:.4f}")

def generate_latex_summary_table(df, output_file="results_table.tex"):
    """ç”ŸæˆLaTeXæ ¼å¼çš„ç»“æœæ±‡æ€»è¡¨"""
    
    # æŒ‰å®éªŒç±»å‹åˆ†ç»„
    if 'experiment_type' not in df.columns:
        print("è­¦å‘Š: æœªæ‰¾åˆ°å®éªŒç±»å‹ä¿¡æ¯ï¼Œè·³è¿‡LaTeXè¡¨æ ¼ç”Ÿæˆ")
        return
    
    # è®¡ç®—æ¯ç§å®éªŒç±»å‹çš„ç»Ÿè®¡ä¿¡æ¯
    summary_stats = []
    
    for exp_type in df['experiment_type'].unique():
        type_df = df[df['experiment_type'] == exp_type]
        
        acc_data = type_df['final_accuracy'].dropna()
        f1_data = type_df['final_f1'].dropna()
        
        if len(acc_data) > 0:
            acc_mean = acc_data.mean()
            acc_std = acc_data.std() if len(acc_data) > 1 else 0
        else:
            acc_mean = acc_std = None
            
        if len(f1_data) > 0:
            f1_mean = f1_data.mean()
            f1_std = f1_data.std() if len(f1_data) > 1 else 0
        else:
            f1_mean = f1_std = None
        
        summary_stats.append({
            'type': exp_type,
            'count': len(type_df),
            'acc_mean': acc_mean,
            'acc_std': acc_std,
            'f1_mean': f1_mean,
            'f1_std': f1_std
        })
    
    # ç”ŸæˆLaTeXè¡¨æ ¼
    latex_content = """\\begin{table}[h]
\\centering
\\caption{Flow Pretraining Experiment Results Summary}
\\label{tab:flow_results_summary}
\\begin{tabular}{lcccc}
\\toprule
Experiment Type & Count & Accuracy (\\%) & F1-Score & Notes \\\\
\\midrule
"""
    
    for stat in summary_stats:
        exp_type = stat['type'].replace('_', '\\_')
        count = stat['count']
        
        if stat['acc_mean'] is not None:
            if stat['acc_std'] is not None and stat['acc_std'] > 0:
                acc_str = f"{stat['acc_mean']*100:.2f} $\\pm$ {stat['acc_std']*100:.2f}"
            else:
                acc_str = f"{stat['acc_mean']*100:.2f}"
        else:
            acc_str = "N/A"
            
        if stat['f1_mean'] is not None:
            if stat['f1_std'] is not None and stat['f1_std'] > 0:
                f1_str = f"{stat['f1_mean']:.3f} $\\pm$ {stat['f1_std']:.3f}"
            else:
                f1_str = f"{stat['f1_mean']:.3f}"
        else:
            f1_str = "N/A"
        
        latex_content += f"{exp_type} & {count} & {acc_str} & {f1_str} & \\\\\n"
    
    latex_content += """\\bottomrule
\\end{tabular}
\\end{table}
"""
    
    with open(output_file, 'w') as f:
        f.write(latex_content)
    
    print(f"âœ… LaTeXè¡¨æ ¼å·²ä¿å­˜è‡³: {output_file}")

def main():
    parser = argparse.ArgumentParser(description="æ”¶é›†å’Œåˆ†æFlowå®éªŒç»“æœ")
    parser.add_argument('--results_dir', type=str, default='results/', 
                       help='å®éªŒç»“æœç›®å½•')
    parser.add_argument('--output_prefix', type=str, default='flow_experiment',
                       help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    parser.add_argument('--generate_latex', action='store_true',
                       help='ç”ŸæˆLaTeXæ ¼å¼è¡¨æ ¼')
    
    args = parser.parse_args()
    
    # æ”¶é›†ç»“æœ
    df = collect_experiment_results(args.results_dir, args.output_prefix)
    
    if df is not None and args.generate_latex:
        generate_latex_summary_table(df, f"{args.output_prefix}_summary.tex")
    
    print("\nğŸ‰ ç»“æœæ”¶é›†å®Œæˆ!")

if __name__ == "__main__":
    main()