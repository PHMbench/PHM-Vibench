# %% [markdown]
# # Vbench æ¡†æ¶æµ‹è¯•ç¬”è®°æœ¬-DG pipeline 
# 
# è¿™ä¸ªç¬”è®°æœ¬æä¾›äº†ä¸€å¥—å…¨é¢çš„æµ‹è¯•åŠŸèƒ½ï¼Œç”¨äºéªŒè¯Vbenchæ¡†æ¶çš„å„ä¸ªå­æ¨¡å—æ˜¯å¦èƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚é€šè¿‡è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ï¼š
# 
# 1. æµ‹è¯•æ•°æ®é›†åŠ è½½å’Œå¤„ç†åŠŸèƒ½
# 2. æµ‹è¯•æ¨¡å‹æ„å»ºå’Œå‰å‘ä¼ æ’­
# 3. æµ‹è¯•ä»»åŠ¡å®šä¹‰å’Œæ‰§è¡Œ
# 4. æµ‹è¯•è®­ç»ƒå™¨åŠŸèƒ½
# 5. éªŒè¯å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹
# 6. å¯è§†åŒ–æ¨¡å‹æ€§èƒ½å’Œæ•°æ®åˆ†å¸ƒ
# 
# è®©æˆ‘ä»¬å¼€å§‹è¿›è¡Œæµ‹è¯•ï¼

# %% [markdown]
# ### å›¾æ ‡

# %% [markdown]
# ## æµ‹è¯•ç¯å¢ƒè®¾ç½®
# 
# é¦–å…ˆï¼Œæˆ‘ä»¬å°†è®¾ç½®æµ‹è¯•ç¯å¢ƒï¼ŒåŒ…æ‹¬å¿…è¦çš„ç›®å½•ç»“æ„å’Œé…ç½®æ–‡ä»¶

# %% [markdown]
# ### å·¥ä½œåŒºï¼Œåªè¿è¡Œä¸€æ¬¡

# %%
# å¯¼å…¥å¿…è¦çš„åº“
import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import yaml
from pprint import pprint
sys.path.insert(0, "/home/user/LQ/B_Signal/Signal_foundation_model/Vbench") # TODO: ä¿®æ”¹ä¸ºä½ çš„é¡¹ç›®è·¯å¾„
# è·å–å½“å‰ç›®å½•
current_dir = os.getcwd()
print(f"å½“å‰ç›®å½•: {current_dir}")

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•ä¸ºä¸Šä¸€çº§ç›®å½•

if 'project_root' not in globals():
    project_root = os.path.dirname(current_dir)
    print(f"è®¾ç½®é¡¹ç›®æ ¹ç›®å½•: {project_root}")
os.chdir(project_root)
print(f"åˆ‡æ¢å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹: {os.getcwd()}")


# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
if project_root not in sys.path:
    sys.path.append(project_root)
    print(f"âœ… å·²å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„: {project_root}")



from src.utils.config_utils import load_config, makedir, path_name, transfer_namespace
from src.data_factory import build_data
from src.model_factory import build_model
from src.task_factory import build_task
from src.trainer_factory import build_trainer


print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—ï¼")
print("è¯·æ£€æŸ¥é¡¹ç›®ç»“æ„å’Œå®‰è£…ä¾èµ–ã€‚")

# %% [markdown]
# ### å¯¼å…¥é…ç½®æ–‡ä»¶
# 
# è®°å¾—ä¿®æ”¹ç¯å¢ƒå˜é‡

# %%
config_path='/home/user/LQ/B_Signal/Signal_foundation_model/Vbench/configs/demo/Single_DG/CWRU.yaml' 

print(f"[INFO] åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
configs = load_config(config_path)

# ç¡®ä¿é…ç½®ä¸­åŒ…å«å¿…è¦çš„éƒ¨åˆ†
required_sections = ['data', 'model', 'task', 'trainer', 'environment']
for section in required_sections:
    if section not in configs:
        print(f"[ERROR] é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ {section} éƒ¨åˆ†")


# è®¾ç½®ç¯å¢ƒå˜é‡å’Œå‘½åç©ºé—´
args_environment = transfer_namespace(configs.get('environment', {}))

args_data = transfer_namespace(configs.get('data', {}))

args_model = transfer_namespace(configs.get('model', {}).get('args', {}))
args_model.name = configs['model'].get('name', 'default')

args_task = transfer_namespace(configs.get('task', {}).get('args', {}))
args_task.name = configs['task'].get('name', 'default')

args_trainer = transfer_namespace(configs.get('trainer', {}).get('args', {}))
args_trainer.name = configs['trainer'].get('name', 'default')

for key, value in configs['environment'].items():
    if key.isupper():
        os.environ[key] = str(value)
        print(f"[INFO] è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}")


# %% [markdown]
# ### æµ‹è¯•ç›®å½•

# %%
# åˆ›å»ºå¿…è¦çš„ç›®å½•
test_dirs = [
    os.path.join(project_root, "results"),
    os.path.join(project_root, "data/processed"),
    os.path.join(project_root, "data/raw"),
    os.path.join(project_root, "save"),
    os.path.join(project_root, "test/results") 
]

for d in test_dirs:
    os.makedirs(d, exist_ok=True)
    print(f"ğŸ“ ç›®å½•å·²å‡†å¤‡: {d}")

# è®¾ç½®é»˜è®¤æµ‹è¯•é…ç½®è·¯å¾„
default_config_path = os.path.join(project_root, "configs/demo/dummy_test.yaml")

# æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if os.path.exists(default_config_path):
    print(f"âœ… æµ‹è¯•é…ç½®æ–‡ä»¶å·²å­˜åœ¨: {default_config_path}")

path, name = path_name(configs, iteration = 1)


# %% [markdown]
# ## 1. data_factory æ•°æ®å·¥å‚æµ‹è¯•
# 

# %% [markdown]
# ### data_factory æµ‹è¯•

# %%
# ç¬¬ä¸€æ¬¡è¿è¡Œæ„å»ºcacheï¼Œcache æ ¹æ®meta_dataæ–‡ä»¶è¿›è¡Œå‘½å
data_factory = build_data(args_data,args_task)
# ç¬¬äºŒæ¬¡è¿è¡Œå¯ä»¥ç›´æ¥è¯»å–cache
data = data_factory.get_data()
print(f"æ•°æ®é›†å¤§å°: {len(data)}")
dataset = data_factory.get_dataset()
print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
dataloader = data_factory.get_dataloader()
print(f"æ•°æ®åŠ è½½å™¨å¤§å°: {len(dataloader)}")


# %%
dataloader

# %% [markdown]
# ### loop dataloader

# %%
# for i, ((inputs,labels), name) in enumerate(dataloader):
#     print(f"ç¬¬ {i+1} æ‰¹æ•°æ®:")
#     print(f"è¾“å…¥: {inputs.shape}")
#     # print(f"è¾“å…¥: {inputs}")
#     print(f"æ ‡ç­¾: {labels}")
#     print(f"åç§°: {name}")


# %% [markdown]
# ## 2. model factory æ¨¡å‹å·¥å‚æµ‹è¯•
# 
# æµ‹è¯•æ¨¡å‹çš„æ„å»ºå’Œå‰å‘ä¼ æ’­

# %%
model = build_model(args_model)

# %% [markdown]
# ## 3. task_factory ä»»åŠ¡å·¥å‚æµ‹è¯•
# 
# æµ‹è¯•ä»»åŠ¡çš„å®šä¹‰å’Œæ‰§è¡Œ

# %%
task= build_task(
    args_task = args_task,
    network = model,
    args_data = args_data,
    args_model = args_model,
    args_trainer = args_trainer,
    args_environment = args_environment,
    metadata = data_factory.get_metadata()
)

# %% [markdown]
# ## 4. trainer_factory è®­ç»ƒå™¨å·¥å‚æµ‹è¯•
# 
# æµ‹è¯•è®­ç»ƒå™¨çš„æ„å»ºå’Œç®€å•è®­ç»ƒ

# %%
trainer = build_trainer(
    args_environment,
    args_trainer,  # è®­ç»ƒå‚æ•° (Namespace)
    args_data,     # æ•°æ®å‚æ•° (Namespace)
    path)

# %% [markdown]
# ## 5. pipeline å®Œæ•´æµç¨‹é›†æˆæµ‹è¯•
# 
# æµ‹è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½åˆ°å®Œæ•´è®­ç»ƒæµç¨‹çš„æ‰€æœ‰ç¯èŠ‚

# %%

from pytorch_lightning import LightningModule, Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
import torch
import pandas as pd
import os
import wandb

def load_best_model_checkpoint(model: LightningModule, trainer: Trainer) -> LightningModule:
    """
    åŠ è½½è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚

    å‚æ•°:
    - model: è¦åŠ è½½æ£€æŸ¥ç‚¹æƒé‡çš„æ¨¡å‹å®ä¾‹ã€‚
    - trainer: ç”¨äºè®­ç»ƒæ¨¡å‹çš„è®­ç»ƒå™¨å®ä¾‹ã€‚

    è¿”å›:
    - åŠ è½½äº†æœ€ä½³æ£€æŸ¥ç‚¹æƒé‡çš„æ¨¡å‹å®ä¾‹ã€‚
    """
    # ä»trainerçš„callbacksä¸­æ‰¾åˆ°ModelCheckpointå®ä¾‹ï¼Œå¹¶è·å–best_model_path
    model_checkpoint = None
    for callback in trainer.callbacks:
        if isinstance(callback, ModelCheckpoint):
            model_checkpoint = callback
            break

    if model_checkpoint is None:
        raise ValueError("ModelCheckpoint callback not found in trainer's callbacks.")

    best_model_path = model_checkpoint.best_model_path
    print(f"Best model path: {best_model_path}")

    # ç¡®ä¿æœ€ä½³æ¨¡å‹è·¯å¾„ä¸æ˜¯ç©ºçš„
    if not best_model_path:
        raise ValueError("No best model path found. Please check if the training process saved checkpoints.")

    # åŠ è½½æœ€ä½³æ£€æŸ¥ç‚¹

    state_dict = torch.load(best_model_path)
    model.load_state_dict(state_dict['state_dict'])
    return model

trainer.fit(task,data_factory.get_dataloader('train'),
            data_factory.get_dataloader('val')) # TODO load best checkpoint
task = load_best_model_checkpoint(task,trainer)
result = trainer.test(task,data_factory.get_dataloader('test'))
# ä¿å­˜ç»“æœ
result_df = pd.DataFrame(result)
result_df.to_csv(os.path.join(path, f'test_result_{1}.csv'), index=False)
if args_trainer.wandb:
    wandb.finish()


# %%



