{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213371a2",
   "metadata": {},
   "source": [
    "# Vbench æ¡†æ¶æµ‹è¯•ç¬”è®°æœ¬\n",
    "\n",
    "è¿™ä¸ªç¬”è®°æœ¬æä¾›äº†ä¸€å¥—å…¨é¢çš„æµ‹è¯•åŠŸèƒ½ï¼Œç”¨äºéªŒè¯Vbenchæ¡†æ¶çš„å„ä¸ªå­æ¨¡å—æ˜¯å¦èƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚é€šè¿‡è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ï¼š\n",
    "\n",
    "1. æµ‹è¯•æ•°æ®é›†åŠ è½½å’Œå¤„ç†åŠŸèƒ½\n",
    "2. æµ‹è¯•æ¨¡å‹æ„å»ºå’Œå‰å‘ä¼ æ’­\n",
    "3. æµ‹è¯•ä»»åŠ¡å®šä¹‰å’Œæ‰§è¡Œ\n",
    "4. æµ‹è¯•è®­ç»ƒå™¨åŠŸèƒ½\n",
    "5. éªŒè¯å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹\n",
    "6. å¯è§†åŒ–æ¨¡å‹æ€§èƒ½å’Œæ•°æ®åˆ†å¸ƒ\n",
    "\n",
    "è®©æˆ‘ä»¬å¼€å§‹è¿›è¡Œæµ‹è¯•ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„\n",
    "# è·å–å½“å‰ç¬”è®°æœ¬è·¯å¾„\n",
    "current_dir = os.getcwd()\n",
    "# è·å–é¡¹ç›®æ ¹ç›®å½•\n",
    "project_root = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'test' else current_dir\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# å¯¼å…¥é¡¹ç›®æ¨¡å—\n",
    "try:\n",
    "    from src.utils.config_utils import load_config, makedir, path_name, transfer_namespace\n",
    "    from src.data_factory import build_dataset\n",
    "    from src.model_factory import build_model\n",
    "    from src.task_factory import build_task\n",
    "    from src.trainer_factory import build_trainer\n",
    "    from src.utils.metrics_utils import compute_metrics\n",
    "    from src.utils.loss_utils import get_loss_function\n",
    "    \n",
    "    print(\"âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—ï¼\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {str(e)}\")\n",
    "    print(\"è¯·æ£€æŸ¥é¡¹ç›®ç»“æ„å’Œå®‰è£…ä¾èµ–ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec2536",
   "metadata": {},
   "source": [
    "## æµ‹è¯•ç¯å¢ƒè®¾ç½®\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬å°†è®¾ç½®æµ‹è¯•ç¯å¢ƒï¼ŒåŒ…æ‹¬å¿…è¦çš„ç›®å½•ç»“æ„å’Œé…ç½®æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fa942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¿…è¦çš„ç›®å½•\n",
    "test_dirs = [\n",
    "    os.path.join(project_root, \"results\"),\n",
    "    os.path.join(project_root, \"data/processed\"),\n",
    "    os.path.join(project_root, \"data/raw\"),\n",
    "    os.path.join(project_root, \"save\"),\n",
    "    os.path.join(project_root, \"test/results\") \n",
    "]\n",
    "\n",
    "for d in test_dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    print(f\"ğŸ“ ç›®å½•å·²å‡†å¤‡: {d}\")\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# è®¾ç½®é»˜è®¤æµ‹è¯•é…ç½®è·¯å¾„\n",
    "default_config_path = os.path.join(project_root, \"configs/demo/dummy_test.yaml\")\n",
    "\n",
    "# æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "if os.path.exists(default_config_path):\n",
    "    print(f\"âœ… æµ‹è¯•é…ç½®æ–‡ä»¶å·²å­˜åœ¨: {default_config_path}\")\n",
    "else:\n",
    "    print(f\"âŒ æœªæ‰¾åˆ°æµ‹è¯•é…ç½®æ–‡ä»¶: {default_config_path}\")\n",
    "    print(\"å°†åˆ›å»ºé»˜è®¤æµ‹è¯•é…ç½®...\")\n",
    "    \n",
    "    # åˆ›å»ºé»˜è®¤é…ç½®ç›®å½•\n",
    "    os.makedirs(os.path.dirname(default_config_path), exist_ok=True)\n",
    "    \n",
    "    # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„æµ‹è¯•é…ç½®\n",
    "    default_config = {\n",
    "        \"dataset\": {\n",
    "            \"name\": \"DummyDataset\",\n",
    "            \"args\": {\n",
    "                \"task_type\": \"classification\",\n",
    "                \"num_samples\": 1000,\n",
    "                \"feature_dim\": 10,\n",
    "                \"num_classes\": 2\n",
    "            }\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"DummyCNNModel\",\n",
    "            \"args\": {\n",
    "                \"input_channels\": 1,\n",
    "                \"hidden_channels\": [16, 32, 64],\n",
    "                \"output_dim\": 2\n",
    "            }\n",
    "        },\n",
    "        \"task\": {\n",
    "            \"name\": \"DummyClassificationTask\",\n",
    "            \"args\": {\n",
    "                \"num_classes\": 2\n",
    "            }\n",
    "        },\n",
    "        \"trainer\": {\n",
    "            \"name\": \"ModularTrainer\",\n",
    "            \"args\": {\n",
    "                \"epochs\": 2,  # æµ‹è¯•ç”¨è¾ƒå°‘çš„è½®æ¬¡\n",
    "                \"batch_size\": 32,\n",
    "                \"loss_fn\": \"cross_entropy\",\n",
    "                \"metrics\": [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # å°†é…ç½®å†™å…¥æ–‡ä»¶\n",
    "    with open(default_config_path, 'w') as f:\n",
    "        yaml.dump(default_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"âœ… å·²åˆ›å»ºé»˜è®¤æµ‹è¯•é…ç½®: {default_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa93fe5",
   "metadata": {},
   "source": [
    "## 1. æ•°æ®å·¥å‚æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•æ•°æ®é›†çš„åŠ è½½å’Œå¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_factory():\n",
    "    print(\"ğŸ” æµ‹è¯•æ•°æ®é›†å·¥å‚...\")\n",
    "    \n",
    "    # åŠ è½½é…ç½®\n",
    "    configs = load_config(default_config_path)\n",
    "    \n",
    "    # æ„å»ºæ•°æ®é›†\n",
    "    try:\n",
    "        dataset = build_dataset(configs['dataset'])\n",
    "        print(f\"âœ… æˆåŠŸæ„å»ºæ•°æ®é›†: {dataset.__class__.__name__}\")\n",
    "        \n",
    "        # è·å–æ•°æ®åŠ è½½å™¨\n",
    "        train_loader = dataset.get_train_loader()\n",
    "        val_loader = dataset.get_val_loader()\n",
    "        test_loader = dataset.get_test_loader()\n",
    "        \n",
    "        # æ‰“å°æ•°æ®åŠ è½½å™¨ä¿¡æ¯\n",
    "        print(f\"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_loader.dataset)} æ ·æœ¬\")\n",
    "        print(f\"ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_loader.dataset)} æ ·æœ¬\")\n",
    "        print(f\"ğŸ“Š æµ‹è¯•é›†å¤§å°: {len(test_loader.dataset)} æ ·æœ¬\")\n",
    "        \n",
    "        # æ£€æŸ¥ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®\n",
    "        x_batch, y_batch = next(iter(train_loader))\n",
    "        print(f\"ğŸ“Š æ‰¹æ¬¡ç‰¹å¾å½¢çŠ¶: {x_batch.shape}\")\n",
    "        print(f\"ğŸ“Š æ‰¹æ¬¡æ ‡ç­¾å½¢çŠ¶: {y_batch.shape}\")\n",
    "        \n",
    "        # å¯è§†åŒ–ä¸€æ‰¹æ•°æ®\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # å·¦å›¾ï¼šç‰¹å¾æ•°æ®å¯è§†åŒ–\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(x_batch[0].reshape(x_batch.shape[1], -1).detach().numpy(), aspect='auto', cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.title('ç‰¹å¾çŸ©é˜µç¤ºä¾‹')\n",
    "        plt.xlabel('ç‰¹å¾')\n",
    "        plt.ylabel('æ ·æœ¬')\n",
    "        \n",
    "        # å³å›¾ï¼šæ ‡ç­¾åˆ†å¸ƒ\n",
    "        plt.subplot(1, 2, 2)\n",
    "        class_counts = np.bincount(y_batch.numpy())\n",
    "        plt.bar(range(len(class_counts)), class_counts)\n",
    "        plt.title('æ ‡ç­¾åˆ†å¸ƒ')\n",
    "        plt.xlabel('ç±»åˆ«')\n",
    "        plt.ylabel('æ ·æœ¬æ•°')\n",
    "        plt.xticks(range(len(class_counts)))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"dataset\": dataset,\n",
    "            \"train_loader\": train_loader,\n",
    "            \"val_loader\": val_loader,\n",
    "            \"test_loader\": test_loader\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"âŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# è¿è¡Œæ•°æ®é›†æµ‹è¯•\n",
    "data_test_results = test_data_factory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91223166",
   "metadata": {},
   "source": [
    "## 2. æ¨¡å‹å·¥å‚æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•æ¨¡å‹çš„æ„å»ºå’Œå‰å‘ä¼ æ’­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_factory():\n",
    "    print(\"ğŸ” æµ‹è¯•æ¨¡å‹å·¥å‚...\")\n",
    "    \n",
    "    # åŠ è½½é…ç½®\n",
    "    configs = load_config(default_config_path)\n",
    "    \n",
    "    try:\n",
    "        # æ„å»ºæ¨¡å‹\n",
    "        model = build_model(configs['model'])\n",
    "        print(f\"âœ… æˆåŠŸæ„å»ºæ¨¡å‹: {model.__class__.__name__}\")\n",
    "        \n",
    "        # æ‰“å°æ¨¡å‹æ‘˜è¦\n",
    "        if hasattr(model, 'summary'):\n",
    "            print(\"ğŸ“Š æ¨¡å‹æ‘˜è¦:\")\n",
    "            print(model.summary())\n",
    "        print(f\"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "        \n",
    "        # å¦‚æœæˆ‘ä»¬æœ‰æ•°æ®åŠ è½½å™¨ï¼Œå°è¯•æ¨¡å‹å‰å‘ä¼ æ’­\n",
    "        if 'data_test_results' in globals() and data_test_results[\"success\"]:\n",
    "            train_loader = data_test_results[\"train_loader\"]\n",
    "            x_batch, _ = next(iter(train_loader))\n",
    "            \n",
    "            # å‰å‘ä¼ æ’­\n",
    "            with torch.no_grad():\n",
    "                outputs = model(x_batch)\n",
    "                \n",
    "            print(f\"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {outputs.shape}\")\n",
    "            \n",
    "            # å¯è§†åŒ–è¾“å‡ºåˆ†å¸ƒ\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            \n",
    "            if outputs.shape[1] <= 10:  # å¦‚æœè¾“å‡ºç»´åº¦ä¸å¤ªå¤§\n",
    "                # è¾“å‡ºæ¿€æ´»å€¼åˆ†å¸ƒ\n",
    "                plt.subplot(1, 2, 1)\n",
    "                output_np = outputs.detach().numpy()\n",
    "                for i in range(outputs.shape[1]):\n",
    "                    plt.hist(output_np[:, i], bins=20, alpha=0.5, label=f'Class {i}')\n",
    "                plt.title('æ¨¡å‹è¾“å‡ºåˆ†å¸ƒ')\n",
    "                plt.xlabel('æ¿€æ´»å€¼')\n",
    "                plt.ylabel('é¢‘ç‡')\n",
    "                if outputs.shape[1] <= 5:  # åªæœ‰å°‘é‡ç±»åˆ«æ—¶æ˜¾ç¤ºå›¾ä¾‹\n",
    "                    plt.legend()\n",
    "                \n",
    "                # å„ç±»åˆ«å¹³å‡æ¿€æ´»å€¼\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.bar(range(outputs.shape[1]), outputs.mean(dim=0).detach().numpy())\n",
    "                plt.title('å„ç±»åˆ«å¹³å‡æ¿€æ´»å€¼')\n",
    "                plt.xlabel('ç±»åˆ«')\n",
    "                plt.ylabel('å¹³å‡æ¿€æ´»å€¼')\n",
    "                plt.xticks(range(outputs.shape[1]))\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                # å¦‚æœè¾“å‡ºç»´åº¦å¾ˆå¤§ï¼Œåªæ˜¾ç¤ºå‰10ä¸ªç»´åº¦\n",
    "                plt.bar(range(10), outputs.mean(dim=0)[:10].detach().numpy())\n",
    "                plt.title('å‰10ä¸ªè¾“å‡ºç»´åº¦çš„å¹³å‡æ¿€æ´»å€¼')\n",
    "                plt.xlabel('è¾“å‡ºç»´åº¦')\n",
    "                plt.ylabel('å¹³å‡æ¿€æ´»å€¼')\n",
    "                plt.show()\n",
    "            \n",
    "        return {\"success\": True, \"model\": model}\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# è¿è¡Œæ¨¡å‹æµ‹è¯•\n",
    "model_test_results = test_model_factory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c928f",
   "metadata": {},
   "source": [
    "## 3. ä»»åŠ¡å·¥å‚æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•ä»»åŠ¡çš„å®šä¹‰å’Œæ‰§è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_task_factory():\n",
    "    print(\"ğŸ” æµ‹è¯•ä»»åŠ¡å·¥å‚...\")\n",
    "    \n",
    "    # åŠ è½½é…ç½®\n",
    "    configs = load_config(default_config_path)\n",
    "    \n",
    "    try:\n",
    "        # éœ€è¦å…ˆæœ‰æ¨¡å‹æ‰èƒ½æµ‹è¯•ä»»åŠ¡\n",
    "        if 'model_test_results' not in globals() or not model_test_results[\"success\"]:\n",
    "            print(\"âš ï¸ æ¨¡å‹å°šæœªæˆåŠŸæ„å»ºï¼Œå…ˆåˆ›å»ºæ¨¡å‹...\")\n",
    "            model = build_model(configs['model'])\n",
    "        else:\n",
    "            model = model_test_results[\"model\"]\n",
    "        \n",
    "        # å°†æ¨¡å‹æ·»åŠ åˆ°ä»»åŠ¡é…ç½®\n",
    "        task_config = configs['task'].copy()\n",
    "        task_config['args'] = task_config.get('args', {}).copy()\n",
    "        task_config['args']['model'] = model\n",
    "        \n",
    "        # æ„å»ºä»»åŠ¡\n",
    "        task = build_task(task_config)\n",
    "        print(f\"âœ… æˆåŠŸæ„å»ºä»»åŠ¡: {task.__class__.__name__}\")\n",
    "        \n",
    "        # æµ‹è¯•ä»»åŠ¡åŠŸèƒ½\n",
    "        if hasattr(task, 'get_loss_fn'):\n",
    "            loss_fn = task.get_loss_fn()\n",
    "            print(f\"âœ… æŸå¤±å‡½æ•°ç±»å‹: {type(loss_fn).__name__}\")\n",
    "        \n",
    "        # å¦‚æœæœ‰æ•°æ®ï¼Œå°è¯•æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤\n",
    "        if 'data_test_results' in globals() and data_test_results[\"success\"]:\n",
    "            train_loader = data_test_results[\"train_loader\"]\n",
    "            x_batch, y_batch = next(iter(train_loader))\n",
    "            \n",
    "            # æ¨¡å‹æ¨ç†\n",
    "            outputs = model(x_batch)\n",
    "            \n",
    "            # å¦‚æœä»»åŠ¡æœ‰evaluateæ–¹æ³•ï¼Œæµ‹è¯•è¯„ä¼°\n",
    "            if hasattr(task, 'evaluate'):\n",
    "                print(\"ğŸ“Š æµ‹è¯•ä»»åŠ¡è¯„ä¼°å‡½æ•°...\")\n",
    "                eval_results = task.evaluate(data_test_results[\"test_loader\"])\n",
    "                print(\"âœ… è¯„ä¼°ç»“æœ:\")\n",
    "                pprint(eval_results)\n",
    "            \n",
    "            # å¦‚æœä»»åŠ¡æœ‰calculate_metricsæ–¹æ³•ï¼Œæµ‹è¯•æŒ‡æ ‡è®¡ç®—\n",
    "            if hasattr(task, 'calculate_metrics'):\n",
    "                print(\"ğŸ“Š æµ‹è¯•ä»»åŠ¡æŒ‡æ ‡è®¡ç®—...\")\n",
    "                metrics = task.calculate_metrics(outputs, y_batch)\n",
    "                print(\"âœ… è®¡ç®—æŒ‡æ ‡:\")\n",
    "                pprint(metrics)\n",
    "                \n",
    "        return {\"success\": True, \"task\": task}\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"âŒ ä»»åŠ¡æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# è¿è¡Œä»»åŠ¡æµ‹è¯•\n",
    "task_test_results = test_task_factory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b77010",
   "metadata": {},
   "source": [
    "## 4. è®­ç»ƒå™¨å·¥å‚æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•è®­ç»ƒå™¨çš„æ„å»ºå’Œç®€å•è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trainer_factory():\n",
    "    print(\"ğŸ” æµ‹è¯•è®­ç»ƒå™¨å·¥å‚...\")\n",
    "    \n",
    "    # åŠ è½½é…ç½®\n",
    "    configs = load_config(default_config_path)\n",
    "    \n",
    "    try:\n",
    "        # ç¡®ä¿æˆ‘ä»¬æœ‰ä»»åŠ¡å’Œæ•°æ®åŠ è½½å™¨\n",
    "        if ('task_test_results' not in globals() or not task_test_results[\"success\"] or\n",
    "            'data_test_results' not in globals() or not data_test_results[\"success\"]):\n",
    "            print(\"âš ï¸ ä»»åŠ¡æˆ–æ•°æ®åŠ è½½å™¨å°šæœªæˆåŠŸæ„å»ºï¼Œæ— æ³•æµ‹è¯•è®­ç»ƒå™¨\")\n",
    "            return {\"success\": False, \"error\": \"ç¼ºå°‘ä»»åŠ¡æˆ–æ•°æ®åŠ è½½å™¨\"}\n",
    "        \n",
    "        task = task_test_results[\"task\"]\n",
    "        model = model_test_results[\"model\"]\n",
    "        dataset = data_test_results[\"dataset\"]\n",
    "        \n",
    "        # æ„å»ºè®­ç»ƒå™¨\n",
    "        trainer = build_trainer(configs['trainer'])\n",
    "        print(f\"âœ… æˆåŠŸæ„å»ºè®­ç»ƒå™¨: {trainer.__class__.__name__}\")\n",
    "        \n",
    "        # ä¸ºæµ‹è¯•å‡†å¤‡ä¸€ä¸ªå°å‹è®­ç»ƒ\n",
    "        mini_epochs = 1  # åªè®­ç»ƒä¸€ä¸ªå‘¨æœŸç”¨äºæµ‹è¯•\n",
    "        configs['trainer']['args']['epochs'] = mini_epochs\n",
    "        \n",
    "        # è®¾ç½®æµ‹è¯•ä¿å­˜è·¯å¾„\n",
    "        save_path = os.path.join(project_root, \"test/results/trainer_test\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # å‡†å¤‡å‘½åç©ºé—´å‚æ•°\n",
    "        args_t = transfer_namespace(configs['trainer'].get('args', {}))\n",
    "        args_m = transfer_namespace(configs['model'].get('args', {}))\n",
    "        args_d = transfer_namespace(configs['dataset'].get('args', {}))\n",
    "        args_task = transfer_namespace(configs['task'].get('args', {}))\n",
    "        \n",
    "        print(f\"ğŸš€ å¼€å§‹æµ‹è¯•è®­ç»ƒ (ä»…è¿è¡Œ {mini_epochs} ä¸ªå‘¨æœŸ)...\")\n",
    "        train_loader = data_test_results[\"train_loader\"]\n",
    "        val_loader = data_test_results[\"val_loader\"]\n",
    "        test_loader = data_test_results[\"test_loader\"]\n",
    "        \n",
    "        # æ‰§è¡Œè®­ç»ƒ\n",
    "        result = trainer(\n",
    "            dataset=dataset,\n",
    "            model=model,\n",
    "            task=task,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            configs=configs,\n",
    "            args_t=args_t,\n",
    "            args_m=args_m,\n",
    "            args_d=args_d,\n",
    "            args_task=args_task,\n",
    "            save_path=save_path,\n",
    "            iteration=0\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâœ… è®­ç»ƒæµ‹è¯•å®Œæˆ\")\n",
    "        print(\"ğŸ“Š è®­ç»ƒç»“æœ:\")\n",
    "        pprint(result)\n",
    "        \n",
    "        # å¦‚æœæœ‰æµ‹è¯•ç»“æœï¼Œç»˜åˆ¶æŒ‡æ ‡å›¾\n",
    "        if isinstance(result, dict) and len(result) > 0:\n",
    "            # ç­›é€‰æ•°å€¼å‹æŒ‡æ ‡\n",
    "            metrics = {k: v for k, v in result.items() if isinstance(v, (int, float))}\n",
    "            if metrics:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.bar(metrics.keys(), metrics.values())\n",
    "                plt.title('æµ‹è¯•è¯„ä¼°æŒ‡æ ‡')\n",
    "                plt.ylabel('å€¼')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        return {\"success\": True, \"trainer\": trainer, \"result\": result}\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"âŒ è®­ç»ƒå™¨æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# è¿è¡Œè®­ç»ƒå™¨æµ‹è¯•\n",
    "trainer_test_results = test_trainer_factory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4579bd",
   "metadata": {},
   "source": [
    "## 5. å·¥å…·æ¨¡å—æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•æŸå¤±å‡½æ•°ã€è¯„ä¼°æŒ‡æ ‡å’Œæ¨¡å‹é€‰æ‹©å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda24b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_utils_modules():\n",
    "    print(\"ğŸ” æµ‹è¯•å·¥å…·æ¨¡å—...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # æµ‹è¯•æŸå¤±å‡½æ•°å·¥å…·\n",
    "    print(\"\\nğŸ“Š æµ‹è¯•æŸå¤±å‡½æ•°å·¥å…·...\")\n",
    "    try:\n",
    "        from src.utils.loss_utils import get_loss_function\n",
    "        \n",
    "        # æµ‹è¯•è·å–ä¸åŒç±»å‹çš„æŸå¤±å‡½æ•°\n",
    "        loss_functions = ['cross_entropy', 'mse', 'focal', 'rul']\n",
    "        for loss_name in loss_functions:\n",
    "            try:\n",
    "                loss_fn = get_loss_function(loss_name)\n",
    "                print(f\"âœ… æˆåŠŸè·å–æŸå¤±å‡½æ•°: {loss_name} -> {type(loss_fn).__name__}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ åŠ è½½æŸå¤±å‡½æ•°å¤±è´¥: {loss_name} - {str(e)}\")\n",
    "        \n",
    "        results['loss_utils'] = True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æŸå¤±å‡½æ•°å·¥å…·æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
    "        results['loss_utils'] = False\n",
    "    \n",
    "    # æµ‹è¯•è¯„ä¼°æŒ‡æ ‡å·¥å…·\n",
    "    print(\"\\nğŸ“Š æµ‹è¯•è¯„ä¼°æŒ‡æ ‡å·¥å…·...\")\n",
    "    try:\n",
    "        from src.utils.metrics_utils import compute_metrics\n",
    "        \n",
    "        # åˆ›å»ºä¸€äº›æµ‹è¯•æ•°æ®\n",
    "        y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1])\n",
    "        y_pred = np.array([0, 1, 1, 1, 0, 0, 0, 1])\n",
    "        \n",
    "        # è®¡ç®—åˆ†ç±»æŒ‡æ ‡\n",
    "        metrics = compute_metrics('classification', y_true, y_pred)\n",
    "        print(\"âœ… åˆ†ç±»æŒ‡æ ‡è®¡ç®—ç»“æœ:\")\n",
    "        for k, v in metrics.items():\n",
    "            if k != 'confusion_matrix':  # æ’é™¤æ··æ·†çŸ©é˜µï¼Œå®ƒå¤ªå¤§äº†\n",
    "                print(f\"   {k}: {v}\")\n",
    "        \n",
    "        # å¦‚æœæœ‰æ··æ·†çŸ©é˜µï¼Œç»˜åˆ¶å®ƒ\n",
    "        if 'confusion_matrix' in metrics:\n",
    "            cm = metrics['confusion_matrix']\n",
    "            plt.figure(figsize=(5, 4))\n",
    "            plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "            plt.title('æ··æ·†çŸ©é˜µ')\n",
    "            plt.colorbar()\n",
    "            plt.xlabel('é¢„æµ‹æ ‡ç­¾')\n",
    "            plt.ylabel('çœŸå®æ ‡ç­¾')\n",
    "            classes = [f\"ç±»åˆ«{i}\" for i in range(cm.shape[0])]\n",
    "            tick_marks = np.arange(len(classes))\n",
    "            plt.xticks(tick_marks, classes)\n",
    "            plt.yticks(tick_marks, classes)\n",
    "            \n",
    "            # åœ¨æ ¼å­ä¸­æ˜¾ç¤ºæ•°å­—\n",
    "            for i in range(cm.shape[0]):\n",
    "                for j in range(cm.shape[1]):\n",
    "                    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                            ha=\"center\", va=\"center\",\n",
    "                            color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        results['metrics_utils'] = True\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"âŒ è¯„ä¼°æŒ‡æ ‡å·¥å…·æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        results['metrics_utils'] = False\n",
    "    \n",
    "    # æµ‹è¯•æ¨¡å‹é€‰æ‹©å·¥å…·\n",
    "    print(\"\\nğŸ“Š æµ‹è¯•æ¨¡å‹é€‰æ‹©å·¥å…·...\")\n",
    "    try:\n",
    "        from src.utils.model_selection import ModelValidator, HyperparameterTuner\n",
    "        print(\"âœ… æ¨¡å‹é€‰æ‹©å·¥å…·å¯¼å…¥æˆåŠŸ\")\n",
    "        # è¿™é‡Œä¸åšå®é™…æµ‹è¯•ï¼Œå› ä¸ºæ¨¡å‹é€‰æ‹©éœ€è¦å®Œæ•´çš„è®­ç»ƒæµç¨‹\n",
    "        results['model_selection'] = True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹é€‰æ‹©å·¥å…·æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
    "        results['model_selection'] = False\n",
    "    \n",
    "    return results\n",
    "\n",
    "# è¿è¡Œå·¥å…·æ¨¡å—æµ‹è¯•\n",
    "utils_test_results = test_utils_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c028d",
   "metadata": {},
   "source": [
    "## 6. å®Œæ•´æµç¨‹é›†æˆæµ‹è¯•\n",
    "\n",
    "æµ‹è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½åˆ°å®Œæ•´è®­ç»ƒæµç¨‹çš„æ‰€æœ‰ç¯èŠ‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb032c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_full_pipeline():\n",
    "    print(\"ğŸ” æµ‹è¯•å®Œæ•´æµç¨‹...\")\n",
    "    \n",
    "    try:\n",
    "        from src.Pipeline_01_default import main as pipeline_main\n",
    "        \n",
    "        # è®¾ç½®æµ‹è¯•é…ç½®\n",
    "        config_path = default_config_path\n",
    "        iterations = 1\n",
    "        use_wandb = False\n",
    "        notes = \"æµ‹è¯•ç”¨ä¾‹è¿è¡Œ\"\n",
    "        seed = 42\n",
    "        \n",
    "        print(f\"ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´æµç¨‹ï¼Œé…ç½®æ–‡ä»¶: {config_path}\")\n",
    "        \n",
    "        # æ‰§è¡Œå®Œæ•´æµç¨‹\n",
    "        results = pipeline_main(\n",
    "            config_path=config_path,\n",
    "            iterations=iterations,\n",
    "            use_wandb=use_wandb,\n",
    "            notes=notes,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâœ… å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸ!\")\n",
    "        print(\"ğŸ“Š æµ‹è¯•ç»“æœ:\")\n",
    "        pprint(results)\n",
    "        \n",
    "        return {\"success\": True, \"results\": results}\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"âŒ å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# è¿è¡Œå®Œæ•´æµç¨‹æµ‹è¯•\n",
    "pipeline_test_results = test_full_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939ed6a",
   "metadata": {},
   "source": [
    "## 7. æµ‹è¯•ç»“æœæ±‡æ€»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_summary():\n",
    "    test_modules = [\n",
    "        (\"æ•°æ®å·¥å‚\", 'data_test_results'),\n",
    "        (\"æ¨¡å‹å·¥å‚\", 'model_test_results'),\n",
    "        (\"ä»»åŠ¡å·¥å‚\", 'task_test_results'),\n",
    "        (\"è®­ç»ƒå™¨å·¥å‚\", 'trainer_test_results'),\n",
    "        (\"å·¥å…·æ¨¡å—\", 'utils_test_results'),\n",
    "        (\"å®Œæ•´æµç¨‹\", 'pipeline_test_results')\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“‹ Vbench æ¡†æ¶æµ‹è¯•æ±‡æ€»\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_passed = True\n",
    "    for name, var_name in test_modules:\n",
    "        if var_name in globals():\n",
    "            result = globals()[var_name]\n",
    "            if isinstance(result, dict) and result.get(\"success\", False):\n",
    "                status = \"âœ… é€šè¿‡\"\n",
    "            else:\n",
    "                status = \"âŒ å¤±è´¥\"\n",
    "                all_passed = False\n",
    "        else:\n",
    "            status = \"â“ æœªæµ‹è¯•\"\n",
    "            all_passed = False\n",
    "        \n",
    "        print(f\"{name:<15} {status}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    if all_passed:\n",
    "        print(\"ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! Vbench æ¡†æ¶å·¥ä½œæ­£å¸¸ã€‚\")\n",
    "    else:\n",
    "        print(\"âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†ç»“æœå¹¶ä¿®å¤é—®é¢˜ã€‚\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# æ‰“å°æµ‹è¯•ç»“æœæ±‡æ€»\n",
    "print_test_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94569f45",
   "metadata": {},
   "source": [
    "## 8. è°ƒè¯•å’Œé—®é¢˜è¯Šæ–­\n",
    "\n",
    "å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·è¿›è¡Œè¯Šæ–­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_issues():\n",
    "    print(\"ğŸ” å¼€å§‹è¯Šæ–­æ½œåœ¨é—®é¢˜...\")\n",
    "    \n",
    "    # æ£€æŸ¥é¡¹ç›®ç»“æ„\n",
    "    print(\"\\nğŸ“‚ æ£€æŸ¥é¡¹ç›®ç»“æ„...\")\n",
    "    essential_dirs = [\n",
    "        \"src\", \"configs\", \"data\", \"results\", \"save\"\n",
    "    ]\n",
    "    for d in essential_dirs:\n",
    "        path = os.path.join(project_root, d)\n",
    "        if os.path.exists(path):\n",
    "            print(f\"âœ… ç›®å½•å­˜åœ¨: {d}\")\n",
    "        else:\n",
    "            print(f\"âŒ ç›®å½•ç¼ºå¤±: {d}\")\n",
    "    \n",
    "    # æ£€æŸ¥å…³é”®æ¨¡å—\n",
    "    print(\"\\nğŸ“¦ æ£€æŸ¥å…³é”®æ¨¡å—...\")\n",
    "    essential_modules = [\n",
    "        (\"data_factory\", \"src/data_factory/__init__.py\"),\n",
    "        (\"model_factory\", \"src/model_factory/__init__.py\"),\n",
    "        (\"task_factory\", \"src/task_factory/__init__.py\"),\n",
    "        (\"trainer_factory\", \"src/trainer_factory/__init__.py\"),\n",
    "        (\"utils\", \"src/utils/__init__.py\")\n",
    "    ]\n",
    "    for name, path in essential_modules:\n",
    "        full_path = os.path.join(project_root, path)\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"âœ… æ¨¡å—å­˜åœ¨: {name}\")\n",
    "        else:\n",
    "            print(f\"âŒ æ¨¡å—ç¼ºå¤±: {name} (è·¯å¾„: {path})\")\n",
    "    \n",
    "    # æ£€æŸ¥PyTorchç‰ˆæœ¬å’Œå¯ç”¨æ€§\n",
    "    print(\"\\nğŸ§© æ£€æŸ¥PyTorch...\")\n",
    "    print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "    print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "        print(f\"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # æ£€æŸ¥é…ç½®æ–‡ä»¶\n",
    "    print(\"\\nğŸ“„ æ£€æŸ¥é…ç½®æ–‡ä»¶...\")\n",
    "    if os.path.exists(default_config_path):\n",
    "        with open(default_config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        print(f\"âœ… é…ç½®æ–‡ä»¶å­˜åœ¨: {default_config_path}\")\n",
    "        \n",
    "        # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„å…³é”®éƒ¨åˆ†\n",
    "        required_sections = ['dataset', 'model', 'task', 'trainer']\n",
    "        for section in required_sections:\n",
    "            if section in config:\n",
    "                if 'name' in config[section]:\n",
    "                    print(f\"âœ… é…ç½®éƒ¨åˆ†æœ‰æ•ˆ: {section} -> {config[section]['name']}\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸ é…ç½®éƒ¨åˆ†ç¼ºå°‘name: {section}\")\n",
    "            else:\n",
    "                print(f\"âŒ é…ç½®éƒ¨åˆ†ç¼ºå¤±: {section}\")\n",
    "    else:\n",
    "        print(f\"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {default_config_path}\")\n",
    "\n",
    "# è¿è¡Œé—®é¢˜è¯Šæ–­\n",
    "diagnose_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dab64d",
   "metadata": {},
   "source": [
    "## 9. è‡ªå®šä¹‰æµ‹è¯•\n",
    "\n",
    "å¯ä»¥ä½¿ç”¨è¿™ä¸ªåŒºåŸŸè¿›è¡Œç‰¹å®šç»„ä»¶çš„æµ‹è¯•å’Œè°ƒè¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿™é‡Œå¯ä»¥æ·»åŠ æ‚¨éœ€è¦çš„è‡ªå®šä¹‰æµ‹è¯•ä»£ç \n",
    "# ä¾‹å¦‚ï¼Œæµ‹è¯•ç‰¹å®šçš„æ¨¡å‹ã€æ•°æ®é›†æˆ–ä»»åŠ¡\n",
    "\n",
    "# ç¤ºä¾‹ï¼šæµ‹è¯•ç‰¹å®šçš„æŸå¤±å‡½æ•°\n",
    "try:\n",
    "    from src.utils.loss_utils import FocalLoss\n",
    "    \n",
    "    # åˆ›å»ºæŸå¤±å‡½æ•°\n",
    "    focal_loss = FocalLoss(gamma=2.0)\n",
    "    print(f\"âœ… æˆåŠŸåˆ›å»ºFocalLoss: {focal_loss}\")\n",
    "    \n",
    "    # åˆ›å»ºæµ‹è¯•è¾“å…¥\n",
    "    inputs = torch.randn(8, 3)  # æ‰¹å¤§å°ä¸º8ï¼Œ3ä¸ªç±»åˆ«\n",
    "    targets = torch.tensor([0, 1, 2, 1, 0, 2, 1, 0])  # ç›®æ ‡ç±»åˆ«\n",
    "    \n",
    "    # è®¡ç®—æŸå¤±\n",
    "    loss = focal_loss(inputs, targets)\n",
    "    print(f\"è®¡ç®—çš„æŸå¤±å€¼: {loss.item()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"æµ‹è¯•å¤±è´¥: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
