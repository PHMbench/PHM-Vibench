{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9deddd9f",
   "metadata": {},
   "source": [
    "# 基本测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85909f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset as HFDataset\n",
    "from datasets import ClassLabel, Features, Sequence, Value, concatenate_datasets, interleave_datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68c3634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据集信息:\n",
      "- dataset_A_engine1: 100 个样本, 特征: {'data': Sequence(feature=Value(dtype='float32', id=None), length=10, id=None), 'label': ClassLabel(names=['normal', 'fault_type_1', 'fault_type_2'], id=None)}\n",
      "- dataset_B_engine2: 50 个样本, 特征: {'data': Sequence(feature=Value(dtype='float32', id=None), length=10, id=None), 'label': ClassLabel(names=['normal', 'fault_type_1', 'fault_type_2'], id=None)}\n",
      "- dataset_C_engine3: 75 个样本, 特征: {'data': Sequence(feature=Value(dtype='float32', id=None), length=10, id=None), 'label': ClassLabel(names=['normal', 'fault_type_1', 'fault_type_2'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 创建或加载单独的 Hugging Face Dataset 对象 ---\n",
    "# 通常，你会从文件（CSV, JSON, Parquet等）或 Hugging Face Hub 加载数据。\n",
    "# 为了示例，我们从 Python 字典创建一些虚拟数据。\n",
    "\n",
    "# 定义所有数据集共享的特征结构\n",
    "# 假设 'data' 是一个长度为10的时间序列，'label' 是分类标签\n",
    "features = Features({\n",
    "    'data': Sequence(Value(dtype='float32'), length=10), # 或者 length=-1 表示可变长度\n",
    "    'label': ClassLabel(names=['normal', 'fault_type_1', 'fault_type_2']) # 示例标签\n",
    "})\n",
    "\n",
    "# 创建示例数据集字典\n",
    "hf_datasets_dict = {\n",
    "    'dataset_A_engine1': HFDataset.from_dict({\n",
    "        'data': np.random.rand(100, 10).astype(np.float32), # 100个样本，每个样本10个特征点\n",
    "        'label': np.random.randint(0, 3, 100) # 100个标签\n",
    "    }, features=features),\n",
    "    'dataset_B_engine2': HFDataset.from_dict({\n",
    "        'data': np.random.rand(50, 10).astype(np.float32), # 50个样本\n",
    "        'label': np.random.randint(0, 3, 50)\n",
    "    }, features=features),\n",
    "    'dataset_C_engine3': HFDataset.from_dict({\n",
    "        'data': np.random.rand(75, 10).astype(np.float32), # 75个样本\n",
    "        'label': np.random.randint(0, 3, 75)\n",
    "    }, features=features),\n",
    "}\n",
    "\n",
    "print(\"原始数据集信息:\")\n",
    "for name, ds in hf_datasets_dict.items():\n",
    "    print(f\"- {name}: {len(ds)} 个样本, 特征: {ds.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f61975cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 11046.94 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 9119.24 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 11478.66 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "添加 'dataset_id' 后的第一个数据集的第一个样本:\n",
      "{'data': [0.23012354969978333, 0.5572117567062378, 0.1324310153722763, 0.8973038196563721, 0.0951453298330307, 0.678874671459198, 0.24881014227867126, 0.7016008496284485, 0.010452426970005035, 0.4540610909461975], 'label': 2, 'dataset_id': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 为每个数据集的样本添加 'dataset_id' ---\n",
    "# 这在后续分析或需要知道样本来源时非常有用。\n",
    "processed_hf_datasets_list = []\n",
    "dataset_id_map = {} # 用于将整数ID映射回名称（如果需要）\n",
    "\n",
    "for i, (ds_name, hf_ds) in enumerate(hf_datasets_dict.items()):\n",
    "    dataset_id_map[i] = ds_name\n",
    "    def add_dataset_id(example, idx, current_ds_id=i): # 使用默认参数捕获当前的ds_id\n",
    "        example['dataset_id'] = current_ds_id # 添加整数ID\n",
    "        # 你也可以直接添加 ds_name，但整数ID有时更方便处理\n",
    "        # example['dataset_name'] = ds_name\n",
    "        return example\n",
    "\n",
    "    # .map() 是一个非常强大的函数，可以对数据集中的每个样本应用一个函数\n",
    "    # with_indices=True 允许我们的函数接收样本的索引\n",
    "    processed_ds = hf_ds.map(add_dataset_id, with_indices=True, batched=False)\n",
    "    processed_hf_datasets_list.append(processed_ds)\n",
    "\n",
    "print(\"\\n添加 'dataset_id' 后的第一个数据集的第一个样本:\")\n",
    "print(processed_hf_datasets_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a442f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "合并后的数据集 (val/test模式):\n",
      "- 总长度: 225\n",
      "- 特征: {'data': Sequence(feature=Value(dtype='float32', id=None), length=10, id=None), 'label': ClassLabel(names=['normal', 'fault_type_1', 'fault_type_2'], id=None), 'dataset_id': Value(dtype='int64', id=None)}\n",
      "- 第1个样本: {'data': [0.23012354969978333, 0.5572117567062378, 0.1324310153722763, 0.8973038196563721, 0.0951453298330307, 0.678874671459198, 0.24881014227867126, 0.7016008496284485, 0.010452426970005035, 0.4540610909461975], 'label': 2, 'dataset_id': 0}\n",
      "- 第100个样本 (原ds_A结束，ds_B开始处): {'data': [0.9916283488273621, 0.6911718845367432, 0.3121793866157532, 0.45244041085243225, 0.16339486837387085, 0.9700718522071838, 0.6193543076515198, 0.3230269253253937, 0.2161582112312317, 0.8906893134117126], 'label': 2, 'dataset_id': 1}\n",
      "- 最后一个样本: {'data': [0.5071959495544434, 0.8304657936096191, 0.41147640347480774, 0.7734295725822449, 0.9347462058067322, 0.698542058467865, 0.25387296080589294, 0.6484841704368591, 0.017896374687552452, 0.6011180877685547], 'label': 2, 'dataset_id': 2}\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 用于 'val'/'test' 模式: 顺序合并数据集 ---\n",
    "# `concatenate_datasets` 将按列表顺序将数据集首尾相连。\n",
    "concatenated_dataset = concatenate_datasets(processed_hf_datasets_list)\n",
    "\n",
    "print(f\"\\n合并后的数据集 (val/test模式):\")\n",
    "print(f\"- 总长度: {len(concatenated_dataset)}\")\n",
    "print(f\"- 特征: {concatenated_dataset.features}\") # 特征应该包含新的 'dataset_id'\n",
    "# 验证前几个和后几个样本的 dataset_id\n",
    "print(f\"- 第1个样本: {concatenated_dataset[0]}\")\n",
    "print(f\"- 第100个样本 (原ds_A结束，ds_B开始处): {concatenated_dataset[100]}\") # 假设ds_A有100个\n",
    "print(f\"- 最后一个样本: {concatenated_dataset[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdff5362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "交错并平衡采样的数据集 (train模式 - all_exhausted):\n",
      "从平衡的训练数据集中获取前10个样本:\n",
      "  Sample 0: data[0]=0.76, label=1, dataset_id=2 (dataset_C_engine3)\n",
      "  Sample 1: data[0]=0.99, label=2, dataset_id=1 (dataset_B_engine2)\n",
      "  Sample 2: data[0]=0.01, label=2, dataset_id=2 (dataset_C_engine3)\n",
      "  Sample 3: data[0]=0.40, label=1, dataset_id=2 (dataset_C_engine3)\n",
      "  Sample 4: data[0]=0.23, label=2, dataset_id=0 (dataset_A_engine1)\n",
      "  Sample 5: data[0]=0.72, label=2, dataset_id=2 (dataset_C_engine3)\n",
      "  Sample 6: data[0]=0.02, label=2, dataset_id=2 (dataset_C_engine3)\n",
      "  Sample 7: data[0]=0.21, label=2, dataset_id=2 (dataset_C_engine3)\n",
      "  Sample 8: data[0]=0.46, label=2, dataset_id=0 (dataset_A_engine1)\n",
      "  Sample 9: data[0]=0.03, label=1, dataset_id=1 (dataset_B_engine2)\n",
      "  Sample 10: data[0]=0.08, label=1, dataset_id=1 (dataset_B_engine2)\n",
      "  Sample 11: data[0]=0.15, label=1, dataset_id=2 (dataset_C_engine3)\n",
      "  Sample 12: data[0]=0.62, label=1, dataset_id=1 (dataset_B_engine2)\n",
      "  Sample 13: data[0]=0.62, label=1, dataset_id=2 (dataset_C_engine3)\n",
      "  Sample 14: data[0]=0.81, label=0, dataset_id=1 (dataset_B_engine2)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 用于 'train' 模式: 交错/平衡采样数据集 ---\n",
    "# `interleave_datasets` 允许你从多个数据集中采样，可以控制采样概率和停止策略。\n",
    "\n",
    "# 4a. 等概率采样，当任一数据集耗尽时停止 (可能不是最佳平衡)\n",
    "# interleaved_train_dataset_simple = interleave_datasets(\n",
    "#     processed_hf_datasets_list,\n",
    "#     stopping_strategy='first_exhausted' # 'first_exhausted' 或 'all_exhausted'\n",
    "# )\n",
    "# print(f\"\\n交错数据集 (train模式 - first_exhausted):\")\n",
    "# print(f\"- 长度: {len(interleaved_train_dataset_simple)}\") # 长度会是 min_len * num_datasets\n",
    "# print(f\"- 第1个样本: {interleaved_train_dataset_simple[0]}\")\n",
    "\n",
    "# 4b. 等概率采样，当所有数据集都至少遍历完一次长的那个时（通过重采样短的）\n",
    "# `stopping_strategy='all_exhausted'` 会从较短的数据集中重复采样，直到最长的数据集耗尽。\n",
    "# 这与你之前 `BalancedDataset` 中训练模式的逻辑相似，即小的被过采样。\n",
    "num_train_datasets = len(processed_hf_datasets_list)\n",
    "equal_probabilities = [1.0 / num_train_datasets] * num_train_datasets\n",
    "\n",
    "interleaved_balanced_train_dataset = interleave_datasets(\n",
    "    processed_hf_datasets_list,\n",
    "    probabilities=equal_probabilities,\n",
    "    stopping_strategy='all_exhausted', # 关键参数，实现对小数据集的过采样\n",
    "    seed=42 # 为了可复现性\n",
    ")\n",
    "# 注意: `interleave_datasets` 返回的 `IterableDataset` 默认可能没有 `__len__`。\n",
    "# 如果你需要一个固定的epoch长度，通常是在训练循环中控制迭代的步数，\n",
    "# 或者你可以用 `.take(N)` 来创建一个固定大小的数据集视图（但这会创建一个新的IterableDataset）。\n",
    "# 对于 PyTorch DataLoader，IterableDataset 不需要实现 `__len__`。\n",
    "print(f\"\\n交错并平衡采样的数据集 (train模式 - all_exhausted):\")\n",
    "# print(f\"- 长度: 通常 IterableDataset 没有固定长度，除非被包装或显式定义\")\n",
    "# 我们可以尝试迭代一些样本来看看效果\n",
    "print(\"从平衡的训练数据集中获取前10个样本:\")\n",
    "for i, sample in enumerate(interleaved_balanced_train_dataset.take(15)): # .take(N) 获取N个样本\n",
    "    print(f\"  Sample {i}: data[0]={sample['data'][0]:.2f}, label={sample['label']}, dataset_id={sample['dataset_id']} ({dataset_id_map[sample['dataset_id']]})\")\n",
    "    if i >= 14:\n",
    "        break\n",
    "\n",
    "# 4c. 根据数据集大小调整采样概率 (可选，如果你不希望完全平衡，而是按比例)\n",
    "# total_samples = sum(len(ds) for ds in processed_hf_datasets_list)\n",
    "# size_based_probabilities = [len(ds) / total_samples for ds in processed_hf_datasets_list]\n",
    "# interleaved_weighted_train_dataset = interleave_datasets(\n",
    "#     processed_hf_datasets_list,\n",
    "#     probabilities=size_based_probabilities,\n",
    "#     stopping_strategy='all_exhausted',\n",
    "#     seed=42\n",
    "# )\n",
    "# print(f\"\\n按数据集大小加权交错采样的数据集 (train模式 - all_exhausted):\")\n",
    "# for i, sample in enumerate(interleaved_weighted_train_dataset.take(10)):\n",
    "#     print(f\"  Sample {i}: dataset_id={sample['dataset_id']}\")\n",
    "#     if i >= 9:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e68121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用 PyTorch DataLoader:\n",
      "从 val_dataloader_hf 获取一个批次:\n",
      "- 特征批次形状: torch.Size([32, 10])\n",
      "- 标签批次形状: torch.Size([32])\n",
      "- Dataset ID 批次: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "- 第一个样本的 Dataset ID (来自map): dataset_A_engine1\n",
      "\n",
      "从 train_dataloader_hf 获取一个批次:\n",
      "Step 0:\n",
      "- 特征批次形状: torch.Size([32, 10])\n",
      "- 标签批次形状: torch.Size([32])\n",
      "- Dataset ID 批次: tensor([2, 1, 2, 2, 0, 2, 2, 2, 0, 1, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 2,\n",
      "        2, 0, 1, 0, 0, 2, 2, 2])\n",
      "- 第一个样本的 Dataset ID (来自map): dataset_C_engine3\n",
      "Step 1:\n",
      "- 特征批次形状: torch.Size([32, 10])\n",
      "- 标签批次形状: torch.Size([32])\n",
      "- Dataset ID 批次: tensor([0, 1, 1, 0, 0, 1, 0, 2, 1, 2, 2, 0, 2, 2, 1, 0, 2, 0, 0, 0, 2, 1, 2, 2,\n",
      "        1, 1, 0, 0, 2, 1, 1, 2])\n",
      "Step 2:\n",
      "- 特征批次形状: torch.Size([32, 10])\n",
      "- 标签批次形状: torch.Size([32])\n",
      "- Dataset ID 批次: tensor([1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 1, 1, 2, 1, 1, 2, 0, 0, 0, 2, 1,\n",
      "        0, 1, 0, 2, 1, 1, 0, 1])\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 设置数据集格式以便与 PyTorch 一起使用 ---\n",
    "# 这会告诉数据集在被索引时返回 PyTorch 张量而不是 Python列表/Numpy数组。\n",
    "concatenated_dataset.set_format(type='torch', columns=['data', 'label', 'dataset_id'])\n",
    "# 对于 IterableDataset (interleave_datasets 的输出)，set_format 的行为略有不同，\n",
    "# 它通常在数据流经它时应用转换。\n",
    "# 如果直接将interleaved_balanced_train_dataset传递给PyTorch DataLoader，\n",
    "# 并且其内部数据集已经知道如何输出正确类型（例如，如果它们来自 `Dataset.from_dict` 时 NumPy 数组可以被 PyTorch 直接处理），\n",
    "# 或者在 `.map` 中转换为 PyTorch 张量，那么可能不需要显式的 `set_format`。\n",
    "# 但通常最好还是加上，或者在 collate_fn 中确保类型正确。\n",
    "# Hugging Face `datasets` 通常能很好地与 PyTorch DataLoader 配合。\n",
    "# 为了安全起见，我们可以这样做：\n",
    "interleaved_balanced_train_dataset = interleaved_balanced_train_dataset.with_format(\"torch\")\n",
    "\n",
    "\n",
    "# --- 6. 与 PyTorch DataLoader 一起使用 ---\n",
    "\n",
    "# 验证/测试 DataLoader\n",
    "val_dataloader_hf = DataLoader(\n",
    "    concatenated_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False # 验证/测试时通常不打乱\n",
    ")\n",
    "\n",
    "# 训练 DataLoader\n",
    "# 注意：对于 IterableDataset，shuffle 参数在 DataLoader 中无效。\n",
    "# shuffle 应该在 `interleave_datasets` (通过 seed 和内部缓冲) 或 Dataset 本身层面处理。\n",
    "# `interleave_datasets` 内部有洗牌逻辑，可以通过 `buffer_size` 参数控制其洗牌效果。\n",
    "train_dataloader_hf = DataLoader(\n",
    "    interleaved_balanced_train_dataset,\n",
    "    batch_size=32\n",
    "    # num_workers > 0 在 IterableDataset 上的支持可能因 datasets 版本和具体情况而异，\n",
    "    # 需要测试。对于简单的 IterableDataset，num_workers=0 通常最安全。\n",
    "    # 对于复杂的预处理，你可能希望在 .map() 中进行，或者使用 .shard() 来手动分配给 worker。\n",
    ")\n",
    "\n",
    "print(f\"\\n使用 PyTorch DataLoader:\")\n",
    "print(\"从 val_dataloader_hf 获取一个批次:\")\n",
    "for batch in val_dataloader_hf:\n",
    "    print(f\"- 特征批次形状: {batch['data'].shape}\")\n",
    "    print(f\"- 标签批次形状: {batch['label'].shape}\")\n",
    "    print(f\"- Dataset ID 批次: {batch['dataset_id']}\")\n",
    "    print(f\"- 第一个样本的 Dataset ID (来自map): {dataset_id_map[batch['dataset_id'][0].item()]}\")\n",
    "    break # 只看一个批次\n",
    "\n",
    "print(\"\\n从 train_dataloader_hf 获取一个批次:\")\n",
    "# 注意: 训练循环通常基于步数，而不是 epoch (因为 IterableDataset 可能没有固定长度)\n",
    "num_train_steps = 10 # 假设我们训练10步\n",
    "for step, batch in enumerate(train_dataloader_hf):\n",
    "    if step >= num_train_steps and num_train_steps > 0 : # 限制迭代次数，因为IterableDataset可能是无限的\n",
    "        break\n",
    "    print(f\"Step {step}:\")\n",
    "    print(f\"- 特征批次形状: {batch['data'].shape}\")\n",
    "    print(f\"- 标签批次形状: {batch['label'].shape}\")\n",
    "    print(f\"- Dataset ID 批次: {batch['dataset_id']}\")\n",
    "    if step == 0: # 只打印第一个批次的ID映射\n",
    "         print(f\"- 第一个样本的 Dataset ID (来自map): {dataset_id_map[batch['dataset_id'][0].item()]}\")\n",
    "    if step >= 2: # 打印少量批次用于演示\n",
    "        break \n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c2dd5",
   "metadata": {},
   "source": [
    "# IdIncludedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# 自定义包装类 IdIncludedDataset\n",
    "# =====================================================================================\n",
    "class IdIncludedDataset(Dataset):\n",
    "    def __init__(self, dataset_dict):\n",
    "        \"\"\"\n",
    "        包装一个 PyTorch Dataset 字典，使得每个样本都包含其原始ID。\n",
    "\n",
    "        Args:\n",
    "            dataset_dict (dict): 一个字典，键是字符串ID，值是 PyTorch Dataset 对象。\n",
    "                                 例如：{'id1': train_dataset_for_id1, 'id2': train_dataset_for_id2}\n",
    "                                 其中 train_dataset_for_id1 等实例的 __getitem__ 返回 (x, y)。\n",
    "        \"\"\"\n",
    "        self.dataset_dict_refs = dataset_dict # 保存对原始数据集字典的引用\n",
    "        self.flat_sample_map = [] # 用于全局索引到 (id, 原始数据集中的索引) 的映射\n",
    "\n",
    "        for id_str, original_dataset in self.dataset_dict_refs.items():\n",
    "            if original_dataset is None:\n",
    "                print(f\"警告: ID '{id_str}' 对应的 dataset 为 None，已跳过。\")\n",
    "                continue\n",
    "            if len(original_dataset) == 0:\n",
    "                print(f\"警告: ID '{id_str}' 对应的 dataset 为空，已跳过。\")\n",
    "                continue\n",
    "            \n",
    "            for i in range(len(original_dataset)):\n",
    "                self.flat_sample_map.append({'id': id_str, 'original_idx': i})\n",
    "        \n",
    "        self._total_samples = len(self.flat_sample_map)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回所有原始数据集中样本的总数。\n",
    "        \"\"\"\n",
    "        return self._total_samples\n",
    "\n",
    "    def __getitem__(self, global_idx):\n",
    "        \"\"\"\n",
    "        根据全局索引获取样本，并返回 (id, (x, y))。\n",
    "\n",
    "        Args:\n",
    "            global_idx (int): 全局样本索引。\n",
    "\n",
    "        Returns:\n",
    "            tuple: (str, tuple), 即 (id, (x, y))\n",
    "                   其中 x 是特征数据, y 是标签。\n",
    "        \"\"\"\n",
    "        if global_idx < 0 or global_idx >= self._total_samples:\n",
    "            raise IndexError(f\"全局索引 {global_idx} 超出范围 (总样本数: {self._total_samples})\")\n",
    "\n",
    "        sample_info = self.flat_sample_map[global_idx]\n",
    "        original_id = sample_info['id']\n",
    "        idx_in_original_dataset = sample_info['original_idx']\n",
    "\n",
    "        # 从原始数据集中获取 (x, y)\n",
    "        original_dataset_instance = self.dataset_dict_refs[original_id]\n",
    "        x, y = original_dataset_instance[idx_in_original_dataset]\n",
    "        \n",
    "        return  (x, y),original_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5658cf",
   "metadata": {},
   "source": [
    "# dataset 包装器 + sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8086d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,Sampler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0eed0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Default_dataset(Dataset): # THU_006or018_basic\n",
    "    def __init__(self, data, metadata, args_data, args_task, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        简化的数据集类\n",
    "        Args:\n",
    "            data: 输入数据，可能是单一ID的数据 [L, C]，或字典格式 {ID: 数据}\n",
    "            metadata: 数据元信息，格式为 {ID: {字段: 值}} 字典\n",
    "            args_data: 数据处理参数\n",
    "            args_task: 任务参数\n",
    "            mode: 数据模式，可选 \"train\", \"valid\", \"test\"\n",
    "        \"\"\"\n",
    "        self.key = list(data.keys())[0]\n",
    "        self.data = data[self.key]  # 取出第一个键的数据\n",
    "        self.metadata = metadata\n",
    "        self.args_data = args_data\n",
    "        self.mode = mode\n",
    "        \n",
    "        # 数据处理参数\n",
    "        self.window_size = args_data.window_size\n",
    "        self.stride = args_data.stride\n",
    "        self.train_ratio = args_data.train_ratio\n",
    "        \n",
    "        # 数据预处理\n",
    "        self.processed_data = []  # 存储处理后的样本\n",
    "                \n",
    "        # 处理数据\n",
    "        self.prepare_data()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        准备数据：将原始数据按窗口大小和步长分割成样本\n",
    "        如果mode是train或valid，则划分数据集\n",
    "        \"\"\"\n",
    "        self._process_single_data(self.data)\n",
    "\n",
    "        # 如果是train或valid模式，进行数据集划分\n",
    "        if self.mode in [\"train\", \"valid\"]:\n",
    "            self._split_data_for_mode()\n",
    "            \n",
    "        self.total_samples = len(self.processed_data) # L'\n",
    "        self.label = self.metadata[self.key][\"Label\"]\n",
    "    \n",
    "    def _process_single_data(self, sample_data):\n",
    "        \"\"\"\n",
    "        处理单个数据样本，应用滑动窗口\n",
    "        \"\"\"\n",
    "        data_length = len(sample_data)\n",
    "        num_samples = max(0, (data_length - self.window_size) // self.stride + 1)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            start_idx = i * self.stride\n",
    "            end_idx = start_idx + self.window_size\n",
    "            \n",
    "            self.processed_data.append(sample_data[start_idx:end_idx])\n",
    "\n",
    "    def _split_data_for_mode(self):\n",
    "        \"\"\"\n",
    "        根据当前模式划分数据集\n",
    "        \"\"\"\n",
    "        if not self.processed_data:\n",
    "            return\n",
    "            \n",
    "        # 计算划分点\n",
    "        total_samples = len(self.processed_data)\n",
    "        train_size = int(self.train_ratio * total_samples)\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            # 训练模式只保留训练数据\n",
    "            self.processed_data = self.processed_data[:train_size]\n",
    "        elif self.mode == \"valid\":\n",
    "            # 验证模式只保留验证数据\n",
    "            self.processed_data = self.processed_data[train_size:]\n",
    "        self.total_samples = len(self.processed_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集长度\"\"\"\n",
    "        return self.total_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"获取指定索引的样本\"\"\"\n",
    "        if idx >= self.total_samples:\n",
    "            raise IndexError(f\"索引 {idx} 超出范围\")\n",
    "        \n",
    "        sample = self.processed_data[idx]\n",
    "\n",
    "        \n",
    "        return sample, self.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a7811",
   "metadata": {},
   "source": [
    "## sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c100661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedIdSampler(Sampler):\n",
    "    def __init__(self, data_source: IdIncludedDataset, common_samples_per_id=None, shuffle_within_id=True, shuffle_all=True):\n",
    "        \"\"\"\n",
    "        Sampler 实现对不同原始数据集(ID)的平衡加载。\n",
    "\n",
    "        Args:\n",
    "            data_source (IdIncludedDataset): 被包装的数据集，必须是 IdIncludedDataset 类型。\n",
    "            common_samples_per_id (int, optional): \n",
    "                每个ID在一个epoch中被采样的目标次数。\n",
    "                如果为 None, 则默认为样本量最大的ID的样本数 (即对小ID进行过采样)。\n",
    "            shuffle_within_id (bool): 是否在为每个ID选择样本时进行随机选择。\n",
    "            shuffle_all (bool): 是否在最后将所有选出的索引进行全局随机打乱。\n",
    "        \"\"\"\n",
    "        super().__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        self.shuffle_within_id = shuffle_within_id\n",
    "        self.shuffle_all = shuffle_all\n",
    "\n",
    "        # 1. 按ID对全局索引进行分组\n",
    "        self.indices_per_id = {}\n",
    "        if not hasattr(self.data_source, 'flat_sample_map'):\n",
    "            raise ValueError(\"data_source 必须是 IdIncludedDataset 的实例，或具有 'flat_sample_map' 属性。\")\n",
    "\n",
    "        for global_idx, sample_info in enumerate(self.data_source.flat_sample_map):\n",
    "            original_id = sample_info['id']\n",
    "            if original_id not in self.indices_per_id:\n",
    "                self.indices_per_id[original_id] = []\n",
    "            self.indices_per_id[original_id].append(global_idx)\n",
    "\n",
    "        self.id_list = list(self.indices_per_id.keys())\n",
    "        if not self.id_list: # 如果没有任何有效的ID\n",
    "            self._num_samples_epoch = 0\n",
    "            self.target_samples_per_id = 0\n",
    "            return\n",
    "\n",
    "        # 2. 确定每个ID的目标采样数\n",
    "        num_actual_samples_per_id = {id_str: len(indices) for id_str, indices in self.indices_per_id.items()}\n",
    "        \n",
    "        if common_samples_per_id is None:\n",
    "            # 默认目标是最大ID的样本量\n",
    "            if not num_actual_samples_per_id:\n",
    "                 self.target_samples_per_id = 0 # 防止空字典\n",
    "            else:\n",
    "                 self.target_samples_per_id = max(num_actual_samples_per_id.values()) if num_actual_samples_per_id else 0\n",
    "        else:\n",
    "            self.target_samples_per_id = common_samples_per_id\n",
    "        \n",
    "        # 3. 计算一个epoch的总样本数\n",
    "        self._num_samples_epoch = self.target_samples_per_id * len(self.id_list)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if not self.id_list or self.target_samples_per_id == 0:\n",
    "            return iter([])\n",
    "\n",
    "        all_epoch_indices = []\n",
    "        for id_str in self.id_list:\n",
    "            id_specific_global_indices = self.indices_per_id[id_str]\n",
    "            num_actual_id_samples = len(id_specific_global_indices)\n",
    "\n",
    "            if num_actual_id_samples == 0:\n",
    "                continue # 这个ID没有样本\n",
    "\n",
    "            if num_actual_id_samples < self.target_samples_per_id:\n",
    "                # 过采样: 需要重复采样\n",
    "                # random.choices 实现带放回采样\n",
    "                chosen_for_id = random.choices(id_specific_global_indices, k=self.target_samples_per_id)\n",
    "            elif num_actual_id_samples > self.target_samples_per_id:\n",
    "                # 欠采样或精确采样: 不带放回采样\n",
    "                if self.shuffle_within_id:\n",
    "                    chosen_for_id = random.sample(id_specific_global_indices, k=self.target_samples_per_id)\n",
    "                else: # 按顺序取前k个\n",
    "                    chosen_for_id = id_specific_global_indices[:self.target_samples_per_id]\n",
    "            else: # num_actual_id_samples == self.target_samples_per_id\n",
    "                # 样本数正好等于目标数\n",
    "                if self.shuffle_within_id:\n",
    "                    chosen_for_id = random.sample(id_specific_global_indices, k=num_actual_id_samples)\n",
    "                else:\n",
    "                    chosen_for_id = list(id_specific_global_indices) # 复制列表\n",
    "            \n",
    "            all_epoch_indices.extend(chosen_for_id)\n",
    "\n",
    "        if self.shuffle_all:\n",
    "            random.shuffle(all_epoch_indices) # 打乱所有ID组合后的总索引列表\n",
    "        \n",
    "        return iter(all_epoch_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._num_samples_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dd3e21",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed1e1240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告: ID 'id_C' 对应的 dataset 为空，已跳过。\n",
      "IdIncludedDataset 总样本数: 11\n",
      "BalancedIdSampler (过采样至最大) epoch 长度: 16\n",
      "BalancedIdSampler (每个ID采样5次) epoch 长度: 10\n",
      "\n",
      "使用 sampler_oversample (过采样至最大ID的样本数):\n",
      "  Batch 0: IDs: ('id_A', 'id_B', 'id_A', 'id_B'), X shape: torch.Size([4, 5, 3]), Y shape: torch.Size([4])\n",
      "  Batch 1: IDs: ('id_B', 'id_B', 'id_A', 'id_B'), X shape: torch.Size([4, 5, 3]), Y shape: torch.Size([4])\n",
      "  一个 epoch 中各ID的采样次数 (sampler_oversample): {'id_A': 8, 'id_B': 8}\n",
      "\n",
      "使用 sampler_common_count (每个ID采样5次):\n",
      "  一个 epoch 中各ID的采样次数 (sampler_common_count): {'id_A': 5, 'id_B': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n"
     ]
    }
   ],
   "source": [
    "# --- 模拟参数和数据 ---\n",
    "class ArgsSim:\n",
    "    def get(self, key, default):\n",
    "        return getattr(self, key, default)\n",
    "\n",
    "args_data_sim = ArgsSim()\n",
    "args_data_sim.window_size = 5\n",
    "args_data_sim.train_ratio = 0.8\n",
    "args_data_sim.stride = 5\n",
    "\n",
    "args_task_sim = ArgsSim() # dummy\n",
    "\n",
    "# 模拟不同大小的原始数据集\n",
    "dataset_dict_sim = {\n",
    "    'id_A': Default_dataset(\n",
    "        data={'id_A': np.random.rand(20, 3)}, # 初始20个“时间点” -> 4个原始样本 -> 3个训练样本\n",
    "        metadata={'id_A': {'Label': 0}},\n",
    "        args_data=args_data_sim, args_task=args_task_sim, mode=\"train\"\n",
    "    ),\n",
    "    'id_B': Default_dataset(\n",
    "        data={'id_B': np.random.rand(50, 3)}, # 初始50个“时间点” -> 10个原始样本 -> 8个训练样本\n",
    "        metadata={'id_B': {'Label': 1}},\n",
    "        args_data=args_data_sim, args_task=args_task_sim, mode=\"train\"\n",
    "    ),\n",
    "    'id_C': Default_dataset( # 一个可能为空的数据集或处理后样本很少\n",
    "        data={'id_C': np.random.rand(8, 3)}, # 初始8个“时间点” -> 1个原始样本 -> 0个训练样本 (如果train_ratio后为0)\n",
    "                                             # 我们在 Default_dataset 中添加了确保至少1个样本的逻辑（如果可能）\n",
    "        metadata={'id_C': {'Label': 2}},      # 假设处理后 id_C 有 1 个训练样本\n",
    "        args_data=args_data_sim, args_task=args_task_sim, mode=\"train\"\n",
    "    )\n",
    "}\n",
    "# 预期 id_A 样本数: int(0.8 * (20//5)) = 3\n",
    "# 预期 id_B 样本数: int(0.8 * (50//5)) = 8\n",
    "# 预期 id_C 样本数: int(0.8 * (8//5)) = int(0.8*1) = 0, 但Default_dataset简化版可能产生1个 (如果原始数据足够)\n",
    "# 假设 Default_dataset 简化版处理后：id_A: 3, id_B: 8, id_C: 1\n",
    "\n",
    "# 包装数据集\n",
    "wrapped_train_dataset = IdIncludedDataset(dataset_dict_sim)\n",
    "print(f\"IdIncludedDataset 总样本数: {len(wrapped_train_dataset)}\") # 应该为 3 + 8 + 1 = 12\n",
    "\n",
    "# --- 使用 BalancedIdSampler ---\n",
    "\n",
    "# 策略1: 过采样小的ID，使得所有ID的样本量都与最大的ID相同 (id_B 有8个样本)\n",
    "# target_samples_per_id 会是 8。epoch 总样本数 = 8 * 3 = 24\n",
    "sampler_oversample = BalancedIdSampler(wrapped_train_dataset, common_samples_per_id=None) \n",
    "print(f\"BalancedIdSampler (过采样至最大) epoch 长度: {len(sampler_oversample)}\")\n",
    "\n",
    "# 策略2: 为每个ID指定一个共同的采样数，比如每个ID采样5次\n",
    "# epoch 总样本数 = 5 * 3 = 15\n",
    "sampler_common_count = BalancedIdSampler(wrapped_train_dataset, common_samples_per_id=5)\n",
    "print(f\"BalancedIdSampler (每个ID采样5次) epoch 长度: {len(sampler_common_count)}\")\n",
    "\n",
    "\n",
    "# 创建 DataLoader\n",
    "print(\"\\n使用 sampler_oversample (过采样至最大ID的样本数):\")\n",
    "balanced_dataloader_1 = DataLoader(\n",
    "    wrapped_train_dataset,\n",
    "    batch_size=4, # 例如 batch_size 为 4\n",
    "    sampler=sampler_oversample,\n",
    "    # collate_fn=... # 如果需要特殊处理批次结构\n",
    ")\n",
    "\n",
    "# 迭代 DataLoader\n",
    "id_counts_in_epoch_1 = {}\n",
    "if len(balanced_dataloader_1) > 0:\n",
    "    for batch_idx, data_batch in enumerate(balanced_dataloader_1):\n",
    "        ids_in_batch, (xs_in_batch, ys_in_batch) = data_batch\n",
    "        for id_val in ids_in_batch:\n",
    "            id_counts_in_epoch_1[id_val] = id_counts_in_epoch_1.get(id_val, 0) + 1\n",
    "        if batch_idx < 2: # 打印前2个批次的信息\n",
    "             print(f\"  Batch {batch_idx}: IDs: {ids_in_batch}, X shape: {xs_in_batch.shape}, Y shape: {ys_in_batch.shape}\")\n",
    "else:\n",
    "    print(\"  DataLoader 为空 (可能是因为 wrapped_train_dataset 为空)\")\n",
    "\n",
    "print(f\"  一个 epoch 中各ID的采样次数 (sampler_oversample): {id_counts_in_epoch_1}\")\n",
    "# 预期 id_A, id_B, id_C 的计数都接近或等于8\n",
    "\n",
    "print(\"\\n使用 sampler_common_count (每个ID采样5次):\")\n",
    "balanced_dataloader_2 = DataLoader(\n",
    "    wrapped_train_dataset,\n",
    "    batch_size=4,\n",
    "    sampler=sampler_common_count\n",
    ")\n",
    "id_counts_in_epoch_2 = {}\n",
    "if len(balanced_dataloader_2) > 0:\n",
    "    for batch_idx, data_batch in enumerate(balanced_dataloader_2):\n",
    "        ids_in_batch, (xs_in_batch, ys_in_batch) = data_batch\n",
    "        for id_val in ids_in_batch:\n",
    "            id_counts_in_epoch_2[id_val] = id_counts_in_epoch_2.get(id_val, 0) + 1\n",
    "else:\n",
    "    print(\"  DataLoader 为空\")\n",
    "print(f\"  一个 epoch 中各ID的采样次数 (sampler_common_count): {id_counts_in_epoch_2}\")\n",
    "# 预期 id_A, id_B, id_C 的计数都接近或等于5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb0825",
   "metadata": {},
   "source": [
    "# HDF5 打开修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef2497d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= 测试HDF5文件: /home/user/data/PHMbenchdata/metadata_5_data.h5 =======\n",
      "文件大小: 0.00 MB\n",
      "文件权限: 664\n",
      "最后修改时间: Thu May 15 06:43:42 2025\n",
      "\n",
      "❌ 文件打开失败: Unable to synchronously open file (file signature not found)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_hdf5_file(file_path):\n",
    "    \"\"\"全面测试HDF5文件\n",
    "    \n",
    "    Args:\n",
    "        file_path: HDF5文件路径\n",
    "    \"\"\"\n",
    "    print(f\"======= 测试HDF5文件: {file_path} =======\")\n",
    "    \n",
    "    # 1. 检查文件是否存在\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"错误: 文件不存在: {file_path}\")\n",
    "        return False\n",
    "    \n",
    "    # 2. 检查文件元数据\n",
    "    file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "    print(f\"文件大小: {file_size:.2f} MB\")\n",
    "    print(f\"文件权限: {oct(os.stat(file_path).st_mode)[-3:]}\")\n",
    "    print(f\"最后修改时间: {time.ctime(os.path.getmtime(file_path))}\")\n",
    "    \n",
    "    # 3. 尝试打开文件\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as h5f:\n",
    "            # 获取文件结构\n",
    "            print(\"\\n文件结构:\")\n",
    "            keys = list(h5f.keys())\n",
    "            print(f\"根级键数量: {len(keys)}\")\n",
    "            \n",
    "            if len(keys) > 10:\n",
    "                print(f\"前10个键: {keys[:10]}\")\n",
    "                print(f\"...等 {len(keys)} 个键\")\n",
    "            else:\n",
    "                print(f\"所有键: {keys}\")\n",
    "            \n",
    "            # 检查随机样本数据\n",
    "            print(\"\\n随机样本检查:\")\n",
    "            sample_keys = np.random.choice(keys, min(5, len(keys)), replace=False)\n",
    "            for key in sample_keys:\n",
    "                try:\n",
    "                    data = h5f[key][:]\n",
    "                    print(f\"键 '{key}': 形状={data.shape}, 类型={data.dtype}, 均值={np.mean(data):.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"键 '{key}' 读取错误: {e}\")\n",
    "            \n",
    "            # 4. 检查文件访问性能\n",
    "            print(\"\\n文件访问性能测试:\")\n",
    "            start_time = time.time()\n",
    "            for key in tqdm(np.random.choice(keys, min(50, len(keys)), replace=False)):\n",
    "                _ = h5f[key][:]\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"读取50个随机样本耗时: {elapsed:.2f}秒\")\n",
    "            \n",
    "            # 5. 检查文件完整性 - 尝试读取所有键\n",
    "            print(\"\\n文件完整性测试:\")\n",
    "            corrupted_keys = []\n",
    "            for key in tqdm(keys):\n",
    "                try:\n",
    "                    _ = h5f[key].shape  # 只获取形状，不加载全部数据\n",
    "                except Exception as e:\n",
    "                    corrupted_keys.append((key, str(e)))\n",
    "            \n",
    "            if corrupted_keys:\n",
    "                print(f\"发现 {len(corrupted_keys)} 个损坏键:\")\n",
    "                for key, error in corrupted_keys[:10]:  # 只显示前10个\n",
    "                    print(f\"  - {key}: {error}\")\n",
    "            else:\n",
    "                print(\"所有键可正常访问，文件完整\")\n",
    "                \n",
    "        print(\"\\n✅ 文件测试完成: 可以正常打开和读取\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 文件打开失败: {e}\")\n",
    "        # 检查是否是文件锁定问题\n",
    "        if \"unable to lock file\" in str(e).lower():\n",
    "            print(\"\\n可能存在文件锁定问题，尝试以下解决方案:\")\n",
    "            print(\"1. 关闭所有可能使用此文件的进程\")\n",
    "            print(\"2. 检查是否有.lock文件，尝试删除: rm -f /home/user/data/PHMbenchdata/metadata_5_data.h5.lock\")\n",
    "            print(\"3. 如果在服务器环境，重启相关服务\")\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "# 设置文件路径\n",
    "hdf5_file = \"/home/user/data/PHMbenchdata/metadata_5_data.h5\"\n",
    "\n",
    "\n",
    "# 测试文件\n",
    "test_hdf5_file(hdf5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421de54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import time\n",
    "import subprocess\n",
    "import psutil\n",
    "import fcntl\n",
    "import errno\n",
    "from tqdm import tqdm\n",
    "\n",
    "def unlock_hdf5_file(file_path):\n",
    "    \"\"\"尝试多种方法解锁HDF5文件\n",
    "    \n",
    "    Args:\n",
    "        file_path: HDF5文件路径\n",
    "    \n",
    "    Returns:\n",
    "        bool: 是否成功解锁文件\n",
    "    \"\"\"\n",
    "    print(f\"======= 尝试解锁HDF5文件: {file_path} =======\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"错误: 文件不存在: {file_path}\")\n",
    "        return False\n",
    "    \n",
    "    # 步骤1: 检查锁文件并尝试删除\n",
    "    lock_file = f\"{file_path}.lock\"\n",
    "    if os.path.exists(lock_file):\n",
    "        print(f\"发现锁文件: {lock_file}\")\n",
    "        try:\n",
    "            os.remove(lock_file)\n",
    "            print(f\"已删除锁文件: {lock_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"无法删除锁文件: {e}\")\n",
    "    \n",
    "    # 步骤2: 查找可能持有文件的进程\n",
    "    print(\"\\n查找可能持有文件的进程...\")\n",
    "    found_processes = find_processes_using_file(file_path)\n",
    "    \n",
    "    if found_processes:\n",
    "        print(\"以下进程可能正在使用该文件:\")\n",
    "        for pid, proc_name in found_processes:\n",
    "            print(f\"PID: {pid}, 名称: {proc_name}\")\n",
    "        print(\"\\n您可以手动终止这些进程: kill -9 <PID>\")\n",
    "    else:\n",
    "        print(\"未找到正在使用此文件的进程\")\n",
    "    \n",
    "    # 步骤3: 尝试不同的打开模式和选项\n",
    "    print(\"\\n尝试不同的打开模式和选项...\")\n",
    "    \n",
    "    # 方法列表，从最不可能破坏数据的到最激进的\n",
    "    methods = [\n",
    "        {\"mode\": \"r\", \"driver\": None, \"swmr\": False, \"rdcc_nbytes\": 0, \"libver\": \"latest\", \"name\": \"只读模式\"},\n",
    "        {\"mode\": \"r\", \"driver\": \"sec2\", \"swmr\": False, \"rdcc_nbytes\": 0, \"libver\": \"latest\", \"name\": \"sec2驱动只读\"},\n",
    "        {\"mode\": \"r+\", \"driver\": \"sec2\", \"swmr\": False, \"rdcc_nbytes\": 0, \"libver\": \"latest\", \"name\": \"sec2驱动读写\"},\n",
    "        {\"mode\": \"a\", \"driver\": \"sec2\", \"swmr\": False, \"rdcc_nbytes\": 0, \"libver\": \"latest\", \"name\": \"sec2驱动追加\"},\n",
    "        {\"mode\": \"r\", \"driver\": \"stdio\", \"swmr\": False, \"rdcc_nbytes\": 0, \"libver\": \"latest\", \"name\": \"stdio驱动\"},\n",
    "        {\"mode\": \"r\", \"driver\": \"core\", \"swmr\": False, \"rdcc_nbytes\": 0, \"libver\": \"latest\", \"name\": \"core驱动(内存映射)\"},\n",
    "        {\"mode\": \"r\", \"driver\": None, \"swmr\": True, \"rdcc_nbytes\": 0, \"libver\": \"latest\", \"name\": \"SWMR模式\"}\n",
    "    ]\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\n尝试: {method['name']}\")\n",
    "        try:\n",
    "            with h5py.File(file_path, **{k: v for k, v in method.items() if k != 'name'}) as h5f:\n",
    "                print(f\"成功打开文件! 包含 {len(h5f.keys())} 个数据集\")\n",
    "                print(\"测试读取一个数据集...\")\n",
    "                if len(h5f.keys()) > 0:\n",
    "                    key = list(h5f.keys())[0]\n",
    "                    shape = h5f[key].shape\n",
    "                    print(f\"数据集 '{key}' 形状: {shape}\")\n",
    "                    \n",
    "                print(f\"✅ {method['name']} 模式测试成功\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 失败: {e}\")\n",
    "    \n",
    "    # 步骤4: 如果所有常规方法都失败，尝试创建文件副本\n",
    "    print(\"\\n尝试创建文件的副本...\")\n",
    "    try:\n",
    "        backup_file = f\"{file_path}.backup\"\n",
    "        print(f\"创建备份文件: {backup_file}\")\n",
    "        \n",
    "        # 使用系统命令复制文件\n",
    "        result = subprocess.run(['cp', file_path, backup_file], check=True)\n",
    "        print(\"文件复制成功!\")\n",
    "        \n",
    "        # 测试打开备份文件\n",
    "        print(\"尝试打开备份文件...\")\n",
    "        with h5py.File(backup_file, 'r') as h5f:\n",
    "            print(f\"成功打开备份文件! 包含 {len(h5f.keys())} 个数据集\")\n",
    "            \n",
    "        print(f\"\\n✅ 建议使用备份文件: {backup_file}\")\n",
    "        print(f\"您可以重命名备份文件: mv {backup_file} {file_path}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 创建备份失败: {e}\")\n",
    "    \n",
    "    # 步骤5: 提供最后的建议\n",
    "    print(\"\\n所有自动解锁方法均已失败，推荐以下手动操作:\")\n",
    "    print(\"1. 重启您的Python环境或Jupyter内核\")\n",
    "    print(\"2. 尝试重启系统\")\n",
    "    print(\"3. 使用系统工具查找持有文件的进程: lsof <文件路径>\")\n",
    "    print(\"4. 如果文件重要，请勿删除。联系系统管理员或HDF5专家\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "def find_processes_using_file(file_path):\n",
    "    \"\"\"查找可能正在使用指定文件的进程\n",
    "    \n",
    "    Args:\n",
    "        file_path: 文件路径\n",
    "        \n",
    "    Returns:\n",
    "        list: (pid, 进程名) 元组列表\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    # 尝试使用lsof命令\n",
    "    try:\n",
    "        output = subprocess.check_output(['lsof', file_path], universal_newlines=True)\n",
    "        lines = output.strip().split('\\n')[1:]  # 跳过标题行\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                pid = parts[1]\n",
    "                try:\n",
    "                    result.append((int(pid), parts[0]))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return result\n",
    "    except (subprocess.SubprocessError, FileNotFoundError):\n",
    "        pass\n",
    "    \n",
    "    # 如果lsof失败，使用psutil\n",
    "    try:\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'open_files']):\n",
    "            try:\n",
    "                for open_file in proc.info['open_files'] or []:\n",
    "                    if file_path in open_file.path:\n",
    "                        result.append((proc.info['pid'], proc.info['name']))\n",
    "                        break\n",
    "            except (psutil.AccessDenied, psutil.NoSuchProcess):\n",
    "                continue\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return result\n",
    "\n",
    "def test_file_after_unlock(file_path):\n",
    "    \"\"\"测试解锁后文件是否可用\n",
    "    \n",
    "    Args:\n",
    "        file_path: HDF5文件路径\n",
    "    \"\"\"\n",
    "    print(\"\\n===== 测试文件可用性 =====\")\n",
    "    \n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as h5f:\n",
    "            keys = list(h5f.keys())\n",
    "            print(f\"文件包含 {len(keys)} 个数据集\")\n",
    "            \n",
    "            if len(keys) > 0:\n",
    "                # 测试读取几个数据集\n",
    "                samples = min(5, len(keys))\n",
    "                for i, key in enumerate(np.random.choice(keys, samples, replace=False)):\n",
    "                    data = h5f[key][:]\n",
    "                    print(f\"数据集 {i+1}/{samples}: '{key}', 形状: {data.shape}, 类型: {data.dtype}\")\n",
    "                \n",
    "            print(\"\\n✅ 文件测试通过，可以正常使用\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 文件测试失败: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置文件路径\n",
    "    hdf5_file = \"/home/user/data/PHMbenchdata/metadata_5_data.h5\"\n",
    "    \n",
    "\n",
    "    \n",
    "    # 尝试解锁文件\n",
    "    if unlock_hdf5_file(hdf5_file):\n",
    "        # 测试文件可用性\n",
    "        test_file_after_unlock(hdf5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af8024ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mh5f\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h5f' is not defined"
     ]
    }
   ],
   "source": [
    "h5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72ec519",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to synchronously open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m h5f \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdf5_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msec2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m h5f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m h5f\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to synchronously open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "\n",
    "h5f = h5py.File(hdf5_file, 'r',driver='sec2')\n",
    "h5f.close()\n",
    "del h5f\n",
    "h5f = h5py.File(hdf5_file, 'a')\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LQ_signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
