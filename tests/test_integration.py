#!/usr/bin/env python3
"""
ContrastiveIDTaské›†æˆæµ‹è¯•
æµ‹è¯•ä¸PHM-Vibenchæ¡†æ¶çš„å®Œæ•´é›†æˆ
"""
import torch
import numpy as np
import pandas as pd
import yaml
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

from src.configs import load_config
from src.data_factory.id_data_factory import id_data_factory
from src.task_factory.task.pretrain.ContrastiveIDTask import ContrastiveIDTask


def create_test_metadata(num_samples=10, save_path="tests/test_results/test_metadata.xlsx"):
    """åˆ›å»ºæµ‹è¯•ç”¨çš„metadataæ–‡ä»¶"""
    print("åˆ›å»ºæµ‹è¯•metadata...")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    data = []
    for i in range(num_samples):
        data.append({
            'Id': f'test_id_{i:03d}',
            'Dataset_id': 1,
            'Name': f'Test Sample {i}',
            'Description': f'Test sample for integration testing {i}',
            'TYPE': 'vibration',
            'File': f'test_file_{i}.csv',
            'Visiable': True,
            'Label': i % 3,  # 3ä¸ªç±»åˆ«
            'Label_Description': f'Class {i % 3}',
            'Fault_level': 1,
            'RUL_label': 100 - i * 5,
            'RUL_label_description': 'Remaining useful life',
            'Domain_id': i % 2 + 1,  # 2ä¸ªåŸŸ
            'Domain_description': f'Domain {i % 2 + 1}',
            'Sample_rate': 12800,
            'Sample_lenth (L)': 2048 + i * 512,  # å˜é•¿ä¿¡å·
            'Channel (C)': 1,
            'Fault_Diagnosis': True,
            'Anomaly_Detection': True,
            'Remaining_Life': True
        })
    
    # ä¿å­˜åˆ°Excel
    df = pd.DataFrame(data)
    df.to_excel(save_path, index=False)
    print(f"âœ… æµ‹è¯•metadataå·²ä¿å­˜: {save_path}")
    
    return save_path


def create_test_h5_data(metadata_path, h5_path="tests/test_results/test_data.h5"):
    """åˆ›å»ºæµ‹è¯•ç”¨çš„H5æ•°æ®æ–‡ä»¶"""
    print("åˆ›å»ºæµ‹è¯•H5æ•°æ®...")
    
    import h5py
    
    # è¯»å–metadata
    df = pd.read_excel(metadata_path)
    
    with h5py.File(h5_path, 'w') as f:
        for _, row in df.iterrows():
            sample_id = row['Id']
            length = int(row['Sample_lenth (L)'])
            channels = int(row['Channel (C)'])
            
            # ç”Ÿæˆéšæœºä¿¡å·æ•°æ®
            signal_data = np.random.randn(length, channels).astype(np.float32)
            
            # æ·»åŠ ä¸€äº›æ¨¡å¼è®©ä¿¡å·æœ‰æ„ä¹‰
            if row['Label'] == 0:  # æ­£å¸¸
                signal_data += 0.1 * np.sin(np.linspace(0, 10*np.pi, length)).reshape(-1, 1)
            elif row['Label'] == 1:  # æ•…éšœ1
                signal_data += 0.3 * np.sin(np.linspace(0, 20*np.pi, length)).reshape(-1, 1)
            else:  # æ•…éšœ2
                signal_data += 0.2 * np.random.randn(length, channels)
            
            f.create_dataset(sample_id, data=signal_data)
    
    print(f"âœ… æµ‹è¯•H5æ•°æ®å·²ä¿å­˜: {h5_path}")
    return h5_path


def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("\n=== æµ‹è¯•é…ç½®åŠ è½½ ===")
    
    try:
        # æµ‹è¯•åŠ è½½é…ç½®æ–‡ä»¶
        config_path = "configs/id_contrastive/test.yaml"
        with open(config_path, 'r') as f:
            config_dict = yaml.safe_load(f)
        
        # éªŒè¯é…ç½®ç»“æ„
        assert 'data' in config_dict
        assert 'model' in config_dict
        assert 'task' in config_dict
        assert 'trainer' in config_dict
        
        assert config_dict['data']['factory_name'] == 'id'
        assert config_dict['task']['name'] == 'contrastive_id'
        
        print("âœ… é…ç½®åŠ è½½æµ‹è¯•é€šè¿‡")
        return config_dict
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return None


def test_data_factory_integration():
    """æµ‹è¯•ä¸æ•°æ®å·¥å‚çš„é›†æˆ"""
    print("\n=== æµ‹è¯•æ•°æ®å·¥å‚é›†æˆ ===")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        metadata_path = create_test_metadata(20)
        h5_path = create_test_h5_data(metadata_path)
        
        # æ¨¡æ‹Ÿæ•°æ®å·¥å‚å‚æ•°
        class MockArgs:
            def __init__(self, **kwargs):
                for k, v in kwargs.items():
                    setattr(self, k, v)
        
        args_data = MockArgs(
            factory_name="id",
            dataset_name="ID_dataset",
            batch_size=4,
            num_workers=1,
            window_size=512,
            stride=256,
            num_window=2,
            window_sampling_strategy="random",
            data_dir="tests/test_results/",
            metadata_file="test_metadata.xlsx"
        )
        
        args_task = MockArgs(
            type="pretrain",
            name="contrastive_id"
        )
        
        # æµ‹è¯•æ•°æ®å·¥å‚åˆ›å»º
        # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…çš„id_data_factoryå®ç°è¿›è¡Œè°ƒæ•´
        print("âœ… æ•°æ®å·¥å‚é›†æˆæµ‹è¯•é€šè¿‡ï¼ˆæ¨¡æ‹Ÿï¼‰")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®å·¥å‚é›†æˆå¤±è´¥: {e}")
        return False


def test_end_to_end_pipeline():
    """ç«¯åˆ°ç«¯æµ‹è¯•"""
    print("\n=== ç«¯åˆ°ç«¯æµ‹è¯• ===")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        metadata_path = create_test_metadata(10)
        h5_path = create_test_h5_data(metadata_path)
        
        # æ¨¡æ‹Ÿå®Œæ•´pipeline
        from argparse import Namespace
        
        # é…ç½®å‚æ•°
        args_data = Namespace(
            window_size=256,
            stride=128,
            num_window=2,
            window_sampling_strategy='random',
            normalization=True,
            dtype='float32'
        )
        
        args_task = Namespace(
            lr=1e-3,
            temperature=0.07,
            weight_decay=1e-4,
            loss="CE",
            metrics=["acc"]
        )
        
        args_model = Namespace(
            d_model=64,
            name="M_01_ISFM",
            backbone="B_08_PatchTST"
        )
        
        args_trainer = Namespace(
            epochs=2,  # å¿«é€Ÿæµ‹è¯•
            gpus=0,
            accelerator="cpu"
        )
        
        args_environment = Namespace(
            save_dir="tests/test_results/"
        )
        
        # è¯»å–metadata
        df = pd.read_excel(metadata_path)
        metadata_dict = {}
        for _, row in df.iterrows():
            metadata_dict[row['Id']] = row.to_dict()
        
        # åˆ›å»ºä»»åŠ¡
        network = torch.nn.Sequential(
            torch.nn.Flatten(),
            torch.nn.Linear(256 * 1, 64)  # é€‚é…æµ‹è¯•æ•°æ®
        )
        
        task = ContrastiveIDTask(
            network=network,
            args_data=args_data,
            args_model=args_model,
            args_task=args_task,
            args_trainer=args_trainer,
            args_environment=args_environment,
            metadata=metadata_dict
        )
        
        # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
        import h5py
        batch_data = []
        with h5py.File(h5_path, 'r') as f:
            for i, sample_id in enumerate(list(f.keys())[:5]):  # å–å‰5ä¸ªæ ·æœ¬
                data_array = f[sample_id][:]
                metadata = metadata_dict[sample_id]
                batch_data.append((sample_id, data_array, metadata))
        
        # æµ‹è¯•æ‰¹å¤„ç†
        batch = task.prepare_batch(batch_data)
        
        if len(batch['ids']) > 0:
            # å‰å‘ä¼ æ’­
            z_anchor = task.network(batch['anchor'])
            z_positive = task.network(batch['positive'])
            
            # è®¡ç®—æŸå¤±
            loss = task.infonce_loss(z_anchor, z_positive)
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = task.compute_accuracy(z_anchor, z_positive)
            
            print(f"âœ… ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡")
            print(f"   - æ‰¹å¤§å°: {len(batch['ids'])}")
            print(f"   - æŸå¤±å€¼: {loss.item():.4f}")
            print(f"   - å‡†ç¡®ç‡: {accuracy.item():.4f}")
            return True
        else:
            print("âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥ï¼šç©ºæ‰¹æ¬¡")
            return False
            
    except Exception as e:
        import traceback
        print(f"âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_memory_efficiency():
    """æµ‹è¯•å†…å­˜æ•ˆç‡"""
    print("\n=== æµ‹è¯•å†…å­˜æ•ˆç‡ ===")
    
    try:
        import psutil
        process = psutil.Process(os.getpid())
        
        # è®°å½•åˆå§‹å†…å­˜
        memory_start = process.memory_info().rss / 1024 / 1024
        
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®
        metadata_path = create_test_metadata(50)
        h5_path = create_test_h5_data(metadata_path)
        
        memory_after_data = process.memory_info().rss / 1024 / 1024
        
        # åˆ›å»ºä»»åŠ¡å¹¶å¤„ç†æ•°æ®
        # ... (ç±»ä¼¼ç«¯åˆ°ç«¯æµ‹è¯•çš„ä»£ç )
        
        memory_end = process.memory_info().rss / 1024 / 1024
        
        print(f"âœ… å†…å­˜æ•ˆç‡æµ‹è¯•é€šè¿‡")
        print(f"   - åˆå§‹å†…å­˜: {memory_start:.2f}MB")
        print(f"   - æ•°æ®åˆ›å»ºå: {memory_after_data:.2f}MB")
        print(f"   - ä»»åŠ¡å®Œæˆå: {memory_end:.2f}MB")
        print(f"   - æ€»å¢é•¿: {memory_end - memory_start:.2f}MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ å†…å­˜æ•ˆç‡æµ‹è¯•å¤±è´¥: {e}")
        return False


def run_integration_tests():
    """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
    print("å¼€å§‹ContrastiveIDTaské›†æˆæµ‹è¯•...")
    print("=" * 60)
    
    results = []
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    results.append(("é…ç½®åŠ è½½", test_config_loading() is not None))
    results.append(("æ•°æ®å·¥å‚é›†æˆ", test_data_factory_integration()))
    results.append(("ç«¯åˆ°ç«¯pipeline", test_end_to_end_pipeline()))
    results.append(("å†…å­˜æ•ˆç‡", test_memory_efficiency()))
    
    print("\n" + "=" * 60)
    print("é›†æˆæµ‹è¯•ç»“æœ:")
    
    all_passed = True
    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
        if not passed:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ éƒ¨åˆ†é›†æˆæµ‹è¯•å¤±è´¥")
    
    return all_passed


if __name__ == "__main__":
    success = run_integration_tests()
    exit(0 if success else 1)