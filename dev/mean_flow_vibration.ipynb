{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b2e45c",
   "metadata": {},
   "source": [
    "# meanflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454bd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    # minmax for raw image, mean_std for vae latent\n",
    "    def __init__(self, mode='minmax', mean=None, std=None):\n",
    "        assert mode in ['minmax', 'mean_std'], \"mode must be 'minmax' or 'mean_std'\"\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'mean_std':\n",
    "            if mean is None or std is None:\n",
    "                raise ValueError(\"mean and std must be provided for 'mean_std' mode\")\n",
    "            self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "            self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    @classmethod\n",
    "    def from_list(cls, config):\n",
    "        \"\"\"\n",
    "        config: [mode, mean, std]\n",
    "        \"\"\"\n",
    "        mode, mean, std = config\n",
    "        return cls(mode, mean, std)\n",
    "\n",
    "    def norm(self, x):\n",
    "        if self.mode == 'minmax':\n",
    "            return x * 2 - 1\n",
    "        elif self.mode == 'mean_std':\n",
    "            return (x - self.mean.to(x.device)) / self.std.to(x.device)\n",
    "\n",
    "    def unnorm(self, x):\n",
    "        if self.mode == 'minmax':\n",
    "            x = x.clip(-1, 1)\n",
    "            return (x + 1) * 0.5\n",
    "        elif self.mode == 'mean_std':\n",
    "            return x * self.std.to(x.device) + self.mean.to(x.device)\n",
    "\n",
    "\n",
    "def stopgrad(x):\n",
    "    return x.detach()\n",
    "\n",
    "\n",
    "def adaptive_l2_loss(error, gamma=0.5, c=1e-3):\n",
    "    \"\"\"\n",
    "    Adaptive L2 loss: sg(w) * ||Δ||_2^2, where w = 1 / (||Δ||^2 + c)^p, p = 1 - γ\n",
    "    Args:\n",
    "        error: Tensor of shape (B, C, W, H)\n",
    "        gamma: Power used in original ||Δ||^{2γ} loss\n",
    "        c: Small constant for stability\n",
    "    Returns:\n",
    "        Scalar loss\n",
    "    \"\"\"\n",
    "    delta_sq = torch.mean(error ** 2, dim=(1, 2, 3), keepdim=False)\n",
    "    p = 1.0 - gamma\n",
    "    w = 1.0 / (delta_sq + c).pow(p)\n",
    "    loss = delta_sq  # ||Δ||^2\n",
    "    return (stopgrad(w) * loss).mean()\n",
    "\n",
    "\n",
    "class MeanFlow:\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels=1,\n",
    "        image_size=32,\n",
    "        num_classes=10,\n",
    "        normalizer=['minmax', None, None],\n",
    "        # mean flow settings\n",
    "        flow_ratio=0.50,\n",
    "        # time distribution, mu, sigma\n",
    "        time_dist=['lognorm', -0.4, 1.0],\n",
    "        cfg_ratio=0.10,\n",
    "        # set scale as none to disable CFG distill\n",
    "        cfg_scale=2.0,\n",
    "        # experimental\n",
    "        cfg_uncond='u',\n",
    "        jvp_api='autograd',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.use_cond = num_classes is not None\n",
    "\n",
    "        self.normer = Normalizer.from_list(normalizer)\n",
    "\n",
    "        self.flow_ratio = flow_ratio\n",
    "        self.time_dist = time_dist\n",
    "        self.cfg_ratio = cfg_ratio\n",
    "        self.w = cfg_scale\n",
    "\n",
    "        self.cfg_uncond = cfg_uncond\n",
    "        self.jvp_api = jvp_api\n",
    "\n",
    "        assert jvp_api in ['funtorch', 'autograd'], \"jvp_api must be 'funtorch' or 'autograd'\"\n",
    "        if jvp_api == 'funtorch':\n",
    "            self.jvp_fn = torch.func.jvp\n",
    "            self.create_graph = False\n",
    "        elif jvp_api == 'autograd':\n",
    "            self.jvp_fn = torch.autograd.functional.jvp\n",
    "            self.create_graph = True\n",
    "\n",
    "    # fix: r should be always not larger than t\n",
    "    def sample_t_r(self, batch_size, device):\n",
    "        if self.time_dist[0] == 'uniform':\n",
    "            samples = np.random.rand(batch_size, 2).astype(np.float32)\n",
    "\n",
    "        elif self.time_dist[0] == 'lognorm':\n",
    "            mu, sigma = self.time_dist[-2], self.time_dist[-1]\n",
    "            normal_samples = np.random.randn(batch_size, 2).astype(np.float32) * sigma + mu\n",
    "            samples = 1 / (1 + np.exp(-normal_samples))  # Apply sigmoid\n",
    "\n",
    "        # Assign t = max, r = min, for each pair\n",
    "        t_np = np.maximum(samples[:, 0], samples[:, 1])\n",
    "        r_np = np.minimum(samples[:, 0], samples[:, 1])\n",
    "\n",
    "        num_selected = int(self.flow_ratio * batch_size)\n",
    "        indices = np.random.permutation(batch_size)[:num_selected]\n",
    "        r_np[indices] = t_np[indices]\n",
    "\n",
    "        t = torch.tensor(t_np, device=device)\n",
    "        r = torch.tensor(r_np, device=device)\n",
    "        return t, r\n",
    "\n",
    "    def loss(self, model, x, c=None):\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "\n",
    "        t, r = self.sample_t_r(batch_size, device)\n",
    "\n",
    "        t_ = rearrange(t, \"b -> b 1 1 1\").detach().clone()\n",
    "        r_ = rearrange(r, \"b -> b 1 1 1\").detach().clone()\n",
    "\n",
    "        e = torch.randn_like(x)\n",
    "        x = self.normer.norm(x)\n",
    "\n",
    "        z = (1 - t_) * x + t_ * e\n",
    "        v = e - x\n",
    "\n",
    "        if c is not None:\n",
    "            assert self.cfg_ratio is not None\n",
    "            uncond = torch.ones_like(c) * self.num_classes\n",
    "            cfg_mask = torch.rand_like(c.float()) < self.cfg_ratio\n",
    "            c = torch.where(cfg_mask, uncond, c)\n",
    "            if self.w is not None:\n",
    "                with torch.no_grad():\n",
    "                    u_t = model(z, t, t, uncond)\n",
    "                v_hat = self.w * v + (1 - self.w) * u_t\n",
    "                if self.cfg_uncond == 'v':\n",
    "                    # In the unconditional case, v = w * v + (1 - w) * u,\n",
    "                    # so if we're choosing to use 'v' for uncond settings, we can just keep v.\n",
    "                    # Apply this only to the unconditional samples indicated by cfg_mask.\n",
    "                    cfg_mask = rearrange(cfg_mask, \"b -> b 1 1 1\").bool()\n",
    "                    v_hat = torch.where(cfg_mask, v, v_hat)\n",
    "            else:\n",
    "                v_hat = v\n",
    "\n",
    "        # forward pass\n",
    "        # u = model(z, t, r, y=c)\n",
    "        model_partial = partial(model, y=c)\n",
    "        jvp_args = (\n",
    "            lambda z, t, r: model_partial(z, t, r),\n",
    "            (z, t, r),\n",
    "            (v_hat, torch.ones_like(t), torch.zeros_like(r)),\n",
    "        )\n",
    "\n",
    "        if self.create_graph:\n",
    "            u, dudt = self.jvp_fn(*jvp_args, create_graph=True)\n",
    "        else:\n",
    "            u, dudt = self.jvp_fn(*jvp_args)\n",
    "\n",
    "        u_tgt = v_hat - (t_ - r_) * dudt\n",
    "\n",
    "        error = u - stopgrad(u_tgt)\n",
    "        loss = adaptive_l2_loss(error)\n",
    "        # loss = F.mse_loss(u, stopgrad(u_tgt))\n",
    "\n",
    "        mse_val = (stopgrad(error) ** 2).mean()\n",
    "        return loss, mse_val\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_each_class(self, model, n_per_class, classes=None,\n",
    "                          sample_steps=5, device='cuda'):\n",
    "        model.eval()\n",
    "\n",
    "        if classes is None:\n",
    "            c = torch.arange(self.num_classes, device=device).repeat(n_per_class)\n",
    "        else:\n",
    "            c = torch.tensor(classes, device=device).repeat(n_per_class)\n",
    "\n",
    "        z = torch.randn(c.shape[0], self.channels,\n",
    "                        self.image_size, self.image_size,\n",
    "                        device=device)\n",
    "\n",
    "        t_vals = torch.linspace(1.0, 0.0, sample_steps + 1, device=device)\n",
    "\n",
    "        # print(t_vals)\n",
    "\n",
    "        for i in range(sample_steps):\n",
    "            t = torch.full((z.size(0),), t_vals[i], device=device)\n",
    "            r = torch.full((z.size(0),), t_vals[i + 1], device=device)\n",
    "\n",
    "            # print(f\"t: {t[0].item():.4f};  r: {r[0].item():.4f}\")\n",
    "\n",
    "            t_ = rearrange(t, \"b -> b 1 1 1\").detach().clone()\n",
    "            r_ = rearrange(r, \"b -> b 1 1 1\").detach().clone()\n",
    "\n",
    "            v = model(z, t, r, c)\n",
    "            z = z - (t_-r_) * v\n",
    "\n",
    "        z = self.normer.unnorm(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50b641",
   "metadata": {},
   "source": [
    "## demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a629ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training demo...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     87\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 89\u001b[0m     loss, mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_flow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     92\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[3], line 150\u001b[0m, in \u001b[0;36mMeanFlow.loss\u001b[0;34m(self, model, x, c)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 150\u001b[0m         u_t \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muncond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     v_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m*\u001b[39m v \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw) \u001b[38;5;241m*\u001b[39m u_t\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg_uncond \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;66;03m# In the unconditional case, v = w * v + (1 - w) * u,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;66;03m# so if we're choosing to use 'v' for uncond settings, we can just keep v.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;66;03m# Apply this only to the unconditional samples indicated by cfg_mask.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/P/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/P/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m, in \u001b[0;36mSimpleUnet.forward\u001b[0;34m(self, z, t, r, y)\u001b[0m\n\u001b[1;32m     46\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1(z))\n\u001b[1;32m     47\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(x1))\n\u001b[0;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcond_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 融入条件\u001b[39;00m\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x, scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# 模拟上采样\u001b[39;00m\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, x1], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 1. 模拟一个简单的U-Net模型 (用于图像)\n",
    "# 注意: 这是一个非常简化的模型，仅用于演示目的。\n",
    "class SimpleUnet(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10, time_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim * 4, time_emb_dim)\n",
    "        )\n",
    "        self.class_emb = nn.Embedding(num_classes + 1, time_emb_dim) # +1 for uncond\n",
    "\n",
    "        self.down1 = nn.Conv2d(in_channels, 32, 3, padding=1)\n",
    "        self.down2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.up1 = nn.Conv2d(64 + 32, 32, 3, padding=1)\n",
    "        self.up2 = nn.Conv2d(32, in_channels, 3, padding=1)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def _time_embedding(self, t, emb_dim):\n",
    "        # t: [B]\n",
    "        half_dim = emb_dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        return torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "\n",
    "    def forward(self, z, t, r, y): # 模型需要接受 z, t, r, 和 y (条件)\n",
    "        time_emb = self._time_embedding(t, 32)\n",
    "        time_emb = self.time_mlp(time_emb)\n",
    "\n",
    "        class_emb = self.class_emb(y)\n",
    "        \n",
    "        cond_emb = time_emb + class_emb\n",
    "        \n",
    "        # 将条件信息融入网络 (简单地加到每个像素上)\n",
    "        cond_emb = rearrange(cond_emb, 'b c -> b c 1 1')\n",
    "\n",
    "        x1 = self.act(self.down1(z))\n",
    "        x2 = self.act(self.down2(x1))\n",
    "        \n",
    "        x = x2 + cond_emb.expand(-1, -1, x2.shape[2], x2.shape[3]) # 融入条件\n",
    "        \n",
    "        x = F.interpolate(x, scale_factor=1) # 模拟上采样\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.act(self.up1(x))\n",
    "        x = self.up2(x)\n",
    "        return x\n",
    "\n",
    "# 2. 设置参数和模拟数据\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 32\n",
    "CHANNELS = 1\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 20 # 增加迭代次数以看到损失下降\n",
    "\n",
    "# 创建假的图像数据和标签\n",
    "dummy_images = torch.rand(BATCH_SIZE * 5, CHANNELS, IMG_SIZE, IMG_SIZE, device=DEVICE)\n",
    "dummy_labels = torch.randint(0, NUM_CLASSES, (BATCH_SIZE * 5,), device=DEVICE)\n",
    "dataset = TensorDataset(dummy_images, dummy_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 3. 初始化所有组件\n",
    "model = SimpleUnet(in_channels=CHANNELS, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "mean_flow = MeanFlow(\n",
    "    channels=CHANNELS,\n",
    "    image_size=IMG_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    cfg_scale=3.0, # 使用分类器无关引导\n",
    "    jvp_api='autograd'\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 4. 训练循环\n",
    "print(\"Starting training demo...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, mse = mean_flow.loss(model, images, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# 5. 生成样本\n",
    "print(\"\\nGenerating samples...\")\n",
    "generated_samples = mean_flow.sample_each_class(\n",
    "    model,\n",
    "    n_per_class=2, # 每个类别生成2个样本\n",
    "    sample_steps=10,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(f\"Generated a tensor of shape: {generated_samples.shape}\")\n",
    "# 在真实场景中，你会在这里将张量保存为图像或进行可视化\n",
    "# import torchvision\n",
    "# torchvision.utils.save_image(generated_samples, \"generated_samples.png\", nrow=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8661dd",
   "metadata": {},
   "source": [
    "# 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376d2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_l2_loss_1d(error, gamma=0.5, c=1e-3):\n",
    "    \"\"\"\n",
    "    适用于 (B, C, L) 数据的自适应L2损失\n",
    "    \"\"\"\n",
    "    # 原来的维度是 (1, 2, 3) for (C, H, W)\n",
    "    # 新的维度是 (1, 2) for (C, L)\n",
    "    delta_sq = torch.mean(error ** 2, dim=(1, 2), keepdim=False)\n",
    "    p = 1.0 - gamma\n",
    "    w = 1.0 / (delta_sq + c).pow(p)\n",
    "    loss = delta_sq\n",
    "    return (stopgrad(w) * loss).mean()\n",
    "class Normalizer1D(Normalizer):\n",
    "    def __init__(self, mode='minmax', mean=None, std=None):\n",
    "        assert mode in ['minmax', 'mean_std'], \"mode must be 'minmax' or 'mean_std'\"\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'mean_std':\n",
    "            if mean is None or std is None:\n",
    "                raise ValueError(\"mean and std must be provided for 'mean_std' mode\")\n",
    "            # 原来是 view(-1, 1, 1) for (C, H, W)\n",
    "            # 现在是 view(-1, 1) for (C, L)\n",
    "            self.mean = torch.tensor(mean).view(-1, 1)\n",
    "            self.std = torch.tensor(std).view(-1, 1)\n",
    "class MeanFlow1D(MeanFlow):\n",
    "    def __init__(self, channels=1, sequence_length=1024, **kwargs):\n",
    "        # 用 sequence_length 替换 image_size\n",
    "        super().__init__(channels=channels, image_size=sequence_length, **kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        # 使用修改后的 Normalizer 和 loss\n",
    "        self.normer = Normalizer1D.from_list(kwargs.get('normalizer', ['minmax', None, None]))\n",
    "\n",
    "    def loss(self, model, x, c=None): # x 的形状是 (B, C, L)\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "\n",
    "        t, r = self.sample_t_r(batch_size, device)\n",
    "\n",
    "        # 关键修改：rearrange 从 \"b -> b 1 1 1\" 到 \"b -> b 1 1\"\n",
    "        t_ = rearrange(t, \"b -> b 1 1\").detach().clone()\n",
    "        r_ = rearrange(r, \"b -> b 1 1\").detach().clone()\n",
    "\n",
    "        e = torch.randn_like(x)\n",
    "        x = self.normer.norm(x)\n",
    "\n",
    "        z = (1 - t_) * x + t_ * e\n",
    "        v = e - x\n",
    "        v_hat = v\n",
    "\n",
    "        if c is not None:\n",
    "            assert self.cfg_ratio is not None\n",
    "            uncond = torch.ones_like(c) * self.num_classes\n",
    "            cfg_mask = torch.rand(c.shape[0], device=c.device) < self.cfg_ratio\n",
    "            c = torch.where(cfg_mask, uncond, c)\n",
    "            if self.w is not None:\n",
    "                with torch.no_grad():\n",
    "                    u_t = model(z, t, t, c)\n",
    "                v_hat = self.w * v + (1 - self.w) * u_t\n",
    "                if self.cfg_uncond == 'v':\n",
    "                    # 关键修改：rearrange\n",
    "                    cfg_mask_expanded = rearrange(cfg_mask, \"b -> b 1 1\")\n",
    "                    v_hat = torch.where(cfg_mask_expanded, v, v_hat)\n",
    "\n",
    "        model_partial = partial(model, y=c)\n",
    "        jvp_args = (\n",
    "            lambda z_arg, t_arg, r_arg: model_partial(z_arg, t_arg, r_arg),\n",
    "            (z, t, r),\n",
    "            (v_hat, torch.ones_like(t), torch.zeros_like(r)),\n",
    "        )\n",
    "\n",
    "        if self.create_graph:\n",
    "            u, dudt = self.jvp_fn(*jvp_args, create_graph=True)\n",
    "        else:\n",
    "            u, dudt = self.jvp_fn(*jvp_args)\n",
    "\n",
    "        u_tgt = v_hat - (t_ - r_) * dudt\n",
    "        error = u - stopgrad(u_tgt)\n",
    "        \n",
    "        # 使用1D版本的loss\n",
    "        loss = adaptive_l2_loss_1d(error)\n",
    "        mse_val = (stopgrad(error) ** 2).mean()\n",
    "        return loss, mse_val\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_each_class(self, model, n_per_class, classes=None,\n",
    "                            sample_steps=5, device='cuda'):\n",
    "        model.eval()\n",
    "\n",
    "        if classes is None:\n",
    "            c = torch.arange(self.num_classes, device=device).repeat(n_per_class)\n",
    "        else:\n",
    "            c = torch.tensor(classes, device=device).repeat(n_per_class)\n",
    "            \n",
    "        # 关键修改：噪声形状\n",
    "        z = torch.randn(c.shape[0], self.channels, self.sequence_length, device=device)\n",
    "\n",
    "        t_vals = torch.linspace(1.0, 0.0, sample_steps + 1, device=device)\n",
    "\n",
    "        for i in range(sample_steps):\n",
    "            t = torch.full((z.size(0),), t_vals[i], device=device)\n",
    "            r = torch.full((z.size(0),), t_vals[i + 1], device=device)\n",
    "\n",
    "            # 关键修改：rearrange\n",
    "            t_ = rearrange(t, \"b -> b 1 1\")\n",
    "            r_ = rearrange(r, \"b -> b 1 1\")\n",
    "            \n",
    "            if self.w is not None and self.use_cond:\n",
    "                uncond = torch.ones_like(c) * self.num_classes\n",
    "                v_cond = model(z, t, r, c)\n",
    "                v_uncond = model(z, t, r, uncond)\n",
    "                v = (1 + self.w) * v_cond - self.w * v_uncond\n",
    "            else:\n",
    "                 v = model(z, t, r, c)\n",
    "\n",
    "            z = z - (t_ - r_) * v\n",
    "\n",
    "        z = self.normer.unnorm(z)\n",
    "        # 如果你需要 (B, L, C) 格式，在这里进行转换\n",
    "        # z = z.permute(0, 2, 1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9c231",
   "metadata": {},
   "source": [
    "##  network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c0845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUnet1D(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10, time_emb_dim=32, sequence_length=1024):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim * 4, time_emb_dim)\n",
    "        )\n",
    "        self.class_emb = nn.Embedding(num_classes + 1, time_emb_dim)\n",
    "\n",
    "        # 使用1D卷积\n",
    "        self.down1 = nn.Conv1d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.down2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 上采样可以使用 ConvTranspose1d 或 Interpolate\n",
    "        self.up1 = nn.Conv1d(64 + 32, 32, kernel_size=3, padding=1)\n",
    "        self.up2 = nn.Conv1d(32, in_channels, kernel_size=3, padding=1)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def _time_embedding(self, t, emb_dim):\n",
    "        half_dim = emb_dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        return torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "\n",
    "    def forward(self, z, t, r, y): # z: (B, C, L)\n",
    "        time_emb = self._time_embedding(t, 64)\n",
    "        time_emb = self.time_mlp(time_emb)\n",
    "\n",
    "        class_emb = self.class_emb(y)\n",
    "        cond_emb = time_emb + class_emb\n",
    "        \n",
    "        # 调整条件形状以匹配序列\n",
    "        cond_emb = rearrange(cond_emb, 'b c -> b c 1')\n",
    "\n",
    "        x1 = self.act(self.down1(z))\n",
    "        x2 = self.act(self.down2(x1))\n",
    "        \n",
    "        # 融入条件信息\n",
    "        x = x2 + cond_emb.expand(-1, -1, x2.shape[2])\n",
    "        \n",
    "        # 使用插值进行上采样\n",
    "        x = F.interpolate(x, size=x1.shape[-1])\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.act(self.up1(x))\n",
    "        x = self.up2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3f00a",
   "metadata": {},
   "source": [
    "## demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173b8357",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x64 and 32x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m signals, labels \u001b[38;5;129;01min\u001b[39;00m dataloader_1d:\n\u001b[1;32m     50\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 51\u001b[0m     loss, mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_flow_1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_1d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m, in \u001b[0;36mMeanFlow1D.loss\u001b[0;34m(self, model, x, c)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 56\u001b[0m         u_t \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     v_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m*\u001b[39m v \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw) \u001b[38;5;241m*\u001b[39m u_t\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg_uncond \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m# 关键修改：rearrange\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/P/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/P/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mSimpleUnet1D.forward\u001b[0;34m(self, z, t, r, y)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, t, r, y): \u001b[38;5;66;03m# z: (B, C, L)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     time_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_embedding(t, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     time_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     class_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_emb(y)\n\u001b[1;32m     32\u001b[0m     cond_emb \u001b[38;5;241m=\u001b[39m time_emb \u001b[38;5;241m+\u001b[39m class_emb\n",
      "File \u001b[0;32m~/.conda/envs/P/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/P/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/P/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/P/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/P/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/P/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x64 and 32x128)"
     ]
    }
   ],
   "source": [
    "# 振动信号生成任务的示例设置\n",
    "SEQ_LEN = 1024\n",
    "CHANNELS = 3  # 假设是3轴振动信号\n",
    "NUM_CLASSES = 5 # 假设有5种工况\n",
    "\n",
    "# 1. 实例化1D模型\n",
    "model_1d = SimpleUnet1D(\n",
    "    in_channels=CHANNELS, \n",
    "    num_classes=NUM_CLASSES, \n",
    "    sequence_length=SEQ_LEN\n",
    ").to(DEVICE)\n",
    "\n",
    "# 2. 实例化 MeanFlow1D\n",
    "mean_flow_1d = MeanFlow1D(\n",
    "    channels=CHANNELS,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    cfg_scale=3.0\n",
    ")\n",
    "\n",
    "# 3. 创建模拟的振动信号数据 (B, C, L)\n",
    "# 生成多谐波信号 (多个正弦波叠加)\n",
    "t = torch.linspace(0, 2 * np.pi, SEQ_LEN, device=DEVICE)\n",
    "dummy_signals = torch.zeros(BATCH_SIZE * 5, CHANNELS, SEQ_LEN, device=DEVICE)\n",
    "\n",
    "for i in range(BATCH_SIZE * 5):\n",
    "    for c in range(CHANNELS):\n",
    "        # 为每个通道生成不同频率的多谐波信号\n",
    "        signal = torch.zeros_like(t)\n",
    "        # 基频和谐波\n",
    "        freqs = [1, 2, 3, 4]  # 基频和3个谐波\n",
    "        amps = [1.0, 0.5, 0.3, 0.2]  # 对应幅度\n",
    "        phases = torch.rand(len(freqs)) * 2 * np.pi  # 随机相位\n",
    "        \n",
    "        for freq, amp, phase in zip(freqs, amps, phases):\n",
    "            signal += amp * torch.sin(freq * (c + 1) * t + phase)\n",
    "        \n",
    "        # 添加少量噪声\n",
    "        signal += 0.1 * torch.randn_like(signal)\n",
    "        dummy_signals[i, c] = signal\n",
    "dummy_labels = torch.randint(0, NUM_CLASSES, (BATCH_SIZE * 5,), device=DEVICE)\n",
    "dataset_1d = TensorDataset(dummy_signals, dummy_labels)\n",
    "dataloader_1d = DataLoader(dataset_1d, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 4. 运行训练循环 (与之前类似，但使用1D组件)\n",
    "optimizer = torch.optim.Adam(model_1d.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for signals, labels in dataloader_1d:\n",
    "        optimizer.zero_grad()\n",
    "        loss, mse = mean_flow_1d.loss(model_1d, signals, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdeb190",
   "metadata": {},
   "source": [
    "## generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 生成信号\n",
    "generated_signals = mean_flow_1d.sample_each_class(\n",
    "    model_1d, \n",
    "    n_per_class=1, \n",
    "    device=DEVICE\n",
    ")\n",
    "print(f\"Generated signals shape: {generated_signals.shape}\") # (B, C, L)\n",
    "\n",
    "# 如果你的最终目标是 (B, L, C)\n",
    "generated_signals_final_shape = generated_signals.permute(0, 2, 1)\n",
    "print(f\"Final shape after permute: {generated_signals_final_shape.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
