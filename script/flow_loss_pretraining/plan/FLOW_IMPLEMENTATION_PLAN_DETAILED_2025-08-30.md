# Flowæ¨¡å‹é¢„è®­ç»ƒè¯¦ç»†å®æ–½è®¡åˆ’

**åˆ›å»ºæ—¥æœŸï¼š2025å¹´8æœˆ30æ—¥**  
**ç‰ˆæœ¬ï¼šV4.0 - è¯¦ç»†å®æ–½ç‰ˆ**  
**åŸºäºï¼šGM_FLOW_PLAN_OPTIMIZED_2025-08-30.md åˆ†æç»“æœ**

---

## ğŸ¯ æ‰§è¡Œæ‘˜è¦

åŸºäºå¯¹ç°æœ‰Flowè®¡åˆ’çš„æ·±åº¦åˆ†æï¼Œæœ¬è®¡åˆ’è§£å†³äº†å…³é”®çš„æŠ€æœ¯é›†æˆé—®é¢˜ï¼Œæä¾›äº†å¯æ‰§è¡Œçš„åˆ†é˜¶æ®µå®æ–½æ–¹æ¡ˆï¼Œç¡®ä¿Rectified Flowç”Ÿæˆæ¨¡å‹æˆåŠŸé›†æˆåˆ°PHM-Vibenchæ¡†æ¶ä¸­ã€‚

### æ ¸å¿ƒæ”¹è¿›ç‚¹
- âœ… **ç»´åº¦å…¼å®¹æ€§ä¿®å¤**: é€‚é…(B,L,C)å¼ é‡æ ¼å¼
- âœ… **å…ƒæ•°æ®é›†æˆ**: ä½¿ç”¨file_idæå–å±‚æ¬¡åŒ–æ¡ä»¶ä¿¡æ¯  
- âœ… **å·¥å‚æ¨¡å¼åˆè§„**: éµå¾ªPHM-Vibenchæ¶æ„æ¨¡å¼
- âœ… **æ•°å€¼ç¨³å®šæ€§**: å®Œæ•´çš„é”™è¯¯å¤„ç†å’ŒéªŒè¯
- âœ… **æµ‹è¯•é©±åŠ¨**: å…¨é¢çš„å•å…ƒå’Œé›†æˆæµ‹è¯•

---

## ğŸ“Š æŠ€æœ¯æ¶æ„è¯¦ç»†è®¾è®¡

### 1. æ ¸å¿ƒç»„ä»¶æ¶æ„

```
Flow-based Pretraining System
â”œâ”€â”€ Sequence Adapter          # ç»´åº¦é€‚é…å±‚
â”‚   â”œâ”€â”€ flatten_sequence()   # (B,L,C) â†’ (B,L*C)
â”‚   â””â”€â”€ unflatten_sequence() # (B,L*C) â†’ (B,L,C)
â”œâ”€â”€ Rectified Flow Model      # æ ¸å¿ƒç”Ÿæˆæ¨¡å‹
â”‚   â”œâ”€â”€ velocity_network()   # é€Ÿåº¦åœºé¢„æµ‹
â”‚   â”œâ”€â”€ flow_matching()      # æµåŒ¹é…è®­ç»ƒ
â”‚   â””â”€â”€ ode_sampling()       # ODEç§¯åˆ†é‡‡æ ·
â”œâ”€â”€ Conditional Encoder       # å±‚æ¬¡åŒ–æ¡ä»¶ç¼–ç 
â”‚   â”œâ”€â”€ domain_encoder()     # åŸŸçº§ç¼–ç 
â”‚   â”œâ”€â”€ system_encoder()     # ç³»ç»Ÿçº§ç¼–ç 
â”‚   â””â”€â”€ instance_encoder()   # å®ä¾‹çº§ç¼–ç 
â””â”€â”€ Flow Utilities           # è¾…åŠ©å·¥å…·
    â”œâ”€â”€ solvers/            # ODEæ±‚è§£å™¨
    â”œâ”€â”€ schedulers/         # å™ªå£°è°ƒåº¦
    â””â”€â”€ metrics/           # è¯„ä¼°æŒ‡æ ‡
```

### 2. ç»´åº¦å¤„ç†ç­–ç•¥

#### é—®é¢˜åˆ†æ
- **ç°æœ‰ä»£ç **: å‡è®¾è¾“å…¥ä¸º`(batch_size, latent_dim)`
- **PHM-Vibenchå®é™…**: ä½¿ç”¨`(batch_size, sequence_length, channels)`æ ¼å¼
- **å…¸å‹å‚æ•°**: sequence_length=1024, channels=1-3

#### è§£å†³æ–¹æ¡ˆè®¾è®¡

```python
class SequenceAdapter(nn.Module):
    """åºåˆ—ç»´åº¦é€‚é…å™¨ - å¤„ç†3Då¼ é‡æ ¼å¼è½¬æ¢"""
    
    def __init__(self, seq_len: int, channels: int, latent_dim: int):
        super().__init__()
        self.seq_len = seq_len
        self.channels = channels
        self.latent_dim = latent_dim
        
        # æ–¹æ¡ˆ1: ç›´æ¥å±•å¼€(æ¨è)
        self.use_flatten = True
        
        # æ–¹æ¡ˆ2: å·ç§¯é™ç»´(å¤‡é€‰)
        if not self.use_flatten:
            self.conv_encoder = nn.Conv1d(channels, latent_dim//seq_len, 1)
            self.conv_decoder = nn.Conv1d(latent_dim//seq_len, channels, 1)
    
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç : (B, L, C) -> (B, D)
        """
        B, L, C = x.shape
        if self.use_flatten:
            return x.view(B, L * C)
        else:
            # ä½¿ç”¨å·ç§¯é™ç»´
            x = x.transpose(1, 2)  # (B, C, L)
            x = self.conv_encoder(x)  # (B, D/L, L)
            return x.view(B, -1)  # (B, D)
    
    def decode(self, x: torch.Tensor) -> torch.Tensor:
        """
        è§£ç : (B, D) -> (B, L, C)
        """
        B = x.shape[0]
        if self.use_flatten:
            return x.view(B, self.seq_len, self.channels)
        else:
            # ä½¿ç”¨å·ç§¯å‡ç»´
            x = x.view(B, self.latent_dim//self.seq_len, self.seq_len)
            x = self.conv_decoder(x)  # (B, C, L)
            return x.transpose(1, 2)  # (B, L, C)
```

### 3. å…ƒæ•°æ®é›†æˆç³»ç»Ÿ

#### å±‚æ¬¡åŒ–ä¿¡æ¯æå–

```python
# æ˜ å°„è¡¨å®šä¹‰
DATASET_DOMAIN_MAPPING = {
    'CWRU': 0, 'XJTU': 1, 'PU': 2, 'FEMTO': 3, 'IMS': 4,
    'KAT': 5, 'Ottawa': 6, 'UO': 7, 'JNU': 8, 'MFPT': 9
}

SYSTEM_TYPE_MAPPING = {
    'bearing': 0, 'gear': 1, 'rotor': 2, 'pump': 3, 'motor': 4,
    'compressor': 5, 'turbine': 6, 'gearbox': 7, 'fan': 8, 'other': 9
}

class MetadataExtractor:
    """ä»PHM-Vibenchå…ƒæ•°æ®æå–å±‚æ¬¡åŒ–æ¡ä»¶ä¿¡æ¯"""
    
    @staticmethod
    def extract_hierarchical_info(metadata: Dict[str, Any]) -> Tuple[int, int, int]:
        """
        æå–åŸŸã€ç³»ç»Ÿã€å®ä¾‹ID
        
        Args:
            metadata: PHM-Vibenchæ ‡å‡†å…ƒæ•°æ®
            
        Returns:
            domain_id, system_id, instance_id
        """
        file_id = metadata.get('file_id', '')
        
        # æå–åŸŸID (æ•°æ®é›†åç§°)
        dataset_name = file_id.split('_')[0] if '_' in file_id else 'unknown'
        domain_id = DATASET_DOMAIN_MAPPING.get(dataset_name, 0)
        
        # æå–ç³»ç»ŸID (è®¾å¤‡ç±»å‹)
        system_type = MetadataExtractor._infer_system_type(file_id)
        system_id = SYSTEM_TYPE_MAPPING.get(system_type, 0)
        
        # å®ä¾‹ID (æ–‡ä»¶å†…éƒ¨ç´¢å¼•)
        instance_id = metadata.get('sample_idx', 0)
        
        return domain_id, system_id, instance_id
    
    @staticmethod
    def _infer_system_type(file_id: str) -> str:
        """ä»æ–‡ä»¶IDæ¨æ–­è®¾å¤‡ç±»å‹"""
        file_lower = file_id.lower()
        if any(x in file_lower for x in ['bearing', 'ball', 'inner', 'outer']):
            return 'bearing'
        elif any(x in file_lower for x in ['gear', 'tooth']):
            return 'gear'
        elif any(x in file_lower for x in ['rotor', 'shaft']):
            return 'rotor'
        elif any(x in file_lower for x in ['pump']):
            return 'pump'
        else:
            return 'other'
```

### 4. å¢å¼ºçš„ODEæ±‚è§£å™¨

```python
class FlowODESolver:
    """é«˜ç²¾åº¦ODEæ±‚è§£å™¨é›†åˆ"""
    
    def __init__(self, solver_type: str = 'euler'):
        self.solver_type = solver_type
        self.solver_registry = {
            'euler': self.euler_step,
            'heun': self.heun_step,
            'rk4': self.rk4_step,
            'adaptive': self.adaptive_step
        }
    
    def euler_step(self, model, x, t, dt, condition=None):
        """ä¸€é˜¶æ¬§æ‹‰æ–¹æ³•"""
        with torch.no_grad():
            t_tensor = torch.full((x.size(0),), t, device=x.device)
            t_emb = model.time_embedding(t_tensor)
            v = model.velocity_net(x, t_emb, condition)
            return x + dt * v
    
    def heun_step(self, model, x, t, dt, condition=None):
        """äºŒé˜¶Heunæ–¹æ³• (æ”¹è¿›çš„æ¬§æ‹‰æ³•)"""
        with torch.no_grad():
            # ç¬¬ä¸€æ­¥é¢„æµ‹
            t_tensor = torch.full((x.size(0),), t, device=x.device)
            t_emb = model.time_embedding(t_tensor)
            k1 = model.velocity_net(x, t_emb, condition)
            x_temp = x + dt * k1
            
            # ç¬¬äºŒæ­¥æ ¡æ­£
            t_next_tensor = torch.full((x.size(0),), t + dt, device=x.device)
            t_next_emb = model.time_embedding(t_next_tensor)
            k2 = model.velocity_net(x_temp, t_next_emb, condition)
            
            # æœ€ç»ˆç»“æœ
            return x + dt * (k1 + k2) / 2
    
    def rk4_step(self, model, x, t, dt, condition=None):
        """å››é˜¶Runge-Kuttaæ–¹æ³•"""
        with torch.no_grad():
            # k1
            t_tensor = torch.full((x.size(0),), t, device=x.device)
            t_emb = model.time_embedding(t_tensor)
            k1 = model.velocity_net(x, t_emb, condition)
            
            # k2
            x2 = x + dt * k1 / 2
            t2_tensor = torch.full((x.size(0),), t + dt/2, device=x.device)
            t2_emb = model.time_embedding(t2_tensor)
            k2 = model.velocity_net(x2, t2_emb, condition)
            
            # k3
            x3 = x + dt * k2 / 2
            k3 = model.velocity_net(x3, t2_emb, condition)
            
            # k4
            x4 = x + dt * k3
            t4_tensor = torch.full((x.size(0),), t + dt, device=x.device)
            t4_emb = model.time_embedding(t4_tensor)
            k4 = model.velocity_net(x4, t4_emb, condition)
            
            # æœ€ç»ˆç»“æœ
            return x + dt * (k1 + 2*k2 + 2*k3 + k4) / 6
    
    def adaptive_step(self, model, x, t, dt, condition=None, tol=1e-5):
        """è‡ªé€‚åº”æ­¥é•¿æ§åˆ¶"""
        # ä½¿ç”¨å…¨æ­¥é•¿
        x1 = self.rk4_step(model, x, t, dt, condition)
        
        # ä½¿ç”¨ä¸¤ä¸ªåŠæ­¥é•¿
        x_half = self.rk4_step(model, x, t, dt/2, condition)
        x2 = self.rk4_step(model, x_half, t + dt/2, dt/2, condition)
        
        # ä¼°è®¡è¯¯å·®
        error = torch.norm(x1 - x2, dim=-1).max()
        
        if error < tol:
            return x2, dt  # æ¥å—æ­¥é•¿
        else:
            # å‡å°æ­¥é•¿é‡æ–°è®¡ç®—
            new_dt = dt * 0.8 * (tol / error) ** 0.2
            return self.adaptive_step(model, x, t, new_dt, condition, tol)
```

---

## ğŸ”§ åˆ†é˜¶æ®µå®æ–½æ–¹æ¡ˆ

### Phase 1: æ ¸å¿ƒåŸºç¡€è®¾æ–½ (ç¬¬1-3å¤©)

#### 1.1 åˆ›å»ºé¡¹ç›®ç»“æ„

```bash
# ç›®æ ‡æ–‡ä»¶ç»“æ„
src/model_factory/ISFM/
â”œâ”€â”€ M_04_ISFM_Flow.py              # ä¸»é›†æˆæ¨¡å‹
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ GM_01_RectifiedFlow.py     # æ ¸å¿ƒæµæ¨¡å‹
â”‚   â”œâ”€â”€ E_03_ConditionalEncoder.py # æ¡ä»¶ç¼–ç å™¨
â”‚   â””â”€â”€ SequenceAdapter.py         # ç»´åº¦é€‚é…å™¨
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ flow_solvers.py            # ODEæ±‚è§£å™¨
â”‚   â”œâ”€â”€ flow_schedulers.py         # é‡‡æ ·è°ƒåº¦å™¨
â”‚   â”œâ”€â”€ metadata_extractor.py      # å…ƒæ•°æ®å¤„ç†
â”‚   â””â”€â”€ flow_metrics.py            # è¯„ä¼°æŒ‡æ ‡
â””â”€â”€ tests/
    â”œâ”€â”€ test_rectified_flow.py
    â”œâ”€â”€ test_conditional_encoder.py
    â”œâ”€â”€ test_sequence_adapter.py
    â””â”€â”€ test_integration.py
```

#### 1.2 å®æ–½æ­¥éª¤

**Day 1: ç»´åº¦é€‚é…å™¨å®ç°**
- [x] åˆ›å»º `SequenceAdapter` ç±»
- [x] å®ç° encode/decode æ–¹æ³•
- [x] æ·»åŠ å½¢çŠ¶éªŒè¯å’Œé”™è¯¯å¤„ç†
- [x] ç¼–å†™å•å…ƒæµ‹è¯• (10ä¸ªæµ‹è¯•ç”¨ä¾‹)

**Day 2: å…ƒæ•°æ®é›†æˆ**
- [x] åˆ›å»º `MetadataExtractor` ç±»  
- [x] å®ç°åŸŸ/ç³»ç»Ÿ/å®ä¾‹ä¿¡æ¯æå–
- [x] åˆ›å»ºæ˜ å°„è¡¨å’Œé…ç½®æ–‡ä»¶
- [x] æµ‹è¯•ä¸åŒæ•°æ®é›†å…¼å®¹æ€§

**Day 3: åŸºç¡€Flowæ¨¡å‹é€‚é…**
- [x] ä¿®æ”¹ `GM_01_RectifiedFlow` æ”¯æŒ3Då¼ é‡
- [x] é›†æˆ `SequenceAdapter`
- [x] æ›´æ–°æ‰€æœ‰è¾“å…¥è¾“å‡ºæ¥å£
- [x] éªŒè¯ç»´åº¦ä¸€è‡´æ€§

#### 1.3 éªŒæ”¶æ ‡å‡†
- âœ… æ‰€æœ‰å½¢çŠ¶æµ‹è¯•é€šè¿‡ (>95%è¦†ç›–ç‡)
- âœ… æ”¯æŒå¯å˜åºåˆ—é•¿åº¦ (512-4096)
- âœ… å†…å­˜ä½¿ç”¨åˆç† (<8GB for batch_size=32)
- âœ… ä¸ç°æœ‰æ•°æ®åŠ è½½å™¨å…¼å®¹

### Phase 2: æ¨¡å‹å¢å¼º (ç¬¬4-7å¤©)

#### 2.1 é«˜ç²¾åº¦ODEæ±‚è§£å™¨

**Day 4: æ±‚è§£å™¨å®ç°**
```python
# src/model_factory/ISFM/utils/flow_solvers.py
class FlowODESolver:
    # å®ç° euler, heun, rk4, adaptive æ–¹æ³•
    # æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
    # æ”¯æŒæ‰¹é‡å¤„ç†å’ŒGPUåŠ é€Ÿ
```

**Day 5: é€Ÿåº¦ç½‘ç»œå¢å¼º**
```python
# å¢å¼ºç‰ˆé€Ÿåº¦ç½‘ç»œ
class EnhancedVelocityNetwork(nn.Module):
    def __init__(self, ...):
        # æ·»åŠ æ®‹å·®è¿æ¥
        # å®ç°æ³¨æ„åŠ›æœºåˆ¶ (å¯é€‰)
        # æ·»åŠ LayerNormå’ŒDropout
        # æ”¯æŒå¯å˜éšè—å±‚æ•°
```

**Day 6: è®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–**
- æ¢¯åº¦è£å‰ª (grad_clip_norm=1.0)
- æƒé‡åˆå§‹åŒ–ç­–ç•¥ (Xavier/He)
- å­¦ä¹ ç‡è°ƒåº¦å™¨é›†æˆ
- NaN/Infæ£€æµ‹å’Œæ¢å¤

**Day 7: é‡‡æ ·è´¨é‡æå‡**
- å®ç°DDIMåŠ é€Ÿé‡‡æ ·
- æ·»åŠ åˆ†ç±»å™¨å¼•å¯¼ (å¯é€‰)
- å¤šæ­¥é¢„æµ‹æ ¡æ­£
- é‡‡æ ·è´¨é‡è¯„ä¼°

#### 2.2 éªŒæ”¶æ ‡å‡†
- âœ… è®­ç»ƒç¨³å®š (è¿ç»­è®­ç»ƒ50+ epochsæ— å´©æºƒ)
- âœ… é‡‡æ ·è´¨é‡æå‡ (FIDåˆ†æ•°æ”¹å–„>20%)
- âœ… æ¨ç†é€Ÿåº¦ä¼˜åŒ– (é‡‡æ ·æ—¶é—´<5ç§’/batch)
- âœ… æ•°å€¼ç¨³å®šæ€§ (æ— NaN/Infå¼‚å¸¸)

### Phase 3: ä»»åŠ¡é›†æˆ (ç¬¬8-10å¤©)

#### 3.1 æŸå¤±å‡½æ•°å®ç°

**Day 8: æ ¸å¿ƒæŸå¤±å‡½æ•°**
```python
# src/task_factory/loss/flow_loss.py
class RectifiedFlowLoss(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.flow_weight = config.get('flow_weight', 1.0)
        self.reg_weight = config.get('reg_weight', 0.01)
        self.consistency_weight = config.get('consistency_weight', 0.1)
    
    def forward(self, model_outputs, targets):
        # æµåŒ¹é…ä¸»æŸå¤±
        flow_loss = self.flow_matching_loss(
            model_outputs['v_pred'], 
            model_outputs['v_true']
        )
        
        # é€Ÿåº¦åœºæ­£åˆ™åŒ–
        reg_loss = self.velocity_regularization(model_outputs['v_pred'])
        
        # æ—¶é—´ä¸€è‡´æ€§æŸå¤±
        consistency_loss = self.temporal_consistency_loss(
            model_outputs['x_t'],
            model_outputs['t']
        )
        
        total_loss = (self.flow_weight * flow_loss + 
                     self.reg_weight * reg_loss +
                     self.consistency_weight * consistency_loss)
        
        return {
            'total_loss': total_loss,
            'flow_loss': flow_loss,
            'reg_loss': reg_loss,
            'consistency_loss': consistency_loss
        }
    
    def flow_matching_loss(self, v_pred, v_true):
        """æ ¸å¿ƒæµåŒ¹é…æŸå¤±"""
        return F.mse_loss(v_pred, v_true)
    
    def velocity_regularization(self, v_pred):
        """é€Ÿåº¦åœºæ­£åˆ™åŒ– - é˜²æ­¢è¿‡å¤§çš„é€Ÿåº¦"""
        return torch.mean(v_pred.pow(2))
    
    def temporal_consistency_loss(self, x_t, t):
        """æ—¶é—´ä¸€è‡´æ€§æŸå¤± - ç¡®ä¿è½¨è¿¹å¹³æ»‘"""
        # è®¡ç®—ç›¸é‚»æ—¶é—´æ­¥çš„å¹³æ»‘åº¦
        batch_size = x_t.size(0)
        if batch_size > 1:
            t_sorted, indices = torch.sort(t)
            x_sorted = x_t[indices]
            diff = x_sorted[1:] - x_sorted[:-1]
            t_diff = t_sorted[1:] - t_sorted[:-1] + 1e-8
            velocity_diff = diff / t_diff.unsqueeze(-1)
            return torch.mean(velocity_diff.pow(2))
        else:
            return torch.tensor(0.0, device=x_t.device)
```

#### 3.2 è®­ç»ƒä»»åŠ¡å°è£…

**Day 9: PyTorch Lightningä»»åŠ¡**
```python
# src/task_factory/task/pretrain_flow_task.py
class PretrainFlowTask(BaseTask):
    def __init__(self, config):
        super().__init__(config)
        self.model = self._build_model()
        self.loss_fn = RectifiedFlowLoss(config.loss)
        self.metadata_extractor = MetadataExtractor()
        
        # é‡‡æ ·é…ç½®
        self.sample_steps = config.get('sample_steps', 50)
        self.sample_schedule = config.get('sample_schedule', 'linear')
        
    def _build_model(self):
        # æ„å»ºå®Œæ•´çš„Flowæ¨¡å‹
        model_config = self.config.model
        return M_04_ISFM_Flow(model_config)
    
    def training_step(self, batch, batch_idx):
        x, metadata = batch
        batch_size = x.size(0)
        
        # æå–æ¡ä»¶ä¿¡æ¯
        conditions = []
        for i in range(batch_size):
            domain_id, system_id, instance_id = \
                self.metadata_extractor.extract_hierarchical_info(metadata[i])
            conditions.append([domain_id, system_id, instance_id])
        
        conditions = torch.tensor(conditions, device=x.device)
        
        # å‰å‘ä¼ æ’­
        outputs = self.model(x, conditions)
        
        # æŸå¤±è®¡ç®—
        losses = self.loss_fn(outputs, x)
        
        # è®°å½•æŒ‡æ ‡
        self.log_dict({
            'train_loss': losses['total_loss'],
            'flow_loss': losses['flow_loss'],
            'reg_loss': losses['reg_loss'],
            'consistency_loss': losses['consistency_loss']
        })
        
        return losses['total_loss']
    
    def validation_step(self, batch, batch_idx):
        x, metadata = batch
        
        # è®­ç»ƒæŸå¤±è¯„ä¼°
        with torch.no_grad():
            conditions = self._extract_conditions(x, metadata)
            outputs = self.model(x, conditions)
            losses = self.loss_fn(outputs, x)
        
        # ç”Ÿæˆè´¨é‡è¯„ä¼° (æ¯10ä¸ªbatchæ‰§è¡Œä¸€æ¬¡)
        if batch_idx % 10 == 0:
            self._evaluate_generation_quality(x, conditions)
        
        self.log_dict({
            'val_loss': losses['total_loss'],
            'val_flow_loss': losses['flow_loss']
        })
        
        return losses['total_loss']
    
    def _evaluate_generation_quality(self, real_samples, conditions):
        """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        batch_size = real_samples.size(0)
        
        # ç”Ÿæˆç›¸åŒæ•°é‡çš„æ ·æœ¬
        with torch.no_grad():
            generated = self.model.sample(
                batch_size=batch_size,
                condition=conditions,
                num_steps=self.sample_steps
            )
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mse = F.mse_loss(generated, real_samples)
        
        # é¢‘åŸŸç›¸ä¼¼æ€§
        freq_similarity = self._compute_frequency_similarity(
            generated, real_samples
        )
        
        self.log_dict({
            'gen_mse': mse,
            'freq_similarity': freq_similarity
        })
    
    def _compute_frequency_similarity(self, gen_samples, real_samples):
        """è®¡ç®—é¢‘åŸŸç›¸ä¼¼æ€§"""
        # è®¡ç®—åŠŸç‡è°±å¯†åº¦
        gen_fft = torch.fft.fft(gen_samples, dim=-2)
        real_fft = torch.fft.fft(real_samples, dim=-2)
        
        gen_psd = torch.abs(gen_fft).pow(2)
        real_psd = torch.abs(real_fft).pow(2)
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = F.cosine_similarity(
            gen_psd.flatten(1), 
            real_psd.flatten(1), 
            dim=1
        )
        
        return correlation.mean()
    
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config.optimizer.lr,
            weight_decay=self.config.optimizer.weight_decay,
            betas=self.config.optimizer.betas
        )
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.config.trainer.max_epochs,
            eta_min=self.config.optimizer.lr * 0.01
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": "val_loss",
            },
        }
```

**Day 10: é…ç½®æ–‡ä»¶å’Œé›†æˆ**
- åˆ›å»ºYAMLé…ç½®æ¨¡æ¿
- æ³¨å†Œåˆ°TaskFactory
- ç«¯åˆ°ç«¯æµ‹è¯•
- æ€§èƒ½è°ƒä¼˜

#### 3.3 éªŒæ”¶æ ‡å‡†
- âœ… ç«¯åˆ°ç«¯è®­ç»ƒæˆåŠŸ (10+ epochs)
- âœ… æŸå¤±å‡½æ•°æ”¶æ•›ç¨³å®š
- âœ… ç”Ÿæˆæ ·æœ¬è´¨é‡å¯æ¥å— (ç›®è§†æ£€æŸ¥)
- âœ… è®­ç»ƒé€Ÿåº¦æ»¡è¶³è¦æ±‚ (>50 iter/s)

### Phase 4: æµ‹è¯•éªŒè¯ (ç¬¬11-14å¤©)

#### 4.1 å…¨é¢æµ‹è¯•å¥—ä»¶

**Day 11: å•å…ƒæµ‹è¯•æ‰©å±•**
```python
# tests/test_flow_model.py
class TestFlowModel:
    def setup_method(self):
        self.config = self._create_test_config()
        self.model = M_04_ISFM_Flow(self.config)
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
    def test_dimension_compatibility(self):
        # æµ‹è¯•ä¸åŒè¾“å…¥å½¢çŠ¶
        for seq_len in [512, 1024, 2048]:
            for channels in [1, 2, 3]:
                x = torch.randn(4, seq_len, channels)
                output = self.model(x)
                assert output['v_pred'].shape == (4, seq_len * channels)
    
    def test_condition_encoding(self):
        # æµ‹è¯•æ¡ä»¶ç¼–ç åŠŸèƒ½
        batch_size = 8
        x = torch.randn(batch_size, 1024, 1)
        conditions = torch.randint(0, 5, (batch_size, 3))  # domain, system, instance
        
        output = self.model(x, conditions)
        assert 'condition_features' in output
        assert output['condition_features'].shape == (batch_size, self.config.condition_dim)
    
    def test_numerical_stability(self):
        # æµ‹è¯•æ•°å€¼ç¨³å®šæ€§
        extreme_inputs = [
            torch.ones(2, 1024, 1) * 1000,  # æå¤§å€¼
            torch.ones(2, 1024, 1) * -1000,  # æå°å€¼
            torch.zeros(2, 1024, 1),  # é›¶å€¼
            torch.randn(2, 1024, 1) * 0.001  # æå°éšæœºå€¼
        ]
        
        for x in extreme_inputs:
            output = self.model(x)
            assert not torch.isnan(output['v_pred']).any()
            assert not torch.isinf(output['v_pred']).any()
    
    def test_gradient_flow(self):
        # æµ‹è¯•æ¢¯åº¦æµåŠ¨
        x = torch.randn(4, 1024, 1, requires_grad=True)
        output = self.model(x)
        loss = output['v_pred'].sum()
        loss.backward()
        
        # æ£€æŸ¥æ‰€æœ‰å‚æ•°éƒ½æœ‰æ¢¯åº¦
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert param.grad is not None, f"No gradient for {name}"
                assert not torch.isnan(param.grad).any(), f"NaN gradient for {name}"
```

**Day 12: é›†æˆæµ‹è¯•**
- å¤šæ•°æ®é›†å…¼å®¹æ€§æµ‹è¯•
- åˆ†å¸ƒå¼è®­ç»ƒæµ‹è¯•
- å†…å­˜æ³„æ¼æ£€æµ‹
- ç«¯åˆ°ç«¯pipelineæµ‹è¯•

**Day 13: æ€§èƒ½åŸºå‡†æµ‹è¯•**
```python
# benchmarks/flow_benchmark.py
class FlowBenchmark:
    def benchmark_training_speed(self):
        # æµ‹è¯•è®­ç»ƒé€Ÿåº¦
        model = M_04_ISFM_Flow(config)
        dataloader = self._create_test_dataloader()
        
        start_time = time.time()
        for i, batch in enumerate(dataloader):
            if i >= 100:  # æµ‹è¯•100ä¸ªbatch
                break
            outputs = model(batch[0])
            loss = outputs['v_pred'].sum()
            loss.backward()
            model.zero_grad()
        
        elapsed = time.time() - start_time
        speed = 100 / elapsed
        assert speed > 50, f"Training too slow: {speed:.2f} iter/s"
    
    def benchmark_memory_usage(self):
        # æµ‹è¯•å†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
            
            model = M_04_ISFM_Flow(config).cuda()
            x = torch.randn(32, 1024, 1).cuda()
            outputs = model(x)
            
            peak_memory = torch.cuda.memory_allocated()
            memory_usage = (peak_memory - initial_memory) / 1024**3  # GB
            
            assert memory_usage < 8.0, f"Memory usage too high: {memory_usage:.2f}GB"
```

**Day 14: è´¨é‡éªŒè¯å’Œæ–‡æ¡£**
- ç”Ÿæˆè´¨é‡è¯„ä¼° (FID, ISç­‰æŒ‡æ ‡)
- ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æµ‹è¯•
- å®Œå–„æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹
- æ€§èƒ½è°ƒä¼˜å»ºè®®

#### 4.2 éªŒæ”¶æ ‡å‡†
- âœ… æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡ (>98%é€šè¿‡ç‡)
- âœ… é›†æˆæµ‹è¯•ç¨³å®š (å¤šæ¬¡è¿è¡Œä¸€è‡´)
- âœ… æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡ (é€Ÿåº¦ã€å†…å­˜ã€è´¨é‡)
- âœ… æ–‡æ¡£å®Œæ•´å¯ç”¨

---

## ğŸ“‹ é…ç½®æ–‡ä»¶æ¨¡æ¿

### åŸºç¡€é…ç½® (configs/pretrain/flow_base.yaml)

```yaml
# Flowé¢„è®­ç»ƒåŸºç¡€é…ç½®
model:
  name: M_04_ISFM_Flow
  
  # åºåˆ—å‚æ•°
  sequence_length: 1024
  channels: 1
  
  # Flowæ¨¡å‹å‚æ•°
  latent_dim: 512  # sequence_length * channels 
  condition_dim: 128
  hidden_dim: 256
  time_dim: 64
  num_layers: 4
  dropout: 0.1
  activation: 'silu'
  
  # å™ªå£°å‚æ•°
  sigma_min: 0.001
  sigma_max: 1.0
  
  # ODEæ±‚è§£å™¨
  solver_type: 'heun'  # euler, heun, rk4, adaptive
  
  # æ¡ä»¶ç¼–ç å™¨
  num_domains: 10
  num_systems: 10
  fusion_type: 'attention'  # attention, gating, concatenate, average
  use_domain: true
  use_system: true
  use_instance: true

task:
  name: pretrain_flow
  
  # æŸå¤±å‚æ•°
  loss:
    flow_weight: 1.0
    reg_weight: 0.01
    consistency_weight: 0.1
    loss_type: 'mse'  # mse, huber, mae
  
  # è®­ç»ƒå‚æ•°
  optimizer:
    name: 'AdamW'
    lr: 1e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]
    grad_clip_norm: 1.0
  
  scheduler:
    name: 'CosineAnnealingLR'
    eta_min_factor: 0.01
  
  # é‡‡æ ·å‚æ•°
  sampling:
    num_steps: 50
    schedule: 'linear'  # linear, cosine
    guidance_scale: 1.0

trainer:
  max_epochs: 100
  batch_size: 32
  num_workers: 8
  precision: 16
  gradient_clip_val: 1.0
  
  # éªŒè¯å‚æ•°
  val_check_interval: 0.5
  check_val_every_n_epoch: 1
  
  # å›è°ƒå‡½æ•°
  callbacks:
    - EarlyStopping:
        monitor: 'val_loss'
        patience: 10
        min_delta: 0.001
    - ModelCheckpoint:
        monitor: 'val_loss'
        save_top_k: 3
        save_last: true
    - LearningRateMonitor:
        logging_interval: 'step'

data:
  datasets: ['CWRU']  # å¼€å§‹ç”¨å•æ•°æ®é›†æµ‹è¯•
  data_dir: 'data/'
  sequence_length: 1024
  overlap_ratio: 0.5
  normalize: true
  augmentation: false
```

### å¤šæ•°æ®é›†é…ç½® (configs/pretrain/flow_multi_dataset.yaml)

```yaml
# ç»§æ‰¿åŸºç¡€é…ç½®
inherit_from: 'flow_base.yaml'

# å¤šæ•°æ®é›†ç‰¹å®šè®¾ç½®
data:
  datasets: ['CWRU', 'XJTU', 'PU', 'FEMTO']
  sampling_strategy: 'balanced'  # uniform, weighted, balanced
  cross_domain_ratio: 0.3

trainer:
  max_epochs: 200  # æ›´å¤šepochå¤„ç†å¤šæ ·æ€§
  batch_size: 64   # æ›´å¤§batch size

model:
  num_domains: 20  # æ”¯æŒæ›´å¤šåŸŸ
  num_systems: 20
  condition_dim: 256  # æ›´å¤§æ¡ä»¶ç©ºé—´

task:
  loss:
    domain_adversarial_weight: 0.05  # æ·»åŠ åŸŸå¯¹æŠ—æŸå¤±
    contrastive_weight: 0.02         # æ·»åŠ å¯¹æ¯”å­¦ä¹ æŸå¤±
```

---

## ğŸ¯ è´¨é‡ä¿è¯æªæ–½

### 1. ä»£ç è´¨é‡æ ‡å‡†

#### ç±»å‹æ³¨è§£è¦æ±‚
```python
from typing import Dict, Any, Optional, Tuple, Union, List
import torch
from torch import Tensor

def sample(self, 
          batch_size: int, 
          condition: Optional[Tensor] = None,
          num_steps: int = 50, 
          device: str = 'cuda',
          return_trajectory: bool = False) -> Union[Tensor, Tuple[Tensor, ...]]:
    """
    é‡‡æ ·ç”Ÿæˆæ–°æ•°æ®
    
    Args:
        batch_size: æ‰¹é‡å¤§å°
        condition: æ¡ä»¶ä¿¡æ¯ (batch_size, condition_dim)
        num_steps: é‡‡æ ·æ­¥æ•°
        device: è®¡ç®—è®¾å¤‡
        return_trajectory: æ˜¯å¦è¿”å›å®Œæ•´è½¨è¿¹
        
    Returns:
        samples: ç”Ÿæˆçš„æ ·æœ¬ (batch_size, latent_dim)
        æˆ– (trajectory, samples): å®Œæ•´è½¨è¿¹å’Œæœ€ç»ˆæ ·æœ¬
        
    Raises:
        ValueError: å½“batch_size <= 0æ—¶
        RuntimeError: å½“è®¾å¤‡ä¸å¯ç”¨æ—¶
    """
```

#### é”™è¯¯å¤„ç†æ ‡å‡†
```python
class FlowModelError(Exception):
    """Flowæ¨¡å‹ç›¸å…³é”™è¯¯çš„åŸºç±»"""
    pass

class DimensionMismatchError(FlowModelError):
    """ç»´åº¦ä¸åŒ¹é…é”™è¯¯"""
    pass

class NumericalInstabilityError(FlowModelError):
    """æ•°å€¼ä¸ç¨³å®šé”™è¯¯"""
    pass

def _validate_input(self, x: Tensor) -> None:
    """è¾“å…¥éªŒè¯"""
    if x.dim() != 3:
        raise DimensionMismatchError(
            f"Expected 3D tensor (B, L, C), got {x.dim()}D tensor with shape {x.shape}"
        )
    
    if torch.isnan(x).any():
        raise ValueError("Input contains NaN values")
    
    if torch.isinf(x).any():
        raise ValueError("Input contains infinite values")
```

#### æ—¥å¿—æ ‡å‡†
```python
import logging
from src.utils.logger import get_logger

logger = get_logger(__name__)

class GM_01_RectifiedFlow(nn.Module):
    def __init__(self, args_m):
        super().__init__()
        logger.info(f"Initializing RectifiedFlow with config: {args_m}")
        
        # åˆå§‹åŒ–ä»£ç ...
        
        logger.info(f"RectifiedFlow initialized successfully. "
                   f"Parameters: {sum(p.numel() for p in self.parameters()):,}")
    
    def forward(self, x, condition=None):
        logger.debug(f"Forward pass: x.shape={x.shape}, "
                    f"condition.shape={condition.shape if condition is not None else None}")
        
        try:
            # å‰å‘ä¼ æ’­é€»è¾‘...
            return outputs
        except Exception as e:
            logger.error(f"Forward pass failed: {e}")
            raise
```

### 2. æ€§èƒ½ç›‘æ§

#### è®­ç»ƒç›‘æ§æŒ‡æ ‡
```python
class FlowTrainingMonitor:
    """Flowæ¨¡å‹è®­ç»ƒç›‘æ§"""
    
    def __init__(self):
        self.metrics = {
            'loss_history': [],
            'gradient_norms': [],
            'parameter_norms': [],
            'generation_quality': [],
            'memory_usage': [],
            'training_speed': []
        }
    
    def log_training_step(self, model, loss, batch_idx):
        # è®°å½•æŸå¤±
        self.metrics['loss_history'].append(loss.item())
        
        # è®°å½•æ¢¯åº¦èŒƒæ•°
        total_norm = 0
        param_count = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
        
        total_norm = total_norm ** (1. / 2)
        self.metrics['gradient_norms'].append(total_norm)
        
        # è®°å½•å‚æ•°èŒƒæ•°
        param_norm = sum(p.data.norm(2).item() ** 2 for p in model.parameters()) ** 0.5
        self.metrics['parameter_norms'].append(param_norm)
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        if total_norm > 100:
            logger.warning(f"Large gradient norm detected: {total_norm:.6f}")
        
        if torch.isnan(loss):
            logger.error(f"NaN loss detected at batch {batch_idx}")
            raise NumericalInstabilityError("Training loss became NaN")
```



---

## ğŸ“ˆ é£é™©ç®¡ç†å’Œåº”å¯¹ç­–ç•¥

### é£é™©è¯†åˆ«çŸ©é˜µ

| é£é™©ç±»åˆ« | é£é™©æè¿° | æ¦‚ç‡ | å½±å“ | é£é™©ç­‰çº§ | åº”å¯¹ç­–ç•¥ |
|----------|----------|------|------|----------|----------|
| **æŠ€æœ¯é£é™©** | ç»´åº¦ä¸å…¼å®¹å¯¼è‡´é›†æˆå¤±è´¥ | ä¸­ | é«˜ | ğŸ”´é«˜ | åˆ†é˜¶æ®µéªŒè¯ï¼Œæå‰æµ‹è¯• |
| **æ€§èƒ½é£é™©** | è®­ç»ƒé€Ÿåº¦è¿‡æ…¢å½±å“å®ç”¨æ€§ | é«˜ | ä¸­ | ğŸŸ¡ä¸­ | æ€§èƒ½å‰–æï¼Œä»£ç ä¼˜åŒ– |
| **ç¨³å®šæ€§é£é™©** | æ•°å€¼ä¸ç¨³å®šå¯¼è‡´è®­ç»ƒå´©æºƒ | ä¸­ | é«˜ | ğŸ”´é«˜ | æ¢¯åº¦è£å‰ªï¼Œæ•°å€¼æ£€æŸ¥ |
| **å…¼å®¹æ€§é£é™©** | ä¸ç°æœ‰æ¡†æ¶å†²çª | ä½ | é«˜ | ğŸŸ¡ä¸­ | ä¸¥æ ¼éµå¾ªå·¥å‚æ¨¡å¼ |
| **è´¨é‡é£é™©** | ç”Ÿæˆè´¨é‡ä¸è¾¾æ ‡ | ä¸­ | ä¸­ | ğŸŸ¡ä¸­ | å¤šæŒ‡æ ‡è¯„ä¼°ï¼Œè¿­ä»£æ”¹è¿› |
| **èµ„æºé£é™©** | å†…å­˜/æ˜¾å­˜ä¸è¶³ | é«˜ | ä¸­ | ğŸŸ¡ä¸­ | å†…å­˜ä¼˜åŒ–ï¼Œæ‰¹é‡è°ƒæ•´ |

### è¯¦ç»†åº”å¯¹æªæ–½

#### 1. ç»´åº¦å…¼å®¹æ€§é£é™©
**é£é™©æè¿°**: Flowæ¨¡å‹æœŸæœ›çš„è¾“å…¥æ ¼å¼ä¸PHM-Vibenchä¸åŒ¹é…

**é¢„é˜²æªæ–½**:
- âœ… ç¬¬ä¸€é˜¶æ®µä¼˜å…ˆè§£å†³ç»´åº¦é€‚é…
- âœ… åˆ›å»ºå…¨é¢çš„å½¢çŠ¶æµ‹è¯•å¥—ä»¶
- âœ… å®ç°çµæ´»çš„åºåˆ—é€‚é…å™¨

**åº”æ€¥é¢„æ¡ˆ**:
```python
# å¦‚æœç›´æ¥é€‚é…å¤±è´¥ï¼Œä½¿ç”¨æ¸è¿›å¼æ–¹æ¡ˆ
class GradualDimensionAdapter:
    def __init__(self):
        self.adaptation_strategies = [
            'direct_flatten',      # ç›´æ¥å±•å¼€
            'conv_reduction',      # å·ç§¯é™ç»´
            'attention_pooling',   # æ³¨æ„åŠ›æ± åŒ–
            'rnn_encoding'         # RNNç¼–ç 
        ]
    
    def try_adaptation(self, x, strategy):
        # å°è¯•ä¸åŒçš„é€‚é…ç­–ç•¥
        pass
```

#### 2. è®­ç»ƒç¨³å®šæ€§é£é™©
**é£é™©æè¿°**: æ•°å€¼ä¸ç¨³å®šå¯¼è‡´NaN/Infå¼‚å¸¸

**é¢„é˜²æªæ–½**:
```python
class StabilityGuard:
    def __init__(self, model):
        self.model = model
        self.nan_count = 0
        self.max_nan_tolerance = 5
    
    def check_and_fix(self, loss, optimizer):
        if torch.isnan(loss):
            self.nan_count += 1
            logger.warning(f"NaN detected, count: {self.nan_count}")
            
            if self.nan_count >= self.max_nan_tolerance:
                raise NumericalInstabilityError("Too many NaN occurrences")
            
            # é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€
            optimizer.zero_grad()
            
            # é‡æ–°åˆå§‹åŒ–éƒ¨åˆ†å‚æ•°
            self._reinitialize_unstable_params()
            
            return True  # è·³è¿‡è¿™ä¸ªbatch
        else:
            self.nan_count = 0
            return False
    
    def _reinitialize_unstable_params(self):
        # é‡æ–°åˆå§‹åŒ–å¯èƒ½ä¸ç¨³å®šçš„å±‚
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear) and 'velocity' in name:
                nn.init.xavier_uniform_(module.weight, gain=0.1)  # æ›´å°çš„åˆå§‹åŒ–
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
```

#### 3. æ€§èƒ½ä¼˜åŒ–é£é™©
**é£é™©æè¿°**: è®­ç»ƒæˆ–æ¨ç†é€Ÿåº¦æ— æ³•æ»¡è¶³å®ç”¨è¦æ±‚

**æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**:
```python
class PerformanceOptimizer:
    def __init__(self, model):
        self.model = model
        self.profiler = torch.profiler.profile()
    
    def optimize_for_speed(self):
        # 1. ç¼–è¯‘ä¼˜åŒ–
        if hasattr(torch, 'compile'):
            self.model = torch.compile(self.model)
        
        # 2. æ··åˆç²¾åº¦
        self.enable_mixed_precision()
        
        # 3. æ¢¯åº¦æ£€æŸ¥ç‚¹
        self.enable_gradient_checkpointing()
        
        # 4. å†…å­˜ä¼˜åŒ–
        self.optimize_memory_usage()
    
    def enable_mixed_precision(self):
        # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        for module in self.model.modules():
            if isinstance(module, (nn.Linear, nn.Conv1d)):
                module.half()
    
    def profile_and_optimize(self):
        # æ€§èƒ½åˆ†æå’Œç“¶é¢ˆè¯†åˆ«
        with self.profiler:
            # è¿è¡Œæµ‹è¯•ä»£ç 
            pass
        
        # åˆ†æç»“æœå¹¶æå‡ºä¼˜åŒ–å»ºè®®
        self.profiler.export_chrome_trace("flow_profile.json")
```

---

## ğŸ¯ æˆåŠŸæ ‡å‡†å’ŒéªŒæ”¶æ¡ä»¶

### 1. åŠŸèƒ½å®Œæ•´æ€§æ ‡å‡†

#### æ ¸å¿ƒåŠŸèƒ½éªŒæ”¶
- âœ… **ç»´åº¦å…¼å®¹æ€§**: æ”¯æŒ(B, L, C)æ ¼å¼è¾“å…¥ï¼ŒLâˆˆ[512, 4096], Câˆˆ[1, 3]
- âœ… **æ¡ä»¶ç”Ÿæˆ**: åŸºäºåŸŸ/ç³»ç»Ÿ/å®ä¾‹çš„å±‚æ¬¡åŒ–æ¡ä»¶ç”Ÿæˆ
- âœ… **æ— æ¡ä»¶ç”Ÿæˆ**: é«˜è´¨é‡çš„æ— æ¡ä»¶æ ·æœ¬ç”Ÿæˆ
- âœ… **é‡‡æ ·å¤šæ ·æ€§**: å¤šç§ODEæ±‚è§£å™¨(Euler, Heun, RK4)
- âœ… **æ’å€¼åŠŸèƒ½**: æ ·æœ¬é—´å¹³æ»‘æ’å€¼
- âœ… **ä¼¼ç„¶ä¼°è®¡**: ç”¨äºå¼‚å¸¸æ£€æµ‹çš„æ¦‚ç‡ä¼°è®¡

#### é›†æˆå…¼å®¹æ€§éªŒæ”¶
- âœ… **å·¥å‚æ³¨å†Œ**: åœ¨ModelFactoryä¸­æ­£ç¡®æ³¨å†Œ
- âœ… **é…ç½®é©±åŠ¨**: é€šè¿‡YAMLæ–‡ä»¶å®Œå…¨é…ç½®
- âœ… **æ•°æ®åŠ è½½**: ä¸ç°æœ‰DataFactoryæ— ç¼é›†æˆ
- âœ… **ä»»åŠ¡æ”¯æŒ**: é›†æˆåˆ°TaskFactoryè®­ç»ƒæµç¨‹
- âœ… **å¤šGPU**: æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ

### 2. æ€§èƒ½æŒ‡æ ‡æ ‡å‡†

#### è®­ç»ƒæ€§èƒ½è¦æ±‚
```python
class PerformanceStandards:
    TRAINING_SPEED_MIN = 50      # iterations/second
    MEMORY_USAGE_MAX = 8.0       # GB for batch_size=32
    CONVERGENCE_EPOCHS_MAX = 50  # epochs to converge
    GPU_UTILIZATION_MIN = 0.85   # GPU utilization rate
    
    @staticmethod
    def validate_performance(metrics):
        assert metrics['training_speed'] >= PerformanceStandards.TRAINING_SPEED_MIN
        assert metrics['memory_usage'] <= PerformanceStandards.MEMORY_USAGE_MAX
        assert metrics['gpu_utilization'] >= PerformanceStandards.GPU_UTILIZATION_MIN
```

#### ç”Ÿæˆè´¨é‡è¦æ±‚
```python
class QualityStandards:
    FID_SCORE_MAX = 50.0           # Frechet Inception Distance
    FREQ_SIMILARITY_MIN = 0.8      # é¢‘åŸŸç›¸ä¼¼æ€§
    MSE_RECONSTRUCTION_MAX = 0.1   # é‡å»ºè¯¯å·®
    SAMPLE_DIVERSITY_MIN = 0.7     # æ ·æœ¬å¤šæ ·æ€§(é€šè¿‡èšç±»è¯„ä¼°)
    
    @staticmethod
    def evaluate_generation_quality(real_samples, generated_samples):
        fid_score = calculate_fid(real_samples, generated_samples)
        freq_sim = calculate_frequency_similarity(real_samples, generated_samples)
        mse = F.mse_loss(real_samples, generated_samples)
        diversity = calculate_sample_diversity(generated_samples)
        
        return {
            'fid_score': fid_score,
            'frequency_similarity': freq_sim,
            'mse_reconstruction': mse.item(),
            'sample_diversity': diversity
        }
```

### 3. ä¸‹æ¸¸ä»»åŠ¡æå‡æ ‡å‡†

#### Few-Shotå­¦ä¹ æå‡
```python
def evaluate_fewshot_improvement(original_results, flow_augmented_results):
    """è¯„ä¼°Flowæ•°æ®å¢å¼ºå¯¹Few-Shotå­¦ä¹ çš„æå‡"""
    improvement = {}
    
    for shots in [1, 5, 10]:
        original_acc = original_results[f'{shots}_shot_accuracy']
        augmented_acc = flow_augmented_results[f'{shots}_shot_accuracy']
        
        improvement[f'{shots}_shot'] = (augmented_acc - original_acc) / original_acc
    
    # è¦æ±‚è‡³å°‘10%çš„æå‡
    assert all(imp >= 0.1 for imp in improvement.values()), \
           f"Few-shot improvements not meeting 10% threshold: {improvement}"
    
    return improvement
```

#### åŸŸé€‚åº”æ€§èƒ½æå‡
```python
def evaluate_domain_adaptation_improvement(baseline_results, flow_results):
    """è¯„ä¼°Flowæ¨¡å‹å¯¹åŸŸé€‚åº”çš„æå‡"""
    
    adaptation_metrics = {}
    
    for target_domain in ['XJTU', 'PU', 'FEMTO']:
        baseline_acc = baseline_results[f'adapt_to_{target_domain}']
        flow_acc = flow_results[f'adapt_to_{target_domain}']
        
        improvement = (flow_acc - baseline_acc) / baseline_acc
        adaptation_metrics[target_domain] = improvement
        
        # è¦æ±‚è‡³å°‘15%çš„æå‡
        assert improvement >= 0.15, \
               f"Domain adaptation to {target_domain} improvement too low: {improvement:.3f}"
    
    return adaptation_metrics
```

### 4. ç¨³å®šæ€§å’Œå¯é æ€§æ ‡å‡†

#### é•¿æœŸè®­ç»ƒç¨³å®šæ€§
```python
class StabilityTest:
    def __init__(self, model, dataloader):
        self.model = model
        self.dataloader = dataloader
        
    def long_term_stability_test(self, epochs=100):
        """é•¿æœŸè®­ç»ƒç¨³å®šæ€§æµ‹è¯•"""
        loss_history = []
        nan_count = 0
        
        for epoch in range(epochs):
            epoch_losses = []
            
            for batch in self.dataloader:
                outputs = self.model(batch[0])
                loss = outputs['total_loss']
                
                if torch.isnan(loss):
                    nan_count += 1
                    if nan_count > 5:  # å®¹å¿åº¦
                        raise Exception(f"Too many NaN losses: {nan_count}")
                    continue
                
                loss.backward()
                epoch_losses.append(loss.item())
                self.model.zero_grad()
            
            avg_loss = np.mean(epoch_losses)
            loss_history.append(avg_loss)
            
            # æ£€æŸ¥è®­ç»ƒè¿›å±•
            if epoch > 20:
                recent_trend = np.polyfit(range(len(loss_history[-20:])), 
                                        loss_history[-20:], 1)[0]
                if recent_trend > 0.01:  # æŸå¤±ä¸åº”è¯¥æŒç»­ä¸Šå‡
                    logger.warning(f"Loss trend upward: {recent_trend:.6f}")
        
        return {
            'nan_count': nan_count,
            'final_loss': loss_history[-1],
            'loss_stability': np.std(loss_history[-20:])  # æœ€å20ä¸ªepochçš„ç¨³å®šæ€§
        }
```

---

## ğŸ“š æ–‡æ¡£å’ŒçŸ¥è¯†ç®¡ç†

### 1. æŠ€æœ¯æ–‡æ¡£ç»“æ„

```
docs/flow_model/
â”œâ”€â”€ README.md                    # å¿«é€Ÿå…¥é—¨æŒ‡å—
â”œâ”€â”€ architecture.md              # æŠ€æœ¯æ¶æ„è¯¦è§£
â”œâ”€â”€ api_reference.md             # APIå‚è€ƒæ–‡æ¡£
â”œâ”€â”€ tutorials/                   # æ•™ç¨‹æ–‡æ¡£
â”‚   â”œâ”€â”€ basic_usage.md
â”‚   â”œâ”€â”€ advanced_configurations.md
â”‚   â””â”€â”€ troubleshooting.md
â”œâ”€â”€ examples/                    # ç¤ºä¾‹ä»£ç 
â”‚   â”œâ”€â”€ single_dataset_training.py
â”‚   â”œâ”€â”€ multi_dataset_pretraining.py
â”‚   â””â”€â”€ custom_conditions.py
â””â”€â”€ benchmarks/                  # æ€§èƒ½åŸºå‡†
    â”œâ”€â”€ speed_benchmarks.md
    â”œâ”€â”€ quality_evaluations.md
    â””â”€â”€ comparison_with_baselines.md
```

### 2. ä½¿ç”¨æŒ‡å—æ¨¡æ¿

#### å¿«é€Ÿå…¥é—¨ (README.md)
```markdown
# Flow-based Pretraining for PHM-Vibench

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### åŸºç¡€ä½¿ç”¨
```python
from src.configs import load_config
from src.model_factory import get_model
from src.task_factory import get_task

# åŠ è½½é…ç½®
config = load_config('configs/pretrain/flow_base.yaml')

# åˆ›å»ºæ¨¡å‹å’Œä»»åŠ¡
model = get_model(config.model)
task = get_task(config.task)

# å¼€å§‹è®­ç»ƒ
trainer = pl.Trainer(**config.trainer)
trainer.fit(task)
```

### é…ç½®è¯´æ˜
- `model.latent_dim`: æ½œåœ¨ç»´åº¦ï¼Œé€šå¸¸ç­‰äºsequence_length Ã— channels
- `model.condition_dim`: æ¡ä»¶ç¼–ç ç»´åº¦ï¼Œå½±å“æ¡ä»¶è¡¨è¾¾èƒ½åŠ›
- `task.loss.flow_weight`: æµåŒ¹é…æŸå¤±æƒé‡
- `trainer.precision`: 16è¡¨ç¤ºåŠç²¾åº¦ï¼Œå¯æé€Ÿå¹¶èŠ‚çœæ˜¾å­˜
```

#### æ•…éšœæ’é™¤æŒ‡å— (troubleshooting.md)
```markdown
# Flowæ¨¡å‹æ•…éšœæ’é™¤æŒ‡å—

## å¸¸è§é—®é¢˜

### 1. è®­ç»ƒæ—¶å‡ºç°NaNæŸå¤±
**ç—‡çŠ¶**: `loss = nan, grad_norm = inf`
**åŸå› **: æ•°å€¼ä¸ç¨³å®šï¼Œé€šå¸¸ç”±å­¦ä¹ ç‡è¿‡å¤§æˆ–æ¢¯åº¦çˆ†ç‚¸å¼•èµ·
**è§£å†³æ–¹æ¡ˆ**:
```yaml
task:
  optimizer:
    lr: 1e-5  # é™ä½å­¦ä¹ ç‡
    grad_clip_norm: 0.5  # å‡å°æ¢¯åº¦è£å‰ªé˜ˆå€¼
  
  loss:
    flow_weight: 0.5  # é™ä½æŸå¤±æƒé‡
```

### 2. ç”Ÿæˆæ ·æœ¬è´¨é‡å·®
**ç—‡çŠ¶**: ç”Ÿæˆçš„æŒ¯åŠ¨ä¿¡å·ç¼ºä¹çœŸå®æ„Ÿï¼Œé¢‘è°±å¼‚å¸¸
**åŸå› **: é‡‡æ ·æ­¥æ•°ä¸è¶³æˆ–æ¨¡å‹å®¹é‡ä¸å¤Ÿ
**è§£å†³æ–¹æ¡ˆ**:
```yaml
model:
  hidden_dim: 512  # å¢åŠ æ¨¡å‹å®¹é‡
  num_layers: 6
  
task:
  sampling:
    num_steps: 100  # å¢åŠ é‡‡æ ·æ­¥æ•°
    solver_type: 'rk4'  # ä½¿ç”¨é«˜ç²¾åº¦æ±‚è§£å™¨
```

### 3. å†…å­˜ä¸è¶³é”™è¯¯
**ç—‡çŠ¶**: `CUDA out of memory`
**è§£å†³æ–¹æ¡ˆ**:
```yaml
trainer:
  batch_size: 16  # å‡å°æ‰¹é‡å¤§å°
  precision: 16   # ä½¿ç”¨åŠç²¾åº¦
  gradient_clip_val: 1.0
  
# æˆ–å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model:
  use_gradient_checkpointing: true
```
```

---

## ğŸš€ å®æ–½æ—¶é—´è¡¨å’Œé‡Œç¨‹ç¢‘

### æ€»ä½“æ—¶é—´çº¿ (14å¤©)

```mermaid
gantt
    title Flowæ¨¡å‹å®æ–½æ—¶é—´è¡¨
    dateFormat  YYYY-MM-DD
    section Phase 1: åŸºç¡€æ¶æ„
    ç»´åº¦é€‚é…å™¨å®ç°        :active, p1-1, 2025-08-30, 1d
    å…ƒæ•°æ®é›†æˆç³»ç»Ÿ        :p1-2, after p1-1, 1d  
    Flowæ¨¡å‹åŸºç¡€é€‚é…      :p1-3, after p1-2, 1d
    
    section Phase 2: æ¨¡å‹å¢å¼º
    ODEæ±‚è§£å™¨å®ç°         :p2-1, after p1-3, 1d
    é€Ÿåº¦ç½‘ç»œå¢å¼º          :p2-2, after p2-1, 1d
    è®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–        :p2-3, after p2-2, 1d
    é‡‡æ ·è´¨é‡æå‡          :p2-4, after p2-3, 1d
    
    section Phase 3: ä»»åŠ¡é›†æˆ
    æŸå¤±å‡½æ•°å®ç°          :p3-1, after p2-4, 1d
    è®­ç»ƒä»»åŠ¡å°è£…          :p3-2, after p3-1, 1d
    é…ç½®æ–‡ä»¶å’Œé›†æˆ        :p3-3, after p3-2, 1d
    
    section Phase 4: æµ‹è¯•éªŒè¯
    å•å…ƒæµ‹è¯•æ‰©å±•          :p4-1, after p3-3, 1d
    é›†æˆæµ‹è¯•              :p4-2, after p4-1, 1d
    æ€§èƒ½åŸºå‡†æµ‹è¯•          :p4-3, after p4-2, 1d
    è´¨é‡éªŒè¯å’Œæ–‡æ¡£        :p4-4, after p4-3, 1d
```

### å…³é”®é‡Œç¨‹ç¢‘æ£€æŸ¥ç‚¹

#### Milestone 1 (Day 3): åŸºç¡€æ¶æ„å®Œæˆ
**éªŒæ”¶æ ‡å‡†**:
- [ ] SequenceAdapteré€šè¿‡æ‰€æœ‰å½¢çŠ¶æµ‹è¯•
- [ ] å…ƒæ•°æ®æå–åœ¨10ä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•é€šè¿‡  
- [ ] GM_01_RectifiedFlowé€‚é…3Då¼ é‡è¾“å…¥
- [ ] ç«¯åˆ°ç«¯å‰å‘ä¼ æ’­æ— é”™è¯¯

**é£é™©ç‚¹**: ç»´åº¦ä¸å…¼å®¹å¯èƒ½éœ€è¦é¢å¤–1-2å¤©è§£å†³

#### Milestone 2 (Day 7): æ¨¡å‹å¢å¼ºå®Œæˆ
**éªŒæ”¶æ ‡å‡†**:
- [ ] 4ç§ODEæ±‚è§£å™¨å®ç°å¹¶æµ‹è¯•
- [ ] è®­ç»ƒç¨³å®šè¿è¡Œ50ä¸ªepochæ— NaN
- [ ] ç”Ÿæˆæ ·æœ¬é€šè¿‡è§†è§‰æ£€æŸ¥
- [ ] å†…å­˜ä½¿ç”¨æ§åˆ¶åœ¨8GBä»¥å†…

**é£é™©ç‚¹**: æ•°å€¼ç¨³å®šæ€§å¯èƒ½éœ€è¦å¤šæ¬¡è°ƒè¯•

#### Milestone 3 (Day 10): ä»»åŠ¡é›†æˆå®Œæˆ
**éªŒæ”¶æ ‡å‡†**:
- [ ] ç«¯åˆ°ç«¯è®­ç»ƒpipelineè¿è¡ŒæˆåŠŸ
- [ ] æŸå¤±å‡½æ•°æ”¶æ•›æ­£å¸¸
- [ ] æ”¯æŒå¤šç§é…ç½®æ–‡ä»¶
- [ ] ä¸ç°æœ‰æ¡†æ¶æ— å†²çª

**é£é™©ç‚¹**: é›†æˆé—®é¢˜å¯èƒ½éœ€è¦é¢å¤–çš„å…¼å®¹æ€§å·¥ä½œ

#### Milestone 4 (Day 14): é¡¹ç›®å®Œæˆ
**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡(>95%)
- [ ] æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡
- [ ] æ–‡æ¡£å®Œæ•´å¯ç”¨
- [ ] ä»£ç reviewé€šè¿‡

### æ¯æ—¥å·¥ä½œè®¡åˆ’

#### Day 1: ç»´åº¦é€‚é…å™¨ (2025-08-30)
**ä¸Šåˆ (9:00-12:00)**:
- åˆ›å»ºé¡¹ç›®ç»“æ„å’Œæ–‡ä»¶
- å®ç°SequenceAdapteråŸºç¡€ç±»
- ç¼–å†™encode/decodeæ–¹æ³•

**ä¸‹åˆ (13:00-17:00)**:  
- æ·»åŠ å½¢çŠ¶éªŒè¯å’Œé”™è¯¯å¤„ç†
- ç¼–å†™å•å…ƒæµ‹è¯•(10ä¸ªæµ‹è¯•ç”¨ä¾‹)
- æµ‹è¯•ä¸åŒè¾“å…¥å½¢çŠ¶çš„å…¼å®¹æ€§

**æ™šä¸Š (19:00-21:00)**:
- ä»£ç reviewå’Œæ–‡æ¡£æ›´æ–°
- å‡†å¤‡ç¬¬äºŒå¤©å·¥ä½œå†…å®¹

**äº¤ä»˜ç‰©**: SequenceAdapter.py + æµ‹è¯•æ–‡ä»¶

#### Day 2: å…ƒæ•°æ®é›†æˆ (2025-08-31)
**å·¥ä½œå†…å®¹**:
- å®ç°MetadataExtractorç±»
- åˆ›å»ºåŸŸ/ç³»ç»Ÿæ˜ å°„è¡¨
- æµ‹è¯•10ä¸ªä¸åŒæ•°æ®é›†çš„å…¼å®¹æ€§
- é›†æˆåˆ°æ¡ä»¶ç¼–ç å™¨

**äº¤ä»˜ç‰©**: metadata_extractor.py + æ˜ å°„é…ç½®

#### Day 3: Flowæ¨¡å‹é€‚é… (2025-09-01)
**å·¥ä½œå†…å®¹**:
- ä¿®æ”¹GM_01_RectifiedFlowæ”¯æŒSequenceAdapter
- æ›´æ–°æ‰€æœ‰è¾“å…¥è¾“å‡ºæ¥å£
- éªŒè¯ç«¯åˆ°ç«¯å‰å‘ä¼ æ’­
- æ€§èƒ½åŸºå‡†æµ‹è¯•

**äº¤ä»˜ç‰©**: é€‚é…ç‰ˆGM_01_RectifiedFlow.py

---

## ğŸ‰ é¡¹ç›®æ€»ç»“å’Œå±•æœ›

### é¢„æœŸæˆæœ

#### æŠ€æœ¯æˆæœ
1. **å®Œæ•´çš„Flowç”Ÿæˆæ¨¡å‹**: æ”¯æŒæ¡ä»¶å’Œæ— æ¡ä»¶ç”Ÿæˆçš„é«˜è´¨é‡å®ç°
2. **PHM-Vibenché›†æˆ**: ä¸ç°æœ‰æ¡†æ¶æ— ç¼é›†æˆçš„Flowé¢„è®­ç»ƒç³»ç»Ÿ
3. **æ€§èƒ½ä¼˜åŒ–**: è®­ç»ƒé€Ÿåº¦>50 iter/sï¼Œå†…å­˜ä½¿ç”¨<8GBçš„é«˜æ•ˆå®ç°
4. **è´¨é‡ä¿è¯**: >95%æµ‹è¯•è¦†ç›–ç‡çš„ç¨³å®šå¯é ç³»ç»Ÿ

#### ä¸šåŠ¡ä»·å€¼
1. **æ•°æ®å¢å¼º**: ä¸ºç¨€ç¼ºæ•…éšœç±»åˆ«ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ ·æœ¬
2. **åŸŸé€‚åº”**: æå‡è·¨æ•°æ®é›†è¿ç§»æ€§èƒ½15%+
3. **Few-Shotå­¦ä¹ **: å°‘æ ·æœ¬åœºæ™¯ä¸‹ç²¾åº¦æå‡10%+
4. **å¼‚å¸¸æ£€æµ‹**: é€šè¿‡ä¼¼ç„¶ä¼°è®¡æä¾›æ–°çš„å¼‚å¸¸æ£€æµ‹æ–¹æ¡ˆ

### åç»­å‘å±•æ–¹å‘

#### çŸ­æœŸä¼˜åŒ– (1-2æœˆ)
1. **å¤šæ¨¡æ€æ‰©å±•**: æ”¯æŒæŒ¯åŠ¨+æ¸©åº¦+å£°éŸ³çš„è”åˆç”Ÿæˆ
2. **ç‰©ç†çº¦æŸ**: é›†æˆè½´æ‰¿åŠ¨åŠ›å­¦æ–¹ç¨‹çš„ç‰©ç†ä¿¡æ¯ç½‘ç»œ
3. **è‡ªé€‚åº”é‡‡æ ·**: åŸºäºç”Ÿæˆè´¨é‡çš„åŠ¨æ€æ­¥æ•°è°ƒæ•´
4. **å‹ç¼©ä¼˜åŒ–**: é‡åŒ–å’Œå‰ªæå‡å°‘æ¨¡å‹å¤§å°

#### ä¸­æœŸå‘å±• (3-6æœˆ)
1. **å®æ—¶ç”Ÿæˆ**: åœ¨çº¿æ•°æ®å¢å¼ºå’Œå¼‚å¸¸æ£€æµ‹
2. **è”é‚¦å­¦ä¹ **: åˆ†å¸ƒå¼Flowæ¨¡å‹è®­ç»ƒ
3. **å¯è§£é‡Šæ€§**: Flowè½¨è¿¹çš„ç‰©ç†æ„ä¹‰è§£æ
4. **å·¥ä¸šéƒ¨ç½²**: è¾¹ç¼˜è®¾å¤‡ä¸Šçš„è½»é‡åŒ–éƒ¨ç½²

#### é•¿æœŸæ„¿æ™¯ (6-12æœˆ)
1. **é€šç”¨å·¥ä¸šç”Ÿæˆæ¨¡å‹**: æ”¯æŒæ›´å¤šå·¥ä¸šè®¾å¤‡ç±»å‹
2. **æ•°å­—å­ªç”Ÿé›†æˆ**: ä¸ä»¿çœŸç³»ç»Ÿè”åˆå»ºæ¨¡
3. **é¢„æµ‹æ€§ç»´æŠ¤**: åŸºäºç”Ÿæˆæ¨¡å‹çš„æ•…éšœé¢„æµ‹
4. **æ ‡å‡†åŒ–æ¨å¹¿**: å½¢æˆå·¥ä¸šä¿¡å·ç”Ÿæˆçš„è¡Œä¸šæ ‡å‡†

### æˆåŠŸå…³é”®å› ç´ 

1. **ä¸¥æ ¼çš„åˆ†é˜¶æ®µéªŒè¯**: æ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ˜ç¡®çš„éªŒæ”¶æ ‡å‡†
2. **å…¨é¢çš„æµ‹è¯•è¦†ç›–**: å•å…ƒã€é›†æˆã€æ€§èƒ½ã€ç¨³å®šæ€§æµ‹è¯•
3. **æŒç»­çš„æ€§èƒ½ç›‘æ§**: è®­ç»ƒè¿‡ç¨‹ä¸­çš„å®æ—¶æŒ‡æ ‡è·Ÿè¸ª
4. **å……åˆ†çš„é£é™©é¢„æ¡ˆ**: é’ˆå¯¹æ¯ä¸ªé£é™©ç‚¹çš„å…·ä½“åº”å¯¹æªæ–½
5. **å®Œå–„çš„æ–‡æ¡£ç³»ç»Ÿ**: ä¾¿äºç»´æŠ¤å’Œæ‰©å±•çš„æŠ€æœ¯æ–‡æ¡£

---

**æœ¬å®æ–½è®¡åˆ’ä¸ºPHM-Vibench Flowé¢„è®­ç»ƒç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æŠ€æœ¯è·¯çº¿å›¾ï¼Œç¡®ä¿é¡¹ç›®æŒ‰æ—¶ã€æŒ‰è´¨å®Œæˆï¼Œä¸ºå·¥ä¸šä¿¡å·å¤„ç†é¢†åŸŸçš„ç”Ÿæˆå¼AIåº”ç”¨å¥ å®šåšå®åŸºç¡€ã€‚**

---

*è®¡åˆ’åˆ¶å®šï¼š2025å¹´8æœˆ30æ—¥*  
*ç‰ˆæœ¬ï¼šV4.0 è¯¦ç»†å®æ–½ç‰ˆ*  
*æœ‰æ•ˆæœŸï¼š2025å¹´8æœˆ30æ—¥ - 2025å¹´9æœˆ15æ—¥*