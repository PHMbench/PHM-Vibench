#!/usr/bin/env python3
"""
ç»Ÿè®¡åˆ†æè„šæœ¬
å¯¹å®éªŒç»“æœè¿›è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒå’Œæ•ˆåº”å¤§å°åˆ†æ
"""

import numpy as np
import pandas as pd
import scipy.stats as stats
from scipy.stats import ttest_ind, mannwhitneyu, wilcoxon, kruskal, f_oneway
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import argparse

class StatisticalAnalyzer:
    """ç»Ÿè®¡åˆ†æç±»"""
    
    def __init__(self, alpha=0.05):
        """
        å‚æ•°:
            alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05
        """
        self.alpha = alpha
        self.results = {}
    
    def compare_two_groups(self, group1_scores, group2_scores, 
                          group1_name="Method 1", group2_name="Method 2",
                          alternative='two-sided'):
        """æ¯”è¾ƒä¸¤ç»„æ–¹æ³•çš„ç»Ÿè®¡æ˜¾è‘—æ€§"""
        
        print(f"\nğŸ“Š æ¯”è¾ƒåˆ†æ: {group1_name} vs {group2_name}")
        print("=" * 50)
        
        # åŸºæœ¬ç»Ÿè®¡
        stats1 = self._compute_descriptive_stats(group1_scores, group1_name)
        stats2 = self._compute_descriptive_stats(group2_scores, group2_name)
        
        print(f"\nğŸ“ˆ æè¿°æ€§ç»Ÿè®¡:")
        print(f"{group1_name}: Î¼={stats1['mean']:.4f}, Ïƒ={stats1['std']:.4f}, n={stats1['n']}")
        print(f"{group2_name}: Î¼={stats2['mean']:.4f}, Ïƒ={stats2['std']:.4f}, n={stats2['n']}")
        
        # æ­£æ€æ€§æ£€éªŒ
        normality1 = self._test_normality(group1_scores, group1_name)
        normality2 = self._test_normality(group2_scores, group2_name)
        
        # æ–¹å·®é½æ€§æ£€éªŒ
        homoscedasticity = self._test_homoscedasticity(group1_scores, group2_scores)
        
        # é€‰æ‹©åˆé€‚çš„ç»Ÿè®¡æ£€éªŒ
        if normality1['normal'] and normality2['normal']:
            if homoscedasticity['equal_var']:
                # ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ (ç­‰æ–¹å·®)
                test_result = self._independent_ttest(
                    group1_scores, group2_scores, equal_var=True, alternative=alternative
                )
                test_name = "Independent t-test (equal variance)"
            else:
                # Welch's tæ£€éªŒ (ä¸ç­‰æ–¹å·®)
                test_result = self._independent_ttest(
                    group1_scores, group2_scores, equal_var=False, alternative=alternative
                )
                test_name = "Welch's t-test (unequal variance)"
        else:
            # éå‚æ•°æ£€éªŒ: Mann-Whitney Uæ£€éªŒ
            test_result = self._mann_whitney_test(
                group1_scores, group2_scores, alternative=alternative
            )
            test_name = "Mann-Whitney U test"
        
        # æ•ˆåº”å¤§å°
        effect_size = self._compute_effect_size(group1_scores, group2_scores)
        
        # ç½®ä¿¡åŒºé—´
        confidence_interval = self._compute_confidence_interval(group1_scores, group2_scores)
        
        # æ±‡æ€»ç»“æœ
        comparison_result = {
            'group1_name': group1_name,
            'group2_name': group2_name,
            'group1_stats': stats1,
            'group2_stats': stats2,
            'normality_test': {
                'group1': normality1,
                'group2': normality2
            },
            'homoscedasticity_test': homoscedasticity,
            'statistical_test': {
                'name': test_name,
                'statistic': test_result['statistic'],
                'p_value': test_result['p_value'],
                'significant': test_result['p_value'] < self.alpha,
                'interpretation': self._interpret_p_value(test_result['p_value'])
            },
            'effect_size': effect_size,
            'confidence_interval': confidence_interval
        }
        
        # æ‰“å°ç»“æœ
        self._print_comparison_results(comparison_result)
        
        return comparison_result
    
    def compare_multiple_groups(self, groups_dict, group_names=None):
        """æ¯”è¾ƒå¤šç»„æ–¹æ³•çš„ç»Ÿè®¡æ˜¾è‘—æ€§"""
        
        if group_names is None:
            group_names = list(groups_dict.keys())
        
        print(f"\nğŸ“Š å¤šç»„æ¯”è¾ƒåˆ†æ")
        print(f"ç»„æ•°: {len(groups_dict)}")
        print("=" * 50)
        
        # å‡†å¤‡æ•°æ®
        groups_data = []
        groups_labels = []
        
        for name in group_names:
            if name in groups_dict:
                scores = groups_dict[name]
                groups_data.append(scores)
                groups_labels.append(name)
                
                # æ‰“å°æè¿°æ€§ç»Ÿè®¡
                stats = self._compute_descriptive_stats(scores, name)
                print(f"{name}: Î¼={stats['mean']:.4f}, Ïƒ={stats['std']:.4f}, n={stats['n']}")
        
        # æ­£æ€æ€§æ£€éªŒ
        print(f"\nğŸ” æ­£æ€æ€§æ£€éªŒ:")
        normality_results = []
        for i, (scores, name) in enumerate(zip(groups_data, groups_labels)):
            normality = self._test_normality(scores, name)
            normality_results.append(normality['normal'])
            print(f"{name}: {'æ­£æ€åˆ†å¸ƒ' if normality['normal'] else 'éæ­£æ€åˆ†å¸ƒ'} (p={normality['p_value']:.4f})")
        
        # æ–¹å·®é½æ€§æ£€éªŒ (Leveneæ£€éªŒ)
        print(f"\nğŸ” æ–¹å·®é½æ€§æ£€éªŒ:")
        levene_stat, levene_p = stats.levene(*groups_data)
        equal_variances = levene_p >= self.alpha
        print(f"Leveneæ£€éªŒ: F={levene_stat:.4f}, p={levene_p:.4f}")
        print(f"æ–¹å·®{'é½æ€§' if equal_variances else 'ä¸é½æ€§'}")
        
        # é€‰æ‹©åˆé€‚çš„ç»Ÿè®¡æ£€éªŒ
        all_normal = all(normality_results)
        
        if all_normal and equal_variances:
            # å•å› ç´ æ–¹å·®åˆ†æ (ANOVA)
            f_stat, p_value = f_oneway(*groups_data)
            test_name = "One-way ANOVA"
            post_hoc = self._tukey_hsd_test(groups_data, groups_labels) if p_value < self.alpha else None
        else:
            # éå‚æ•°æ£€éªŒ: Kruskal-Wallisæ£€éªŒ
            h_stat, p_value = kruskal(*groups_data)
            f_stat = h_stat  # ä¸ºäº†ä¿æŒä¸€è‡´æ€§
            test_name = "Kruskal-Wallis H test"
            post_hoc = self._dunn_test(groups_data, groups_labels) if p_value < self.alpha else None
        
        # æ•ˆåº”å¤§å° (eta squared)
        eta_squared = self._compute_eta_squared(groups_data)
        
        # æ±‡æ€»ç»“æœ
        multiple_comparison_result = {
            'groups': groups_labels,
            'group_stats': {
                name: self._compute_descriptive_stats(scores, name) 
                for name, scores in zip(groups_labels, groups_data)
            },
            'normality_test': {
                name: self._test_normality(scores, name)
                for name, scores in zip(groups_labels, groups_data)
            },
            'levene_test': {
                'statistic': levene_stat,
                'p_value': levene_p,
                'equal_variances': equal_variances
            },
            'statistical_test': {
                'name': test_name,
                'statistic': f_stat,
                'p_value': p_value,
                'significant': p_value < self.alpha,
                'interpretation': self._interpret_p_value(p_value)
            },
            'effect_size': {
                'eta_squared': eta_squared,
                'interpretation': self._interpret_eta_squared(eta_squared)
            },
            'post_hoc': post_hoc
        }
        
        # æ‰“å°ç»“æœ
        self._print_multiple_comparison_results(multiple_comparison_result)
        
        return multiple_comparison_result
    
    def paired_comparison(self, before_scores, after_scores, 
                         method1_name="Before", method2_name="After"):
        """é…å¯¹æ¯”è¾ƒ (å¦‚åŒä¸€æ•°æ®é›†ä¸Šä¸åŒæ–¹æ³•çš„æ¯”è¾ƒ)"""
        
        if len(before_scores) != len(after_scores):
            raise ValueError("é…å¯¹æ•°æ®é•¿åº¦å¿…é¡»ç›¸ç­‰")
        
        print(f"\nğŸ“Š é…å¯¹æ¯”è¾ƒåˆ†æ: {method1_name} vs {method2_name}")
        print("=" * 50)
        
        # è®¡ç®—å·®å€¼
        differences = np.array(after_scores) - np.array(before_scores)
        
        # æè¿°æ€§ç»Ÿè®¡
        print(f"\nğŸ“ˆ é…å¯¹å·®å€¼ç»Ÿè®¡:")
        print(f"å¹³å‡å·®å€¼: {np.mean(differences):.4f}")
        print(f"å·®å€¼æ ‡å‡†å·®: {np.std(differences, ddof=1):.4f}")
        print(f"é…å¯¹æ•°: {len(differences)}")
        
        # å·®å€¼çš„æ­£æ€æ€§æ£€éªŒ
        diff_normality = self._test_normality(differences, "å·®å€¼")
        
        # é€‰æ‹©åˆé€‚çš„æ£€éªŒ
        if diff_normality['normal']:
            # é…å¯¹tæ£€éªŒ
            t_stat, p_value = stats.ttest_rel(after_scores, before_scores)
            test_name = "Paired t-test"
        else:
            # Wilcoxonç¬¦å·ç§©æ£€éªŒ
            w_stat, p_value = wilcoxon(after_scores, before_scores)
            t_stat = w_stat  # ä¸ºäº†ä¿æŒä¸€è‡´æ€§
            test_name = "Wilcoxon signed-rank test"
        
        # æ•ˆåº”å¤§å° (Cohen's d for paired data)
        effect_size = np.mean(differences) / np.std(differences, ddof=1)
        
        # é…å¯¹ç»“æœ
        paired_result = {
            'method1_name': method1_name,
            'method2_name': method2_name,
            'n_pairs': len(differences),
            'mean_difference': np.mean(differences),
            'std_difference': np.std(differences, ddof=1),
            'difference_normality': diff_normality,
            'statistical_test': {
                'name': test_name,
                'statistic': t_stat,
                'p_value': p_value,
                'significant': p_value < self.alpha,
                'interpretation': self._interpret_p_value(p_value)
            },
            'effect_size': {
                'cohens_d': effect_size,
                'interpretation': self._interpret_cohens_d(effect_size)
            }
        }
        
        # æ‰“å°ç»“æœ
        self._print_paired_comparison_results(paired_result)
        
        return paired_result
    
    def _compute_descriptive_stats(self, scores, name):
        """è®¡ç®—æè¿°æ€§ç»Ÿè®¡"""
        return {
            'name': name,
            'n': len(scores),
            'mean': np.mean(scores),
            'std': np.std(scores, ddof=1),
            'min': np.min(scores),
            'max': np.max(scores),
            'median': np.median(scores),
            'q25': np.percentile(scores, 25),
            'q75': np.percentile(scores, 75)
        }
    
    def _test_normality(self, scores, name):
        """æµ‹è¯•æ­£æ€æ€§ (Shapiro-Wilkæ£€éªŒ)"""
        if len(scores) < 3:
            return {'name': name, 'normal': True, 'p_value': 1.0, 'test': 'insufficient_data'}
        
        if len(scores) <= 5000:
            # Shapiro-Wilkæ£€éªŒ (é€‚ç”¨äºå°æ ·æœ¬)
            stat, p_value = stats.shapiro(scores)
            test_name = 'Shapiro-Wilk'
        else:
            # D'Agostinoå’ŒPearsonæ£€éªŒ (é€‚ç”¨äºå¤§æ ·æœ¬)
            stat, p_value = stats.normaltest(scores)
            test_name = "D'Agostino-Pearson"
        
        return {
            'name': name,
            'test': test_name,
            'statistic': stat,
            'p_value': p_value,
            'normal': p_value >= self.alpha
        }
    
    def _test_homoscedasticity(self, group1_scores, group2_scores):
        """æ–¹å·®é½æ€§æ£€éªŒ (Leveneæ£€éªŒ)"""
        stat, p_value = stats.levene(group1_scores, group2_scores)
        
        return {
            'test': 'Levene',
            'statistic': stat,
            'p_value': p_value,
            'equal_var': p_value >= self.alpha
        }
    
    def _independent_ttest(self, group1_scores, group2_scores, equal_var=True, alternative='two-sided'):
        """ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ"""
        t_stat, p_value = ttest_ind(group1_scores, group2_scores, equal_var=equal_var, alternative=alternative)
        
        return {
            'statistic': t_stat,
            'p_value': p_value
        }
    
    def _mann_whitney_test(self, group1_scores, group2_scores, alternative='two-sided'):
        """Mann-Whitney Uæ£€éªŒ"""
        u_stat, p_value = mannwhitneyu(group1_scores, group2_scores, alternative=alternative)
        
        return {
            'statistic': u_stat,
            'p_value': p_value
        }
    
    def _compute_effect_size(self, group1_scores, group2_scores):
        """è®¡ç®—æ•ˆåº”å¤§å° (Cohen's d)"""
        n1, n2 = len(group1_scores), len(group2_scores)
        mean1, mean2 = np.mean(group1_scores), np.mean(group2_scores)
        var1, var2 = np.var(group1_scores, ddof=1), np.var(group2_scores, ddof=1)
        
        # åˆå¹¶æ ‡å‡†å·®
        pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))
        
        # Cohen's d
        cohens_d = (mean1 - mean2) / pooled_std
        
        return {
            'cohens_d': cohens_d,
            'interpretation': self._interpret_cohens_d(cohens_d),
            'pooled_std': pooled_std
        }
    
    def _compute_confidence_interval(self, group1_scores, group2_scores, confidence=0.95):
        """è®¡ç®—å‡å€¼å·®çš„ç½®ä¿¡åŒºé—´"""
        n1, n2 = len(group1_scores), len(group2_scores)
        mean1, mean2 = np.mean(group1_scores), np.mean(group2_scores)
        var1, var2 = np.var(group1_scores, ddof=1), np.var(group2_scores, ddof=1)
        
        # å‡å€¼å·®
        mean_diff = mean1 - mean2
        
        # æ ‡å‡†è¯¯
        se = np.sqrt(var1/n1 + var2/n2)
        
        # è‡ªç”±åº¦ (Welch-Satterthwaiteæ–¹ç¨‹)
        df = (var1/n1 + var2/n2)**2 / ((var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1))
        
        # ä¸´ç•Œå€¼
        alpha = 1 - confidence
        t_critical = stats.t.ppf(1 - alpha/2, df)
        
        # ç½®ä¿¡åŒºé—´
        margin_of_error = t_critical * se
        ci_lower = mean_diff - margin_of_error
        ci_upper = mean_diff + margin_of_error
        
        return {
            'mean_difference': mean_diff,
            'confidence_level': confidence,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'margin_of_error': margin_of_error
        }
    
    def _compute_eta_squared(self, groups_data):
        """è®¡ç®—eta squared (æ•ˆåº”å¤§å°)"""
        # è®¡ç®—ç»„é—´å¹³æ–¹å’Œå’Œæ€»å¹³æ–¹å’Œ
        all_data = np.concatenate(groups_data)
        grand_mean = np.mean(all_data)
        
        ss_between = sum([len(group) * (np.mean(group) - grand_mean)**2 for group in groups_data])
        ss_total = sum([(x - grand_mean)**2 for x in all_data])
        
        eta_squared = ss_between / ss_total if ss_total > 0 else 0
        
        return eta_squared
    
    def _tukey_hsd_test(self, groups_data, groups_labels):
        """Tukey HSDäº‹åæ£€éªŒ"""
        try:
            from statsmodels.stats.multicomp import pairwise_tukeyhsd
            
            # å‡†å¤‡æ•°æ®
            all_data = []
            all_labels = []
            
            for data, label in zip(groups_data, groups_labels):
                all_data.extend(data)
                all_labels.extend([label] * len(data))
            
            # æ‰§è¡ŒTukey HSD
            tukey_result = pairwise_tukeyhsd(all_data, all_labels, alpha=self.alpha)
            
            return {
                'test': 'Tukey HSD',
                'summary': str(tukey_result)
            }
            
        except ImportError:
            print("âš ï¸  statsmodelsæœªå®‰è£…ï¼Œè·³è¿‡Tukey HSDæ£€éªŒ")
            return None
    
    def _dunn_test(self, groups_data, groups_labels):
        """Dunnäº‹åæ£€éªŒ (éå‚æ•°)"""
        # ç®€åŒ–ç‰ˆçš„Dunnæ£€éªŒ
        comparisons = []
        
        for i in range(len(groups_data)):
            for j in range(i+1, len(groups_data)):
                u_stat, p_value = mannwhitneyu(groups_data[i], groups_data[j])
                comparisons.append({
                    'group1': groups_labels[i],
                    'group2': groups_labels[j],
                    'u_statistic': u_stat,
                    'p_value': p_value,
                    'significant': p_value < (self.alpha / len(comparisons))  # Bonferroniæ ¡æ­£
                })
        
        return {
            'test': 'Dunn (simplified)',
            'comparisons': comparisons
        }
    
    def _interpret_p_value(self, p_value):
        """è§£é‡Špå€¼"""
        if p_value < 0.001:
            return "ææ˜¾è‘— (p < 0.001)"
        elif p_value < 0.01:
            return "é«˜åº¦æ˜¾è‘— (p < 0.01)"
        elif p_value < 0.05:
            return "æ˜¾è‘— (p < 0.05)"
        elif p_value < 0.1:
            return "è¾¹ç¼˜æ˜¾è‘— (p < 0.1)"
        else:
            return "ä¸æ˜¾è‘— (p â‰¥ 0.1)"
    
    def _interpret_cohens_d(self, cohens_d):
        """è§£é‡ŠCohen's dæ•ˆåº”å¤§å°"""
        abs_d = abs(cohens_d)
        
        if abs_d < 0.2:
            return "å¯å¿½ç•¥æ•ˆåº”"
        elif abs_d < 0.5:
            return "å°æ•ˆåº”"
        elif abs_d < 0.8:
            return "ä¸­ç­‰æ•ˆåº”"
        else:
            return "å¤§æ•ˆåº”"
    
    def _interpret_eta_squared(self, eta_squared):
        """è§£é‡Šeta squaredæ•ˆåº”å¤§å°"""
        if eta_squared < 0.01:
            return "å¯å¿½ç•¥æ•ˆåº”"
        elif eta_squared < 0.06:
            return "å°æ•ˆåº”"
        elif eta_squared < 0.14:
            return "ä¸­ç­‰æ•ˆåº”"
        else:
            return "å¤§æ•ˆåº”"
    
    def _print_comparison_results(self, result):
        """æ‰“å°ä¸¤ç»„æ¯”è¾ƒç»“æœ"""
        print(f"\nğŸ” ç»Ÿè®¡æ£€éªŒç»“æœ:")
        print(f"æ£€éªŒæ–¹æ³•: {result['statistical_test']['name']}")
        print(f"æ£€éªŒç»Ÿè®¡é‡: {result['statistical_test']['statistic']:.4f}")
        print(f"på€¼: {result['statistical_test']['p_value']:.6f}")
        print(f"æ˜¾è‘—æ€§: {result['statistical_test']['interpretation']}")
        
        print(f"\nğŸ“ æ•ˆåº”å¤§å°:")
        print(f"Cohen's d: {result['effect_size']['cohens_d']:.4f}")
        print(f"æ•ˆåº”å¤§å°: {result['effect_size']['interpretation']}")
        
        print(f"\nğŸ“Š ç½®ä¿¡åŒºé—´ ({result['confidence_interval']['confidence_level']*100:.0f}%):")
        print(f"å‡å€¼å·®: {result['confidence_interval']['mean_difference']:.4f}")
        print(f"ç½®ä¿¡åŒºé—´: [{result['confidence_interval']['ci_lower']:.4f}, {result['confidence_interval']['ci_upper']:.4f}]")
        
        print(f"\nğŸ“‹ ç»“è®º:")
        if result['statistical_test']['significant']:
            print(f"âœ… {result['group1_name']}å’Œ{result['group2_name']}ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚")
            if result['confidence_interval']['ci_lower'] > 0:
                print(f"   {result['group1_name']} æ˜¾è‘—ä¼˜äº {result['group2_name']}")
            elif result['confidence_interval']['ci_upper'] < 0:
                print(f"   {result['group2_name']} æ˜¾è‘—ä¼˜äº {result['group1_name']}")
        else:
            print(f"âŒ {result['group1_name']}å’Œ{result['group2_name']}ä¹‹é—´æ— æ˜¾è‘—å·®å¼‚")
    
    def _print_multiple_comparison_results(self, result):
        """æ‰“å°å¤šç»„æ¯”è¾ƒç»“æœ"""
        print(f"\nğŸ” å¤šç»„ç»Ÿè®¡æ£€éªŒç»“æœ:")
        print(f"æ£€éªŒæ–¹æ³•: {result['statistical_test']['name']}")
        print(f"æ£€éªŒç»Ÿè®¡é‡: {result['statistical_test']['statistic']:.4f}")
        print(f"på€¼: {result['statistical_test']['p_value']:.6f}")
        print(f"æ˜¾è‘—æ€§: {result['statistical_test']['interpretation']}")
        
        print(f"\nğŸ“ æ•ˆåº”å¤§å°:")
        print(f"Eta squared: {result['effect_size']['eta_squared']:.4f}")
        print(f"æ•ˆåº”å¤§å°: {result['effect_size']['interpretation']}")
        
        if result['post_hoc']:
            print(f"\nğŸ” äº‹åæ£€éªŒ:")
            if 'comparisons' in result['post_hoc']:
                for comp in result['post_hoc']['comparisons']:
                    sig_mark = "âœ…" if comp['significant'] else "âŒ"
                    print(f"  {comp['group1']} vs {comp['group2']}: p={comp['p_value']:.4f} {sig_mark}")
    
    def _print_paired_comparison_results(self, result):
        """æ‰“å°é…å¯¹æ¯”è¾ƒç»“æœ"""
        print(f"\nğŸ” é…å¯¹ç»Ÿè®¡æ£€éªŒç»“æœ:")
        print(f"æ£€éªŒæ–¹æ³•: {result['statistical_test']['name']}")
        print(f"æ£€éªŒç»Ÿè®¡é‡: {result['statistical_test']['statistic']:.4f}")
        print(f"på€¼: {result['statistical_test']['p_value']:.6f}")
        print(f"æ˜¾è‘—æ€§: {result['statistical_test']['interpretation']}")
        
        print(f"\nğŸ“ æ•ˆåº”å¤§å°:")
        print(f"Cohen's d: {result['effect_size']['cohens_d']:.4f}")
        print(f"æ•ˆåº”å¤§å°: {result['effect_size']['interpretation']}")


def load_experiment_results(results_csv):
    """ä»CSVæ–‡ä»¶åŠ è½½å®éªŒç»“æœ"""
    
    if not Path(results_csv).exists():
        raise FileNotFoundError(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_csv}")
    
    df = pd.read_csv(results_csv)
    
    # æŒ‰å®éªŒç±»å‹åˆ†ç»„
    results_by_type = {}
    
    if 'experiment_type' in df.columns and 'final_accuracy' in df.columns:
        for exp_type in df['experiment_type'].unique():
            if pd.notna(exp_type):
                type_df = df[df['experiment_type'] == exp_type]
                accuracies = type_df['final_accuracy'].dropna().values
                if len(accuracies) > 0:
                    results_by_type[exp_type] = accuracies
    
    return results_by_type


def main():
    parser = argparse.ArgumentParser(description="Flowå®éªŒç»“æœç»Ÿè®¡åˆ†æ")
    parser.add_argument('--results_file', type=str, required=True,
                       help='å®éªŒç»“æœCSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--alpha', type=float, default=0.05,
                       help='æ˜¾è‘—æ€§æ°´å¹³')
    parser.add_argument('--output_dir', type=str, default='statistical_analysis',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç»Ÿè®¡åˆ†æå™¨
    analyzer = StatisticalAnalyzer(alpha=args.alpha)
    
    # åŠ è½½ç»“æœ
    try:
        results = load_experiment_results(args.results_file)
        print(f"ğŸ“ å·²åŠ è½½å®éªŒç»“æœ: {args.results_file}")
        print(f"å‘ç°å®éªŒç±»å‹: {list(results.keys())}")
        
    except Exception as e:
        print(f"âŒ åŠ è½½ç»“æœå¤±è´¥: {e}")
        return
    
    if len(results) < 2:
        print("âš ï¸  éœ€è¦è‡³å°‘2ç§å®éªŒç±»å‹è¿›è¡Œæ¯”è¾ƒåˆ†æ")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # å¤šç»„æ¯”è¾ƒ
    if len(results) > 2:
        print(f"\nğŸ¯ è¿›è¡Œå¤šç»„æ¯”è¾ƒåˆ†æ...")
        multiple_result = analyzer.compare_multiple_groups(results)
        
        # ä¿å­˜å¤šç»„æ¯”è¾ƒç»“æœ
        with open(output_dir / 'multiple_comparison.json', 'w') as f:
            json.dump(multiple_result, f, indent=2, default=str)
    
    # ä¸¤ä¸¤æ¯”è¾ƒ
    methods = list(results.keys())
    pairwise_results = {}
    
    for i in range(len(methods)):
        for j in range(i+1, len(methods)):
            method1, method2 = methods[i], methods[j]
            
            print(f"\nğŸ” ä¸¤ä¸¤æ¯”è¾ƒ: {method1} vs {method2}")
            comparison_result = analyzer.compare_two_groups(
                results[method1], results[method2], method1, method2
            )
            
            pairwise_results[f"{method1}_vs_{method2}"] = comparison_result
    
    # ä¿å­˜ä¸¤ä¸¤æ¯”è¾ƒç»“æœ
    with open(output_dir / 'pairwise_comparisons.json', 'w') as f:
        json.dump(pairwise_results, f, indent=2, default=str)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_analysis_report(results, multiple_result if len(results) > 2 else None, 
                           pairwise_results, output_dir)
    
    print(f"\nâœ… ç»Ÿè®¡åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")


def generate_analysis_report(results, multiple_result, pairwise_results, output_dir):
    """ç”Ÿæˆç»Ÿè®¡åˆ†ææŠ¥å‘Š"""
    
    report_file = output_dir / 'statistical_analysis_report.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# Flowå®éªŒç»Ÿè®¡åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n\n")
        
        # æ•°æ®æ‘˜è¦
        f.write("## æ•°æ®æ‘˜è¦\n\n")
        f.write("| å®éªŒç±»å‹ | æ ·æœ¬æ•° | å‡å€¼ | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ |\n")
        f.write("|----------|--------|------|--------|--------|--------|\n")
        
        for method_name, scores in results.items():
            f.write(f"| {method_name} | {len(scores)} | {np.mean(scores):.4f} | {np.std(scores, ddof=1):.4f} | {np.min(scores):.4f} | {np.max(scores):.4f} |\n")
        
        # ç»Ÿè®¡æ£€éªŒç»“æœ
        if multiple_result:
            f.write(f"\n## å¤šç»„æ¯”è¾ƒç»“æœ\n\n")
            f.write(f"**æ£€éªŒæ–¹æ³•:** {multiple_result['statistical_test']['name']}\n")
            f.write(f"**ç»Ÿè®¡é‡:** {multiple_result['statistical_test']['statistic']:.4f}\n")
            f.write(f"**på€¼:** {multiple_result['statistical_test']['p_value']:.6f}\n")
            f.write(f"**æ˜¾è‘—æ€§:** {multiple_result['statistical_test']['interpretation']}\n")
            f.write(f"**æ•ˆåº”å¤§å° (Î·Â²):** {multiple_result['effect_size']['eta_squared']:.4f} ({multiple_result['effect_size']['interpretation']})\n\n")
        
        # ä¸¤ä¸¤æ¯”è¾ƒ
        f.write(f"## ä¸¤ä¸¤æ¯”è¾ƒç»“æœ\n\n")
        for comparison_name, result in pairwise_results.items():
            f.write(f"### {result['group1_name']} vs {result['group2_name']}\n\n")
            f.write(f"- **æ£€éªŒæ–¹æ³•:** {result['statistical_test']['name']}\n")
            f.write(f"- **på€¼:** {result['statistical_test']['p_value']:.6f}\n")
            f.write(f"- **æ˜¾è‘—æ€§:** {result['statistical_test']['interpretation']}\n")
            f.write(f"- **æ•ˆåº”å¤§å° (Cohen's d):** {result['effect_size']['cohens_d']:.4f} ({result['effect_size']['interpretation']})\n")
            f.write(f"- **95%ç½®ä¿¡åŒºé—´:** [{result['confidence_interval']['ci_lower']:.4f}, {result['confidence_interval']['ci_upper']:.4f}]\n\n")
        
        f.write("## ç»“è®ºå»ºè®®\n\n")
        f.write("æ ¹æ®ç»Ÿè®¡åˆ†æç»“æœï¼Œå»ºè®®åœ¨è®ºæ–‡ä¸­æŠ¥å‘Šä»¥ä¸‹å†…å®¹ï¼š\n")
        f.write("1. æè¿°æ€§ç»Ÿè®¡ (å‡å€¼ã€æ ‡å‡†å·®ã€æ ·æœ¬æ•°)\n")
        f.write("2. ç»Ÿè®¡æ£€éªŒç»“æœ (æ£€éªŒæ–¹æ³•ã€på€¼ã€æ˜¾è‘—æ€§)\n")
        f.write("3. æ•ˆåº”å¤§å° (Cohen's d æˆ– Î·Â²)\n")
        f.write("4. ç½®ä¿¡åŒºé—´ (ç”¨äºä¼°è®¡å®é™…æ•ˆåº”å¤§å°èŒƒå›´)\n")
        f.write("5. å®é™…æ˜¾è‘—æ€§è§£é‡Š (ä¸ä»…ä»…ä¾èµ–på€¼)\n")
    
    print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")


if __name__ == "__main__":
    main()