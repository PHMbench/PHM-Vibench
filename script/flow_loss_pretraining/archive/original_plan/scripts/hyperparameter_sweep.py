#!/usr/bin/env python3
"""
è¶…å‚æ•°æœç´¢è„šæœ¬
æ”¯æŒç½‘æ ¼æœç´¢ã€éšæœºæœç´¢å’Œè´å¶æ–¯ä¼˜åŒ–
"""

import itertools
import subprocess
import yaml
import json
import numpy as np
import pandas as pd
from pathlib import Path
import argparse
from datetime import datetime
import random
import time

class HyperparameterSweep:
    """è¶…å‚æ•°æœç´¢ç±»"""
    
    def __init__(self, config_template="configs/demo/Pretraining/Flow/flow_baseline_experiment.yaml"):
        self.config_template = config_template
        self.results = []
        
    def grid_search(self, param_grid, max_experiments=None, output_dir="hyperparameter_sweep"):
        """ç½‘æ ¼æœç´¢"""
        
        print(f"ğŸ” å¼€å§‹ç½‘æ ¼æœç´¢è¶…å‚æ•°ä¼˜åŒ–")
        print(f"æœç´¢ç©ºé—´: {param_grid}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        full_output_dir = f"{output_dir}_{timestamp}"
        Path(full_output_dir).mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        
        all_combinations = list(itertools.product(*param_values))
        
        if max_experiments and len(all_combinations) > max_experiments:
            print(f"âš ï¸  é™åˆ¶å®éªŒæ•°é‡ä» {len(all_combinations)} åˆ° {max_experiments}")
            all_combinations = random.sample(all_combinations, max_experiments)
        
        print(f"æ€»å®éªŒæ•°: {len(all_combinations)}")
        
        # è¿è¡Œå®éªŒ
        for i, combination in enumerate(all_combinations, 1):
            params = dict(zip(param_names, combination))
            
            print(f"\nğŸ”¬ å®éªŒ {i}/{len(all_combinations)}: {params}")
            
            try:
                result = self._run_single_experiment(params, f"GridSearch_{i}")
                result['experiment_id'] = i
                result['search_type'] = 'grid'
                result['params'] = params
                
                self.results.append(result)
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                self._save_intermediate_results(full_output_dir)
                
            except Exception as e:
                print(f"âŒ å®éªŒ {i} å¤±è´¥: {e}")
                error_result = {
                    'experiment_id': i,
                    'search_type': 'grid',
                    'params': params,
                    'status': 'failed',
                    'error': str(e)
                }
                self.results.append(error_result)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_final_results(full_output_dir)
        return self.results
    
    def random_search(self, param_distributions, n_experiments, output_dir="random_search"):
        """éšæœºæœç´¢"""
        
        print(f"ğŸ² å¼€å§‹éšæœºæœç´¢è¶…å‚æ•°ä¼˜åŒ–")
        print(f"å‚æ•°åˆ†å¸ƒ: {param_distributions}")
        print(f"å®éªŒæ¬¡æ•°: {n_experiments}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        full_output_dir = f"{output_dir}_{timestamp}"
        Path(full_output_dir).mkdir(parents=True, exist_ok=True)
        
        # è¿è¡Œéšæœºå®éªŒ
        for i in range(1, n_experiments + 1):
            # éšæœºé‡‡æ ·å‚æ•°
            params = {}
            for param_name, distribution in param_distributions.items():
                if isinstance(distribution, list):
                    # ç¦»æ•£åˆ†å¸ƒ
                    params[param_name] = random.choice(distribution)
                elif isinstance(distribution, dict):
                    if distribution['type'] == 'uniform':
                        # è¿ç»­å‡åŒ€åˆ†å¸ƒ
                        params[param_name] = np.random.uniform(
                            distribution['low'], distribution['high']
                        )
                    elif distribution['type'] == 'loguniform':
                        # å¯¹æ•°å‡åŒ€åˆ†å¸ƒ
                        params[param_name] = np.exp(np.random.uniform(
                            np.log(distribution['low']), np.log(distribution['high'])
                        ))
                    elif distribution['type'] == 'choice':
                        # å¸¦æƒé‡çš„é€‰æ‹©
                        params[param_name] = np.random.choice(
                            distribution['choices'], p=distribution.get('weights')
                        )
            
            print(f"\nğŸ”¬ éšæœºå®éªŒ {i}/{n_experiments}: {params}")
            
            try:
                result = self._run_single_experiment(params, f"RandomSearch_{i}")
                result['experiment_id'] = i
                result['search_type'] = 'random'
                result['params'] = params
                
                self.results.append(result)
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                self._save_intermediate_results(full_output_dir)
                
            except Exception as e:
                print(f"âŒ éšæœºå®éªŒ {i} å¤±è´¥: {e}")
                error_result = {
                    'experiment_id': i,
                    'search_type': 'random',
                    'params': params,
                    'status': 'failed',
                    'error': str(e)
                }
                self.results.append(error_result)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_final_results(full_output_dir)
        return self.results
    
    def bayesian_optimization(self, param_bounds, n_experiments, output_dir="bayesian_opt"):
        """è´å¶æ–¯ä¼˜åŒ– (éœ€è¦å®‰è£…scikit-optimize)"""
        
        try:
            from skopt import gp_minimize
            from skopt.space import Real, Integer, Categorical
            from skopt.utils import use_named_args
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£… scikit-optimize: pip install scikit-optimize")
        
        print(f"ğŸ¤– å¼€å§‹è´å¶æ–¯ä¼˜åŒ–")
        print(f"å‚æ•°è¾¹ç•Œ: {param_bounds}")
        print(f"å®éªŒæ¬¡æ•°: {n_experiments}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        full_output_dir = f"{output_dir}_{timestamp}"
        Path(full_output_dir).mkdir(parents=True, exist_ok=True)
        
        # å®šä¹‰æœç´¢ç©ºé—´
        dimensions = []
        param_names = []
        
        for param_name, bounds in param_bounds.items():
            param_names.append(param_name)
            
            if bounds['type'] == 'real':
                dimensions.append(Real(bounds['low'], bounds['high'], name=param_name))
            elif bounds['type'] == 'integer':
                dimensions.append(Integer(bounds['low'], bounds['high'], name=param_name))
            elif bounds['type'] == 'categorical':
                dimensions.append(Categorical(bounds['choices'], name=param_name))
        
        # å®šä¹‰ç›®æ ‡å‡½æ•°
        @use_named_args(dimensions)
        def objective(**params):
            """è´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
            
            experiment_id = len(self.results) + 1
            print(f"\nğŸ”¬ è´å¶æ–¯å®éªŒ {experiment_id}: {params}")
            
            try:
                result = self._run_single_experiment(params, f"BayesOpt_{experiment_id}")
                result['experiment_id'] = experiment_id
                result['search_type'] = 'bayesian'
                result['params'] = params
                
                self.results.append(result)
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                self._save_intermediate_results(full_output_dir)
                
                # è¿”å›è´Ÿçš„å‡†ç¡®ç‡ (å› ä¸ºgp_minimizeæ˜¯æœ€å°åŒ–)
                accuracy = result.get('accuracy', 0)
                return -accuracy
                
            except Exception as e:
                print(f"âŒ è´å¶æ–¯å®éªŒ {experiment_id} å¤±è´¥: {e}")
                error_result = {
                    'experiment_id': experiment_id,
                    'search_type': 'bayesian',
                    'params': params,
                    'status': 'failed',
                    'error': str(e)
                }
                self.results.append(error_result)
                return 0  # è¿”å›æœ€åæ€§èƒ½
        
        # æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–
        result = gp_minimize(
            func=objective,
            dimensions=dimensions,
            n_calls=n_experiments,
            random_state=42,
            acq_func='EI'  # Expected Improvement
        )
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_final_results(full_output_dir)
        
        return {
            'best_params': dict(zip(param_names, result.x)),
            'best_score': -result.fun,
            'all_results': self.results
        }
    
    def _run_single_experiment(self, params, experiment_name):
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        
        # æ„å»ºé…ç½®è¦†ç›–å­—ç¬¦ä¸²
        override_list = []
        for param_name, param_value in params.items():
            override_list.append(f"{param_name}={param_value}")
        override_str = ",".join(override_list)
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "run_flow_experiment_batch.py", "custom",
            "--experiments", "baseline",
            "--config_override", override_str,
            "--notes", f"HyperSearch_{experiment_name}",
            "--wandb"
        ]
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è¿è¡Œå®éªŒ
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)  # 1å°æ—¶è¶…æ—¶
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            # è§£æç»“æœ
            experiment_result = self._parse_experiment_result(experiment_name)
            experiment_result['status'] = 'success'
            experiment_result['duration'] = duration
            experiment_result['stdout'] = result.stdout[-1000:]  # ä¿ç•™æœ€å1000å­—ç¬¦
            
            return experiment_result
        else:
            raise RuntimeError(f"å®éªŒå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}, é”™è¯¯: {result.stderr}")
    
    def _parse_experiment_result(self, experiment_name):
        """è§£æå®éªŒç»“æœ"""
        
        # å°è¯•ä»ç»“æœæ–‡ä»¶ä¸­è¯»å–æŒ‡æ ‡
        results_dir = Path("results")
        
        # æŸ¥æ‰¾åŒ¹é…çš„å®éªŒç›®å½•
        experiment_dirs = list(results_dir.glob(f"*{experiment_name}*"))
        
        if not experiment_dirs:
            print(f"âš ï¸  æœªæ‰¾åˆ°å®éªŒç›®å½•: {experiment_name}")
            return {'accuracy': 0, 'f1_score': 0}
        
        # å–æœ€æ–°çš„å®éªŒç›®å½•
        experiment_dir = max(experiment_dirs, key=lambda x: x.stat().st_mtime)
        
        # å°è¯•è¯»å–æŒ‡æ ‡æ–‡ä»¶
        metrics_file = experiment_dir / "metrics.json"
        if metrics_file.exists():
            with open(metrics_file) as f:
                metrics = json.load(f)
            return {
                'accuracy': metrics.get('test_accuracy', metrics.get('accuracy', 0)),
                'f1_score': metrics.get('test_f1', metrics.get('f1_score', 0)),
                'train_time': metrics.get('training_time', 0),
                'params_count': metrics.get('model_params', 0)
            }
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°metrics.jsonï¼Œå°è¯•ä»Lightningæ—¥å¿—è§£æ
        lightning_dir = experiment_dir / "lightning_logs" / "version_0"
        metrics_csv = lightning_dir / "metrics.csv"
        
        if metrics_csv.exists():
            df = pd.read_csv(metrics_csv)
            if not df.empty:
                last_row = df.iloc[-1]
                return {
                    'accuracy': last_row.get('val_accuracy', last_row.get('test_accuracy', 0)),
                    'f1_score': last_row.get('val_f1', last_row.get('test_f1', 0)),
                    'train_time': 0,
                    'params_count': 0
                }
        
        print(f"âš ï¸  æ— æ³•è§£æå®éªŒç»“æœ: {experiment_name}")
        return {'accuracy': 0, 'f1_score': 0}
    
    def _save_intermediate_results(self, output_dir):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        results_file = Path(output_dir) / "intermediate_results.json"
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2)
    
    def _save_final_results(self, output_dir):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        
        # ä¿å­˜JSONæ ¼å¼
        results_json = Path(output_dir) / "final_results.json"
        with open(results_json, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼
        if self.results:
            df_data = []
            for result in self.results:
                if result.get('status') == 'success':
                    row = result.copy()
                    # å±•å¼€paramså­—å…¸
                    if 'params' in row:
                        params = row.pop('params')
                        for param_name, param_value in params.items():
                            row[f'param_{param_name.replace(".", "_")}'] = param_value
                    df_data.append(row)
            
            if df_data:
                df = pd.DataFrame(df_data)
                results_csv = Path(output_dir) / "final_results.csv"
                df.to_csv(results_csv, index=False)
                
                # æ‰“å°æœ€ä½³ç»“æœ
                if 'accuracy' in df.columns:
                    best_idx = df['accuracy'].idxmax()
                    best_result = df.loc[best_idx]
                    
                    print(f"\nğŸ† æœ€ä½³ç»“æœ:")
                    print(f"  å‡†ç¡®ç‡: {best_result['accuracy']:.4f}")
                    print(f"  F1åˆ†æ•°: {best_result.get('f1_score', 'N/A')}")
                    print(f"  å‚æ•°:")
                    
                    for col in df.columns:
                        if col.startswith('param_'):
                            param_name = col.replace('param_', '').replace('_', '.')
                            print(f"    {param_name}: {best_result[col]}")
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description="Flowè¶…å‚æ•°æœç´¢")
    parser.add_argument('--method', choices=['grid', 'random', 'bayesian'], 
                       default='grid', help='æœç´¢æ–¹æ³•')
    parser.add_argument('--config', type=str, 
                       default='configs/demo/Pretraining/Flow/flow_baseline_experiment.yaml',
                       help='åŸºç¡€é…ç½®æ–‡ä»¶')
    parser.add_argument('--max_experiments', type=int, help='æœ€å¤§å®éªŒæ•°é‡')
    parser.add_argument('--output_dir', type=str, default='hyperparameter_sweep',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¶…å‚æ•°æœç´¢å™¨
    sweep = HyperparameterSweep(args.config)
    
    if args.method == 'grid':
        # ç½‘æ ¼æœç´¢å‚æ•°ç©ºé—´
        param_grid = {
            'task.lr': [1e-4, 5e-4, 1e-3],
            'task.flow_lr': [1e-4, 5e-4, 1e-3],
            'task.contrastive_weight': [0.1, 0.3, 0.5],
            'model.hidden_dim': [256, 512],
            'task.num_steps': [50, 100, 200],
            'task.batch_size': [32, 64]
        }
        
        results = sweep.grid_search(param_grid, args.max_experiments, args.output_dir)
        
    elif args.method == 'random':
        # éšæœºæœç´¢å‚æ•°åˆ†å¸ƒ
        param_distributions = {
            'task.lr': {'type': 'loguniform', 'low': 1e-5, 'high': 1e-2},
            'task.flow_lr': {'type': 'loguniform', 'low': 1e-5, 'high': 1e-2},
            'task.contrastive_weight': {'type': 'uniform', 'low': 0.0, 'high': 1.0},
            'model.hidden_dim': [128, 256, 512, 1024],
            'task.num_steps': [20, 50, 100, 200, 500],
            'task.batch_size': [16, 32, 64, 128]
        }
        
        n_experiments = args.max_experiments or 50
        results = sweep.random_search(param_distributions, n_experiments, args.output_dir)
        
    elif args.method == 'bayesian':
        # è´å¶æ–¯ä¼˜åŒ–å‚æ•°è¾¹ç•Œ
        param_bounds = {
            'task.lr': {'type': 'real', 'low': 1e-5, 'high': 1e-2},
            'task.flow_lr': {'type': 'real', 'low': 1e-5, 'high': 1e-2},
            'task.contrastive_weight': {'type': 'real', 'low': 0.0, 'high': 1.0},
            'model.hidden_dim': {'type': 'categorical', 'choices': [128, 256, 512, 1024]},
            'task.num_steps': {'type': 'integer', 'low': 20, 'high': 500},
            'task.batch_size': {'type': 'categorical', 'choices': [16, 32, 64, 128]}
        }
        
        n_experiments = args.max_experiments or 30
        results = sweep.bayesian_optimization(param_bounds, n_experiments, args.output_dir)
    
    print(f"\nğŸ‰ è¶…å‚æ•°æœç´¢å®Œæˆ!")
    print(f"æ€»å®éªŒæ•°: {len(results) if isinstance(results, list) else len(results['all_results'])}")


if __name__ == "__main__":
    main()