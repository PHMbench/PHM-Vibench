"""
SOTAå¯¹æ¯”å®éªŒæ¡†æ¶
é¢å‘é¡¶çº§è®ºæ–‡å‘è¡¨çš„å…¨é¢åŸºå‡†æµ‹è¯•ç³»ç»Ÿ

æ”¯æŒä¸ä»¥ä¸‹8ç§SOTAæ–¹æ³•å¯¹æ¯”ï¼š
1. DANN (Domain Adversarial Neural Networks)
2. CORAL (Deep CORAL)  
3. MMD (Maximum Mean Discrepancy)
4. CDAN (Conditional Domain Adversarial Networks)
5. MCD (Maximum Classifier Discrepancy)
6. SHOT (Source Hypothesis Transfer)
7. NRC (Neighborhood Reciprocal Clustering)
8. Transformer-based baseline

Authors: PHMbench Team
Target: ICML/NeurIPS 2025
"""

import os
import sys
import yaml
import json
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple
from pathlib import Path
import subprocess
import argparse
from datetime import datetime
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/lq/LQcode/2_project/PHMBench/PHM-Vibench-metric')

@dataclass
class ExperimentResult:
    """å®éªŒç»“æœæ•°æ®ç±»"""
    method_name: str
    dataset: str
    accuracy: float
    f1_score: float
    precision: float
    recall: float
    training_time: float
    inference_time: float
    memory_usage: float
    convergence_epoch: int
    config_path: str
    run_id: int = 0

class SOTAComparison:
    """SOTAæ–¹æ³•å¯¹æ¯”æ¡†æ¶"""
    
    def __init__(self, base_config_path: str, results_dir: str = "results/sota_comparison"):
        """
        åˆå§‹åŒ–SOTAå¯¹æ¯”æ¡†æ¶
        
        Args:
            base_config_path: HSEåŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„
            results_dir: ç»“æœä¿å­˜ç›®å½•
        """
        self.base_config_path = base_config_path
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # SOTAæ–¹æ³•é…ç½®æ˜ å°„
        self.sota_methods = {
            "HSE-CL": {
                "name": "HSEå¼‚æ„å¯¹æ¯”å­¦ä¹ ",
                "task_name": "hse_contrastive",
                "task_type": "CDDG",
                "description": "æˆ‘ä»¬æå‡ºçš„HSEç³»ç»Ÿçº§å¯¹æ¯”å­¦ä¹ æ–¹æ³•"
            },
            "DANN": {
                "name": "Domain Adversarial Neural Networks", 
                "task_name": "classification",
                "task_type": "CDDG",
                "description": "åŸŸå¯¹æŠ—ç¥ç»ç½‘ç»œ",
                "modifications": {"task.domain_loss": "adversarial", "task.domain_loss_weight": 0.1}
            },
            "CORAL": {
                "name": "Deep CORAL",
                "task_name": "classification", 
                "task_type": "CDDG",
                "description": "æ·±åº¦CORALåŸŸé€‚åº”",
                "modifications": {"task.domain_loss": "coral", "task.domain_loss_weight": 0.1}
            },
            "MMD": {
                "name": "Maximum Mean Discrepancy",
                "task_name": "classification",
                "task_type": "CDDG", 
                "description": "æœ€å¤§å‡å€¼å·®å¼‚",
                "modifications": {"task.domain_loss": "mmd", "task.domain_loss_weight": 0.1}
            },
            "CDAN": {
                "name": "Conditional Domain Adversarial Networks",
                "task_name": "classification",
                "task_type": "CDDG",
                "description": "æ¡ä»¶åŸŸå¯¹æŠ—ç½‘ç»œ",
                "modifications": {"task.domain_loss": "cdan", "task.domain_loss_weight": 0.1}
            },
            "MCD": {
                "name": "Maximum Classifier Discrepancy", 
                "task_name": "classification",
                "task_type": "CDDG",
                "description": "æœ€å¤§åˆ†ç±»å™¨å·®å¼‚",
                "modifications": {"task.classifier_discrepancy": True}
            },
            "SHOT": {
                "name": "Source Hypothesis Transfer",
                "task_name": "classification",
                "task_type": "CDDG", 
                "description": "æºå‡è®¾è¿ç§»",
                "modifications": {"task.self_training": True, "task.pseudo_label_threshold": 0.9}
            },
            "NRC": {
                "name": "Neighborhood Reciprocal Clustering",
                "task_name": "classification",
                "task_type": "CDDG",
                "description": "é‚»åŸŸäº’åèšç±»",
                "modifications": {"task.clustering_loss": "nrc", "task.cluster_weight": 0.1}
            },
            "Transformer": {
                "name": "Transformer Baseline",
                "task_name": "classification",
                "task_type": "CDDG",
                "description": "æ ‡å‡†TransformeråŸºçº¿",
                "modifications": {"model.backbone": "B_08_PatchTST", "task.contrast_weight": 0.0}
            }
        }
        
        # å®éªŒé…ç½®
        self.datasets = ["CWRU", "XJTU", "THU", "MFPT", "PU"]
        self.target_systems = [1, 5, 13, 19, 21]  # å¯¹åº”æ¯ä¸ªæ•°æ®é›†çš„ç›®æ ‡åŸŸ
        self.num_runs = 5  # æ¯ä¸ªæ–¹æ³•é‡å¤è¿è¡Œæ¬¡æ•°
        self.max_workers = 2  # å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
        
        # ç»“æœå­˜å‚¨
        self.results: List[ExperimentResult] = []
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_file = self.results_dir / "sota_comparison.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def create_method_config(self, method_key: str, target_system_id: int) -> str:
        """
        ä¸ºæŒ‡å®šæ–¹æ³•åˆ›å»ºé…ç½®æ–‡ä»¶
        
        Args:
            method_key: æ–¹æ³•æ ‡è¯†ç¬¦
            target_system_id: ç›®æ ‡ç³»ç»ŸID
            
        Returns:
            ç”Ÿæˆçš„é…ç½®æ–‡ä»¶è·¯å¾„
        """
        method_info = self.sota_methods[method_key]
        
        # åŠ è½½åŸºç¡€é…ç½®
        with open(self.base_config_path, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        
        # ä¿®æ”¹ä»»åŠ¡é…ç½®
        base_config['task']['name'] = method_info['task_name']
        base_config['task']['type'] = method_info['task_type']
        base_config['task']['target_system_id'] = [target_system_id]
        
        # åº”ç”¨æ–¹æ³•ç‰¹å®šä¿®æ”¹
        if 'modifications' in method_info:
            for key, value in method_info['modifications'].items():
                keys = key.split('.')
                config_section = base_config
                for k in keys[:-1]:
                    config_section = config_section.setdefault(k, {})
                config_section[keys[-1]] = value
        
        # æ›´æ–°å®éªŒæ ‡è¯†
        base_config['environment']['project'] = f"SOTA_{method_key}_vs_HSE"
        base_config['environment']['notes'] = f"{method_info['description']} vs HSEå¯¹æ¯”å®éªŒ"
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = self.results_dir / f"{method_key}_target{target_system_id}.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(base_config, f, default_flow_style=False, allow_unicode=True)
            
        return str(config_path)
    
    def run_single_experiment(self, method_key: str, target_system_id: int, run_id: int) -> ExperimentResult:
        """
        è¿è¡Œå•ä¸ªå®éªŒ
        
        Args:
            method_key: æ–¹æ³•æ ‡è¯†ç¬¦
            target_system_id: ç›®æ ‡ç³»ç»ŸID
            run_id: è¿è¡ŒID
            
        Returns:
            å®éªŒç»“æœ
        """
        method_info = self.sota_methods[method_key]
        dataset_name = self.get_dataset_name(target_system_id)
        
        self.logger.info(f"å¼€å§‹è¿è¡Œ {method_key} åœ¨æ•°æ®é›† {dataset_name} (ç›®æ ‡ç³»ç»Ÿ {target_system_id}), Run {run_id}")
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        config_path = self.create_method_config(method_key, target_system_id)
        
        # æ„å»ºè¿è¡Œå‘½ä»¤
        cmd = [
            "python", "main.py",
            "--config", config_path,
            "--override", f"{{\"environment.seed\": {42 + run_id}}}"  # ä¸åŒè¿è¡Œä½¿ç”¨ä¸åŒç§å­
        ]
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.now()
        
        try:
            # è¿è¡Œå®éªŒ
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=7200,  # 2å°æ—¶è¶…æ—¶
                cwd="/home/lq/LQcode/2_project/PHMBench/PHM-Vibench-metric"
            )
            
            # è®¡ç®—è®­ç»ƒæ—¶é—´
            training_time = (datetime.now() - start_time).total_seconds()
            
            if result.returncode == 0:
                # è§£æå®éªŒç»“æœ
                metrics = self.parse_experiment_results(result.stdout, result.stderr)
                
                return ExperimentResult(
                    method_name=method_key,
                    dataset=dataset_name,
                    accuracy=metrics.get('accuracy', 0.0),
                    f1_score=metrics.get('f1_score', 0.0), 
                    precision=metrics.get('precision', 0.0),
                    recall=metrics.get('recall', 0.0),
                    training_time=training_time,
                    inference_time=metrics.get('inference_time', 0.0),
                    memory_usage=metrics.get('memory_usage', 0.0),
                    convergence_epoch=metrics.get('convergence_epoch', 50),
                    config_path=config_path,
                    run_id=run_id
                )
            else:
                self.logger.error(f"å®éªŒå¤±è´¥ {method_key} - {dataset_name}: {result.stderr}")
                return self.create_failed_result(method_key, dataset_name, run_id, config_path)
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"å®éªŒè¶…æ—¶ {method_key} - {dataset_name}")
            return self.create_failed_result(method_key, dataset_name, run_id, config_path)
        except Exception as e:
            self.logger.error(f"å®éªŒå¼‚å¸¸ {method_key} - {dataset_name}: {str(e)}")
            return self.create_failed_result(method_key, dataset_name, run_id, config_path)
    
    def parse_experiment_results(self, stdout: str, stderr: str) -> Dict[str, float]:
        """è§£æå®éªŒç»“æœ"""
        metrics = {}
        
        try:
            # ä»è¾“å‡ºä¸­è§£æå…³é”®æŒ‡æ ‡
            lines = stdout.split('\n') + stderr.split('\n')
            
            for line in lines:
                if 'test_acc' in line.lower():
                    # æå–å‡†ç¡®ç‡
                    parts = line.split()
                    for i, part in enumerate(parts):
                        if 'acc' in part.lower() and i + 1 < len(parts):
                            try:
                                metrics['accuracy'] = float(parts[i + 1])
                            except ValueError:
                                pass
                            
                elif 'f1' in line.lower():
                    # æå–F1åˆ†æ•°
                    parts = line.split()
                    for i, part in enumerate(parts):
                        if 'f1' in part.lower() and i + 1 < len(parts):
                            try:
                                metrics['f1_score'] = float(parts[i + 1])
                            except ValueError:
                                pass
                                
            # é»˜è®¤å€¼
            metrics.setdefault('accuracy', 0.0)
            metrics.setdefault('f1_score', 0.0)
            metrics.setdefault('precision', 0.0)
            metrics.setdefault('recall', 0.0)
            metrics.setdefault('inference_time', 0.0)
            metrics.setdefault('memory_usage', 0.0)
            metrics.setdefault('convergence_epoch', 50)
            
        except Exception as e:
            self.logger.warning(f"ç»“æœè§£æå¤±è´¥: {str(e)}")
            metrics = {
                'accuracy': 0.0,
                'f1_score': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'inference_time': 0.0,
                'memory_usage': 0.0,
                'convergence_epoch': 50
            }
        
        return metrics
    
    def create_failed_result(self, method_key: str, dataset_name: str, run_id: int, config_path: str) -> ExperimentResult:
        """åˆ›å»ºå¤±è´¥å®éªŒçš„ç»“æœå¯¹è±¡"""
        return ExperimentResult(
            method_name=method_key,
            dataset=dataset_name,
            accuracy=0.0,
            f1_score=0.0,
            precision=0.0,
            recall=0.0,
            training_time=0.0,
            inference_time=0.0,
            memory_usage=0.0,
            convergence_epoch=0,
            config_path=config_path,
            run_id=run_id
        )
    
    def get_dataset_name(self, target_system_id: int) -> str:
        """æ ¹æ®ç³»ç»ŸIDè·å–æ•°æ®é›†åç§°"""
        mapping = {1: "CWRU", 5: "XJTU", 13: "THU", 19: "MFPT", 21: "PU"}
        return mapping.get(target_system_id, f"System_{target_system_id}")
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰SOTAæ–¹æ³•å¯¹æ¯”å®éªŒ"""
        self.logger.info("å¼€å§‹è¿è¡ŒSOTAæ–¹æ³•å¯¹æ¯”å®éªŒ")
        
        total_experiments = len(self.sota_methods) * len(self.target_systems) * self.num_runs
        self.logger.info(f"æ€»è®¡å®éªŒæ•°é‡: {total_experiments}")
        
        completed = 0
        
        # ä¸²è¡Œè¿è¡Œå®éªŒï¼ˆé¿å…èµ„æºå†²çªï¼‰
        for method_key in self.sota_methods.keys():
            for target_system_id in self.target_systems:
                for run_id in range(self.num_runs):
                    result = self.run_single_experiment(method_key, target_system_id, run_id)
                    self.results.append(result)
                    completed += 1
                    
                    self.logger.info(f"å®Œæˆè¿›åº¦: {completed}/{total_experiments} ({completed/total_experiments*100:.1f}%)")
                    
                    # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
                    if completed % 10 == 0:
                        self.save_intermediate_results()
        
        self.logger.info("æ‰€æœ‰å®éªŒå®Œæˆ!")
    
    def save_intermediate_results(self):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        results_file = self.results_dir / "intermediate_results.json"
        
        results_data = []
        for result in self.results:
            results_data.append({
                'method_name': result.method_name,
                'dataset': result.dataset,
                'accuracy': result.accuracy,
                'f1_score': result.f1_score,
                'precision': result.precision,
                'recall': result.recall,
                'training_time': result.training_time,
                'run_id': result.run_id
            })
        
        with open(results_file, 'w') as f:
            json.dump(results_data, f, indent=2)
    
    def analyze_results(self) -> pd.DataFrame:
        """åˆ†æå®éªŒç»“æœ"""
        self.logger.info("å¼€å§‹åˆ†æå®éªŒç»“æœ")
        
        # è½¬æ¢ä¸ºDataFrame
        results_data = []
        for result in self.results:
            results_data.append({
                'Method': result.method_name,
                'Dataset': result.dataset, 
                'Accuracy': result.accuracy,
                'F1-Score': result.f1_score,
                'Precision': result.precision,
                'Recall': result.recall,
                'Training_Time': result.training_time,
                'Run_ID': result.run_id
            })
        
        df = pd.DataFrame(results_data)
        
        # è®¡ç®—ç»Ÿè®¡æ‘˜è¦
        summary_stats = df.groupby(['Method', 'Dataset']).agg({
            'Accuracy': ['mean', 'std', 'min', 'max'],
            'F1-Score': ['mean', 'std'],
            'Training_Time': ['mean', 'std']
        }).round(4)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.results_dir / "detailed_results.csv"
        df.to_csv(results_file, index=False)
        
        # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
        summary_file = self.results_dir / "summary_statistics.csv"
        summary_stats.to_csv(summary_file)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {results_file} å’Œ {summary_file}")
        
        return df
    
    def statistical_significance_test(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
        """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        self.logger.info("è¿›è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ")
        
        significance_results = {}
        
        # è·å–HSE-CLçš„ç»“æœä½œä¸ºåŸºå‡†
        hse_results = df[df['Method'] == 'HSE-CL']['Accuracy'].values
        
        for method in df['Method'].unique():
            if method == 'HSE-CL':
                continue
                
            method_results = df[df['Method'] == method]['Accuracy'].values
            
            if len(method_results) > 0 and len(hse_results) > 0:
                # æ‰§è¡Œtæ£€éªŒ
                t_stat, p_value = stats.ttest_ind(hse_results, method_results)
                
                # è®¡ç®—æ•ˆæœé‡ (Cohen's d)
                pooled_std = np.sqrt(((len(hse_results) - 1) * np.var(hse_results) + 
                                     (len(method_results) - 1) * np.var(method_results)) / 
                                    (len(hse_results) + len(method_results) - 2))
                cohen_d = (np.mean(hse_results) - np.mean(method_results)) / pooled_std
                
                significance_results[method] = {
                    't_statistic': t_stat,
                    'p_value': p_value,
                    'cohen_d': cohen_d,
                    'significant': p_value < 0.01,  # Î± = 0.01
                    'hse_mean': np.mean(hse_results),
                    'method_mean': np.mean(method_results),
                    'improvement': np.mean(hse_results) - np.mean(method_results)
                }
        
        # ä¿å­˜æ˜¾è‘—æ€§æ£€éªŒç»“æœ
        sig_file = self.results_dir / "significance_test.json"
        with open(sig_file, 'w') as f:
            json.dump(significance_results, f, indent=2, default=float)
        
        return significance_results
    
    def generate_paper_tables(self, df: pd.DataFrame) -> str:
        """ç”Ÿæˆè®ºæ–‡ç”¨çš„LaTeXè¡¨æ ¼"""
        self.logger.info("ç”Ÿæˆè®ºæ–‡LaTeXè¡¨æ ¼")
        
        # è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
        summary = df.groupby('Method').agg({
            'Accuracy': ['mean', 'std'],
            'F1-Score': ['mean', 'std'],
            'Training_Time': ['mean', 'std']
        }).round(4)
        
        # ç”ŸæˆLaTeXè¡¨æ ¼
        latex_table = """
\\begin{table}[htbp]
\\centering
\\caption{Performance Comparison with SOTA Methods}
\\label{tab:sota_comparison}
\\begin{tabular}{lccccc}
\\toprule
Method & Accuracy (\\%) & F1-Score & Precision & Recall & Training Time (s) \\\\
\\midrule
"""
        
        # æŒ‰å‡†ç¡®ç‡æ’åº
        methods_ranked = summary.sort_values(('Accuracy', 'mean'), ascending=False)
        
        for method in methods_ranked.index:
            acc_mean = methods_ranked.loc[method, ('Accuracy', 'mean')] * 100
            acc_std = methods_ranked.loc[method, ('Accuracy', 'std')] * 100
            f1_mean = methods_ranked.loc[method, ('F1-Score', 'mean')]
            f1_std = methods_ranked.loc[method, ('F1-Score', 'std')]
            time_mean = methods_ranked.loc[method, ('Training_Time', 'mean')]
            
            # çªå‡ºæ˜¾ç¤ºæœ€ä½³ç»“æœ
            if method == 'HSE-CL':
                latex_table += f"\\textbf{{{method}}} & \\textbf{{{acc_mean:.2f} Â± {acc_std:.2f}}} & \\textbf{{{f1_mean:.3f} Â± {f1_std:.3f}}} & - & - & {time_mean:.1f} \\\\\n"
            else:
                latex_table += f"{method} & {acc_mean:.2f} Â± {acc_std:.2f} & {f1_mean:.3f} Â± {f1_std:.3f} & - & - & {time_mean:.1f} \\\\\n"
        
        latex_table += """\\bottomrule
\\end{tabular}
\\end{table}
"""
        
        # ä¿å­˜LaTeXè¡¨æ ¼
        latex_file = self.results_dir / "sota_comparison_table.tex"
        with open(latex_file, 'w') as f:
            f.write(latex_table)
        
        return latex_table
    
    def create_visualization(self, df: pd.DataFrame):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        self.logger.info("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # 1. å‡†ç¡®ç‡å¯¹æ¯”æŸ±çŠ¶å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # å¹³å‡å‡†ç¡®ç‡å¯¹æ¯”
        method_acc = df.groupby('Method')['Accuracy'].agg(['mean', 'std']).sort_values('mean', ascending=False)
        
        ax1 = axes[0, 0]
        bars = ax1.bar(range(len(method_acc)), method_acc['mean'], yerr=method_acc['std'], 
                       capsize=5, alpha=0.8)
        
        # çªå‡ºæ˜¾ç¤ºHSE-CL
        for i, method in enumerate(method_acc.index):
            if method == 'HSE-CL':
                bars[i].set_color('red')
                bars[i].set_alpha(1.0)
        
        ax1.set_xlabel('Methods')
        ax1.set_ylabel('Accuracy')
        ax1.set_title('Average Accuracy Comparison')
        ax1.set_xticks(range(len(method_acc)))
        ax1.set_xticklabels(method_acc.index, rotation=45)
        ax1.grid(axis='y', alpha=0.3)
        
        # 2. F1åˆ†æ•°å¯¹æ¯”
        method_f1 = df.groupby('Method')['F1-Score'].agg(['mean', 'std']).sort_values('mean', ascending=False)
        
        ax2 = axes[0, 1]
        bars2 = ax2.bar(range(len(method_f1)), method_f1['mean'], yerr=method_f1['std'],
                        capsize=5, alpha=0.8)
        
        for i, method in enumerate(method_f1.index):
            if method == 'HSE-CL':
                bars2[i].set_color('red')
                bars2[i].set_alpha(1.0)
        
        ax2.set_xlabel('Methods')
        ax2.set_ylabel('F1-Score')
        ax2.set_title('F1-Score Comparison')
        ax2.set_xticks(range(len(method_f1)))
        ax2.set_xticklabels(method_f1.index, rotation=45)
        ax2.grid(axis='y', alpha=0.3)
        
        # 3. è®­ç»ƒæ—¶é—´å¯¹æ¯”
        method_time = df.groupby('Method')['Training_Time'].agg(['mean', 'std']).sort_values('mean')
        
        ax3 = axes[1, 0]
        ax3.bar(range(len(method_time)), method_time['mean'], yerr=method_time['std'],
                capsize=5, alpha=0.8, color='green')
        ax3.set_xlabel('Methods')
        ax3.set_ylabel('Training Time (s)')
        ax3.set_title('Training Time Comparison')
        ax3.set_xticks(range(len(method_time)))
        ax3.set_xticklabels(method_time.index, rotation=45)
        ax3.grid(axis='y', alpha=0.3)
        
        # 4. è·¨æ•°æ®é›†æ€§èƒ½çƒ­åŠ›å›¾
        pivot_acc = df.pivot_table(values='Accuracy', index='Method', columns='Dataset', aggfunc='mean')
        
        ax4 = axes[1, 1]
        sns.heatmap(pivot_acc, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax4)
        ax4.set_title('Cross-Dataset Performance Heatmap')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        fig_file = self.results_dir / "sota_comparison_plots.png"
        plt.savefig(fig_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° {fig_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="SOTAæ–¹æ³•å¯¹æ¯”å®éªŒ")
    parser.add_argument("--base_config", type=str, 
                       default="configs/demo/HSE_Contrastive/hse_cddg.yaml",
                       help="HSEåŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--results_dir", type=str,
                       default="results/sota_comparison",
                       help="ç»“æœä¿å­˜ç›®å½•")
    parser.add_argument("--methods", type=str, nargs="+",
                       default=None,
                       help="è¦è¿è¡Œçš„æ–¹æ³•åˆ—è¡¨ (é»˜è®¤è¿è¡Œæ‰€æœ‰æ–¹æ³•)")
    parser.add_argument("--datasets", type=int, nargs="+",
                       default=[1, 5, 13, 19, 21],
                       help="ç›®æ ‡ç³»ç»ŸIDåˆ—è¡¨")
    parser.add_argument("--num_runs", type=int, default=5,
                       help="æ¯ä¸ªæ–¹æ³•çš„é‡å¤è¿è¡Œæ¬¡æ•°")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯¹æ¯”æ¡†æ¶
    comparison = SOTAComparison(args.base_config, args.results_dir)
    
    # è®¾ç½®å®éªŒå‚æ•°
    if args.methods:
        comparison.sota_methods = {k: v for k, v in comparison.sota_methods.items() 
                                  if k in args.methods}
    comparison.target_systems = args.datasets
    comparison.num_runs = args.num_runs
    
    print(f"ğŸš€ å¼€å§‹SOTAå¯¹æ¯”å®éªŒ")
    print(f"   - æ–¹æ³•æ•°é‡: {len(comparison.sota_methods)}")
    print(f"   - æ•°æ®é›†æ•°é‡: {len(comparison.target_systems)}")
    print(f"   - é‡å¤æ¬¡æ•°: {comparison.num_runs}")
    print(f"   - æ€»å®éªŒæ•°: {len(comparison.sota_methods) * len(comparison.target_systems) * comparison.num_runs}")
    
    # è¿è¡Œå®éªŒ
    comparison.run_all_experiments()
    
    # åˆ†æç»“æœ
    df = comparison.analyze_results()
    
    # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    sig_results = comparison.statistical_significance_test(df)
    
    # ç”Ÿæˆè®ºæ–‡è¡¨æ ¼
    latex_table = comparison.generate_paper_tables(df)
    
    # åˆ›å»ºå¯è§†åŒ–
    comparison.create_visualization(df)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ¯ å®éªŒå®Œæˆæ‘˜è¦")
    print("="*50)
    
    hse_results = df[df['Method'] == 'HSE-CL']['Accuracy']
    if len(hse_results) > 0:
        print(f"HSE-CLå¹³å‡å‡†ç¡®ç‡: {hse_results.mean():.4f} Â± {hse_results.std():.4f}")
    
    print("\næ˜¾è‘—æ€§æ£€éªŒç»“æœ:")
    for method, stats in sig_results.items():
        improvement = stats['improvement'] * 100
        significance = "âœ“" if stats['significant'] else "âœ—"
        print(f"  {method}: {improvement:+.2f}% (p={stats['p_value']:.4f}) {significance}")
    
    print(f"\nğŸ“Š è¯¦ç»†ç»“æœä¿å­˜åœ¨: {comparison.results_dir}")
    print("ğŸ“ è®ºæ–‡ç´ æå·²ç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    main()