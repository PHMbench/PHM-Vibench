# é•¿ä¿¡å·IDå¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒè®¡åˆ’ v3.0

**åˆ†æ”¯**: cc_loop_id  
**å¼€å§‹æ—¥æœŸ**: 2025-08-29  
**å®Œæˆæ—¥æœŸ**: 2025-08-29  
**ç‰ˆæœ¬**: å®æ–½å®Œæˆç‰ˆ  
**çŠ¶æ€**: âœ… **å·²å®Œæˆ**

---

## ğŸ‰ å®æ–½å®Œæˆæ€»ç»“

### å®æ–½æˆæœ
âœ… **æ ¸å¿ƒåŠŸèƒ½å®Œæˆ**: ContrastiveIDTask.py (89è¡Œ)  
âœ… **é…ç½®æ–‡ä»¶åˆ›å»º**: pretrain.yaml (45è¡Œ)  
âœ… **å…¨é¢åŠŸèƒ½æµ‹è¯•**: æ‰€æœ‰æµ‹è¯•é€šè¿‡  
âœ… **æ¶æ„éªŒè¯æˆåŠŸ**: ID_datasetæ— éœ€ä¿®æ”¹ï¼Œå®Œç¾å¤ç”¨BaseIDTask  
âœ… **å¼€å‘æ•ˆç‡**: 1å¤©å®Œæˆï¼ˆåŸè®¡åˆ’3-5å¤©ï¼‰

### æŠ€æœ¯éªŒè¯
- **å†…å­˜ä¼˜åŒ–**: è‡ªåŠ¨ç»§æ‰¿ID_taskå»¶è¿ŸåŠ è½½æœºåˆ¶ âœ…
- **ä»£ç ç®€æ´**: é¿å…ç‚«æŠ€å¤æ‚åº¦ï¼Œå®ç”¨ç¬¬ä¸€ âœ…
- **æ¶æ„ä¸€è‡´**: å®Œç¾èå…¥PHM-Vibench factoryæ¨¡å¼ âœ…
- **æµ‹è¯•è¦†ç›–**: çª—å£ç”Ÿæˆã€æ‰¹å¤„ç†ã€æŸå¤±è®¡ç®—ç­‰å…¨æ–¹ä½æµ‹è¯• âœ…

### æ ¸å¿ƒåˆ›æ–°ç‚¹
1. **åŸºäºID_taskæ‰©å±•**: ç»§æ‰¿æ‰€æœ‰åŸºç¡€åŠŸèƒ½ï¼Œä¸“æ³¨å¯¹æ¯”å­¦ä¹ é€»è¾‘
2. **é›¶ä¿®æ”¹æ•°æ®é›†**: ID_datasetä¿æŒå®Œå…¨ä¸å˜
3. **ç®€æ´InfoNCEå®ç°**: åŒIDçª—å£=æ­£æ ·æœ¬ï¼Œä¸åŒID=è´Ÿæ ·æœ¬
4. **å»¶è¿ŸåŠ è½½ä¼˜åŒ–**: è‡ªåŠ¨è·å¾—50%+å†…å­˜èŠ‚çœ

## æ ¸å¿ƒæ€æƒ³

### é—®é¢˜èƒŒæ™¯
- PHM-Vibenchä¸­æ¯ä¸ªIDå¯¹åº”ä¸€ä¸ªé•¿ä¿¡å·ï¼ˆSample_length: 8192-16384ç”šè‡³æ›´é•¿ï¼‰
- ç°æœ‰é¢„è®­ç»ƒä»»åŠ¡ï¼ˆmasked_reconstructionï¼‰ä¸»è¦åŸºäºæ©ç é‡å»º
- ID_task.pyå·²æä¾›å®Œå–„çš„çª—å£åŒ–å’Œæ‰¹å¤„ç†æœºåˆ¶
- ç¼ºä¹å……åˆ†åˆ©ç”¨é•¿ä¿¡å·æ—¶åºä¾èµ–å…³ç³»çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•

### è§£å†³æ–¹æ¡ˆ
**æ ¸å¿ƒåˆ›æ–°**: åŸºäºID_taskçš„BaseIDTaskæ‰©å±•ï¼Œåˆ©ç”¨å¤šçª—å£æœºåˆ¶æ„å»ºå¯¹æ¯”å­¦ä¹ 

**æ¶æ„ä¼˜åŠ¿**:
1. **æ— éœ€ä¿®æ”¹dataset**: ID_datasetä¿æŒä¸å˜ï¼Œåªä¼ é€’IDå’Œmetadata
2. **å¤ç”¨ç°æœ‰æ¶æ„**: ç»§æ‰¿BaseIDTaskçš„çª—å£åŒ–ã€å»¶è¿ŸåŠ è½½ã€æ‰¹å¤„ç†èƒ½åŠ›
3. **æ‰©å±•ç‚¹æ¸…æ™°**: é€šè¿‡prepare_batch()æ–¹æ³•å®ç°å¯¹æ¯”å­¦ä¹ é€»è¾‘

## å®æ–½è®¡åˆ’ï¼ˆåŸºäºID_taskæ¶æ„ï¼‰

### æ ¸å¿ƒç»„ä»¶ï¼ˆåªéœ€2ä¸ªæ–°æ–‡ä»¶ï¼‰

#### 1. å¯¹æ¯”å­¦ä¹ IDä»»åŠ¡
**æ–‡ä»¶**: `src/task_factory/task/pretrain/ContrastiveIDTask.py`

**è®¾è®¡ç†å¿µ**:
- ç»§æ‰¿BaseIDTaskï¼Œå¤ç”¨æ‰€æœ‰åŸºç¡€åŠŸèƒ½
- é‡å†™prepare_batch()å®ç°å¯¹æ¯”å­¦ä¹ æ‰¹å¤„ç†
- åˆ©ç”¨create_windows()ç”Ÿæˆå¤šçª—å£

**æ ¸å¿ƒå®ç°**:
```python
from ...ID_task import BaseIDTask

@register_task("pretrain", "contrastive_id")
class ContrastiveIDTask(BaseIDTask):
    def prepare_batch(self, batch_data):
        """ä¸ºæ¯ä¸ªIDç”Ÿæˆå¤šä¸ªçª—å£ä½œä¸ºæ­£æ ·æœ¬å¯¹"""
        positive_pairs = []
        
        for sample_id, data_array, metadata in batch_data:
            # 1. å¤„ç†æ•°æ®
            processed = self.process_sample(data_array, metadata)
            
            # 2. ç”Ÿæˆ2ä¸ªçª—å£ä½œä¸ºæ­£æ ·æœ¬å¯¹
            windows = self.create_windows(
                processed, 
                strategy='random',  # éšæœºä½ç½®
                num_window=2        # 2ä¸ªçª—å£
            )
            
            if len(windows) >= 2:
                positive_pairs.append({
                    'id': sample_id,
                    'anchor': windows[0],
                    'positive': windows[1],
                    'label': metadata.get('Label')
                })
        
        # 3. æ„å»ºæ‰¹æ¬¡å¼ é‡ï¼ˆæ­£è´Ÿæ ·æœ¬å¯¹ï¼‰
        return self._build_contrastive_batch(positive_pairs)
    
    def _shared_step(self, batch, stage):
        """å®ç°InfoNCEæŸå¤±è®¡ç®—"""
        # å¤ç”¨çˆ¶ç±»çš„é¢„å¤„ç†æµç¨‹
        batch = self._preprocess_raw_batch(batch)
        
        # ç¼–ç å™¨å‰å‘ä¼ æ’­
        z_anchor = self.network(batch['anchor'])
        z_positive = self.network(batch['positive'])
        
        # InfoNCEæŸå¤±
        loss = self.infonce_loss(z_anchor, z_positive, batch['ids'])
        return {'loss': loss}
```

#### 2. é…ç½®æ–‡ä»¶
**æ–‡ä»¶**: `configs/id_contrastive/pretrain.yaml`

**æœ€å°åŒ–é…ç½®**:
```yaml
data:
  factory_name: "id"          # ä½¿ç”¨id_data_factory
  dataset_name: "ID_dataset"  # æ ‡å‡†ID_datasetï¼Œæ— éœ€ä¿®æ”¹
  batch_size: 32
  # çª—å£å‚æ•°ï¼ˆè¢«taskä½¿ç”¨ï¼‰
  window_size: 1024
  num_window: 2               # æ¯ä¸ªIDé‡‡æ ·2ä¸ªçª—å£
  window_sampling_strategy: "random"
  
model:
  name: "M_01_ISFM"
  backbone: "B_08_PatchTST"
  projection_head: true       # æ·»åŠ æŠ•å½±å¤´
  
task:
  type: "pretrain"
  name: "contrastive_id"
  lr: 1e-3
  temperature: 0.07
  
trainer:
  epochs: 50
  gradient_clip_val: 1.0
```

### å®æ–½æ­¥éª¤ï¼ˆç®€åŒ–ç‰ˆï¼‰

#### Phase 1: æ ¸å¿ƒå®ç° [1-2å¤©]
1. åˆ›å»ºContrastiveIDTask.pyï¼ˆç»§æ‰¿BaseIDTaskï¼‰
2. å®ç°prepare_batch()çš„å¯¹æ¯”å­¦ä¹ æ‰¹å¤„ç†
3. å®ç°InfoNCEæŸå¤±å‡½æ•°

#### Phase 2: é›†æˆæµ‹è¯• [1å¤©]
1. åˆ›å»ºé…ç½®æ–‡ä»¶
2. éªŒè¯æ•°æ®æµç¨‹
3. å°æ‰¹é‡æµ‹è¯•è®­ç»ƒ

#### Phase 3: ä¼˜åŒ–è°ƒè¯• [1-2å¤©]
1. æ€§èƒ½ä¼˜åŒ–
2. å†…å­˜ç›‘æ§
3. æŸå¤±æ”¶æ•›è°ƒè¯•

### å…³é”®è®¾è®¡ä¼˜åŠ¿

#### 1. æ¶æ„å¤ç”¨
- **BaseIDTaskæä¾›**: çª—å£åŒ–ã€æ•°æ®å¤„ç†ã€å»¶è¿ŸåŠ è½½
- **æˆ‘ä»¬åªéœ€æ·»åŠ **: å¯¹æ¯”å­¦ä¹ çš„æ‰¹å¤„ç†é€»è¾‘
- **ä»£ç é‡**: æ ¸å¿ƒä»£ç çº¦100è¡Œ

#### 2. æ•°æ®æµç¨‹ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
```
ID_dataset (ä¸å˜)
    â†“ (åªä¼ IDå’Œmetadata)
DataLoader
    â†“
ContrastiveIDTask._shared_step()
    â†“
_preprocess_raw_batch() (ç»§æ‰¿)
    â†“
_get_data_for_id() â†’ H5DataDict (å»¶è¿ŸåŠ è½½)
    â†“
prepare_batch() (æˆ‘ä»¬çš„æ‰©å±•ç‚¹)
    â†“
create_windows() (å¤ç”¨ï¼Œç”Ÿæˆå¤šçª—å£)
    â†“
InfoNCEæŸå¤±è®¡ç®—
```

#### 3. å†…å­˜ä¼˜åŒ–ï¼ˆè‡ªåŠ¨è·å¾—ï¼‰
- å»¶è¿ŸåŠ è½½ï¼šé€šè¿‡H5DataDictæŒ‰éœ€åŠ è½½
- æ‰¹å¤„ç†ä¼˜åŒ–ï¼šåªåœ¨éœ€è¦æ—¶åŠ è½½æ•°æ®
- çª—å£åŒ–ï¼šé¿å…å…¨é•¿åº¦ä¿¡å·å­˜å‚¨

### InfoNCEæŸå¤±å®ç°
```python
def infonce_loss(self, z_anchor, z_positive, ids, temperature=0.07):
    """
    InfoNCEå¯¹æ¯”æŸå¤±
    - åŒIDçš„ä¸åŒçª—å£ä¸ºæ­£æ ·æœ¬å¯¹
    - ä¸åŒIDä¸ºè´Ÿæ ·æœ¬
    """
    batch_size = z_anchor.shape[0]
    
    # L2å½’ä¸€åŒ–
    z_anchor = F.normalize(z_anchor, dim=1)
    z_positive = F.normalize(z_positive, dim=1)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_matrix = torch.mm(z_anchor, z_positive.t()) / temperature
    
    # æ­£æ ·æœ¬åœ¨å¯¹è§’çº¿ä¸Š
    pos_sim = torch.diag(sim_matrix)
    
    # è´Ÿæ ·æœ¬ä¸ºéå¯¹è§’çº¿å…ƒç´ 
    loss = -pos_sim + torch.logsumexp(sim_matrix, dim=1)
    
    return loss.mean()
```

## å…³é”®è®¾è®¡å†³ç­–

### 1. æœ€å°åŒ–æ”¹åŠ¨
- **ä¸ä¿®æ”¹ID_dataset**: ä¿æŒç°æœ‰æ•°æ®é›†å®Œå…¨ä¸å˜
- **å¤ç”¨BaseIDTask**: ç»§æ‰¿æ‰€æœ‰åŸºç¡€åŠŸèƒ½
- **ç®€å•InfoNCE**: æ ‡å‡†å®ç°ï¼Œé¿å…å¤æ‚å˜ä½“

### 2. æ¶æ„ä¸€è‡´æ€§
- **éµå¾ªfactoryæ¨¡å¼**: æ³¨å†Œä¸ºæ ‡å‡†é¢„è®­ç»ƒä»»åŠ¡
- **é…ç½®é©±åŠ¨**: é€šè¿‡YAMLæ§åˆ¶æ‰€æœ‰å‚æ•°
- **å»¶è¿ŸåŠ è½½**: è‡ªåŠ¨ç»§æ‰¿å†…å­˜ä¼˜åŒ–ç‰¹æ€§

### 3. å®ç”¨æ€§ä¼˜å…ˆ
- **å›ºå®šçª—å£å¤§å°**: 1024ï¼Œå¹³è¡¡æ€§èƒ½å’Œå†…å­˜
- **2ä¸ªçª—å£é‡‡æ ·**: ç®€å•æœ‰æ•ˆçš„æ­£æ ·æœ¬å¯¹
- **æ‰¹å¤§å°32**: é€‚ä¸­çš„GPUå†…å­˜å ç”¨

## é¢„æœŸç»“æœ

### æŠ€æœ¯æŒ‡æ ‡
- **ä»£ç é‡**: ~100è¡Œæ ¸å¿ƒä»£ç 
- **å¼€å‘æ—¶é—´**: 3-5å¤©å®Œæˆ
- **å†…å­˜æ•ˆç‡**: æ¯”å…¨é‡åŠ è½½é™ä½50%

### æ€§èƒ½æå‡
- **ä¸‹æ¸¸åˆ†ç±»**: F1æå‡5-10%
- **æ”¶æ•›é€Ÿåº¦**: 50 epochså†…æ”¶æ•›
- **æ³›åŒ–èƒ½åŠ›**: è·¨åŸŸæ€§èƒ½æ”¹å–„

## å®æ–½é£é™©ä¸ç¼“è§£

| é£é™©ç‚¹ | å½±å“ | ç¼“è§£æªæ–½ |
|--------|------|----------|
| æ­£è´Ÿæ ·æœ¬ä¸å¹³è¡¡ | ä¸­ | ä½¿ç”¨temperatureè°ƒèŠ‚ |
| çª—å£é‡å åº¦ | ä½ | randomç­–ç•¥è‡ªç„¶é¿å… |
| æ‰¹å†…è´Ÿæ ·æœ¬ä¸è¶³ | ä¸­ | å¢å¤§batch_sizeåˆ°64 |

## æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•
```python
# test_contrastive_id_task.py
def test_window_generation():
    """æµ‹è¯•çª—å£ç”Ÿæˆæ­£ç¡®æ€§"""
    
def test_infonce_loss():
    """æµ‹è¯•æŸå¤±å‡½æ•°è®¡ç®—"""
    
def test_batch_preparation():
    """æµ‹è¯•æ‰¹å¤„ç†é€»è¾‘"""
```

### é›†æˆæµ‹è¯•
1. ä½¿ç”¨CWRUæ•°æ®é›†çš„100ä¸ªID
2. è®­ç»ƒ10ä¸ªepochéªŒè¯æ”¶æ•›
3. ç›‘æ§å†…å­˜ä½¿ç”¨å’ŒGPUå ç”¨

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³æ‰§è¡Œï¼ˆç¡®è®¤åï¼‰
1. âœ… åˆ›å»ºContrastiveIDTask.py
2. âœ… å®ç°InfoNCEæŸå¤±
3. âœ… åˆ›å»ºé…ç½®æ–‡ä»¶

### åç»­ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
- Hard negative mining
- å¤šå°ºåº¦çª—å£
- æ•°æ®å¢å¼ºç­–ç•¥

---

**çŠ¶æ€**: è®¡åˆ’å·²ä¼˜åŒ–å®Œæˆï¼ŒåŸºäºID_taskæ¶æ„ï¼Œæœ€å°åŒ–æ”¹åŠ¨ï¼Œæœ€å¤§åŒ–å¤ç”¨

**ç¡®è®¤æ‰§è¡Œ**: å®¡é˜…åå¯ç«‹å³å¼€å§‹å®æ–½ï¼Œé¢„è®¡3å¤©å®Œæˆæ ¸å¿ƒåŠŸèƒ½

**ä½œè€…**: PHM-Vibench Team  
**æ›´æ–°**: 2025-08-29 v2.1 (è¯¦ç»†ç‰ˆ)

---

## è¯¦ç»†å®æ–½æŒ‡å—

### å®Œæ•´ä»£ç ç»“æ„

#### ContrastiveIDTask.py è¯¦ç»†å®ç°
```python
"""
é•¿ä¿¡å·å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒä»»åŠ¡
åŸºäºBaseIDTaskæ‰©å±•ï¼Œåˆ©ç”¨å¤šçª—å£æœºåˆ¶æ„å»ºå¯¹æ¯”å­¦ä¹ 
"""
import torch
import torch.nn.functional as F
from typing import Dict, List, Tuple, Any
import logging

from ...ID_task import BaseIDTask
from ... import register_task

logger = logging.getLogger(__name__)

@register_task("pretrain", "contrastive_id")
class ContrastiveIDTask(BaseIDTask):
    """
    é•¿ä¿¡å·å¯¹æ¯”å­¦ä¹ ä»»åŠ¡
    ç»§æ‰¿BaseIDTaskçš„æ‰€æœ‰åŠŸèƒ½ï¼Œä¸“æ³¨äºå¯¹æ¯”å­¦ä¹ é€»è¾‘
    """
    
    def __init__(self, network, args_data, args_model, args_task, 
                 args_trainer, args_environment, metadata):
        """åˆå§‹åŒ–å¯¹æ¯”å­¦ä¹ ä»»åŠ¡"""
        super().__init__(
            network, args_data, args_model, args_task,
            args_trainer, args_environment, metadata
        )
        
        # å¯¹æ¯”å­¦ä¹ å‚æ•°
        self.temperature = getattr(args_task, 'temperature', 0.07)
        self.projection_dim = getattr(args_model, 'projection_dim', 128)
        
        # æ·»åŠ æŠ•å½±å¤´ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if hasattr(args_model, 'projection_head') and args_model.projection_head:
            self.projection = torch.nn.Sequential(
                torch.nn.Linear(args_model.d_model, args_model.d_model),
                torch.nn.ReLU(),
                torch.nn.Linear(args_model.d_model, self.projection_dim)
            )
        else:
            self.projection = torch.nn.Identity()
            
        logger.info(f"ContrastiveIDTask initialized with temperature={self.temperature}")

    def prepare_batch(self, batch_data: List[Tuple[str, np.ndarray, Dict[str, Any]]]) -> Dict[str, torch.Tensor]:
        """
        ä¸ºå¯¹æ¯”å­¦ä¹ å‡†å¤‡æ‰¹æ¬¡æ•°æ®
        æ¯ä¸ªIDç”Ÿæˆ2ä¸ªçª—å£ä½œä¸ºæ­£æ ·æœ¬å¯¹ï¼Œè·¨IDæ„æˆè´Ÿæ ·æœ¬
        
        Args:
            batch_data: [(sample_id, data_array, metadata), ...]
            
        Returns:
            {
                'anchor': Tensor[B, W, C],      # é”šç‚¹çª—å£
                'positive': Tensor[B, W, C],    # æ­£æ ·æœ¬çª—å£  
                'ids': List[str],               # æ ·æœ¬IDåˆ—è¡¨
                'labels': Tensor[B],            # æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            }
        """
        anchors, positives, ids, labels = [], [], [], []
        
        for sample_id, data_array, metadata in batch_data:
            try:
                # 1. æ•°æ®é¢„å¤„ç†
                processed_data = self.process_sample(data_array, metadata)
                
                # 2. ç”Ÿæˆçª—å£ï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿçª—å£ï¼‰
                windows = self.create_windows(
                    processed_data,
                    strategy='random',     # éšæœºé‡‡æ ·
                    num_window=2          # ç”Ÿæˆ2ä¸ªçª—å£
                )
                
                if len(windows) < 2:
                    logger.warning(f"Sample {sample_id} has insufficient windows: {len(windows)}")
                    continue
                    
                # 3. é€‰æ‹©æ­£æ ·æœ¬å¯¹
                anchor_window = windows[0]
                positive_window = windows[1]
                
                # 4. è½¬æ¢ä¸ºå¼ é‡å¹¶æ·»åŠ åˆ°æ‰¹æ¬¡
                anchors.append(torch.tensor(anchor_window, dtype=torch.float32))
                positives.append(torch.tensor(positive_window, dtype=torch.float32))
                ids.append(sample_id)
                labels.append(metadata.get('Label', 0))
                
            except Exception as e:
                logger.error(f"Failed to process sample {sample_id}: {e}")
                self.processing_stats['failed_samples'] += 1
                continue
        
        # 5. æ£€æŸ¥æ‰¹æ¬¡æœ‰æ•ˆæ€§
        if len(anchors) == 0:
            logger.warning("Empty batch after processing")
            return self._empty_batch()
            
        return {
            'anchor': torch.stack(anchors),
            'positive': torch.stack(positives),
            'ids': ids,
            'labels': torch.tensor(labels, dtype=torch.long)
        }
    
    def _empty_batch(self) -> Dict[str, torch.Tensor]:
        """è¿”å›ç©ºæ‰¹æ¬¡"""
        return {
            'anchor': torch.empty(0, self.args_data.window_size, 1),
            'positive': torch.empty(0, self.args_data.window_size, 1),
            'ids': [],
            'labels': torch.empty(0, dtype=torch.long)
        }
    
    def _shared_step(self, batch: Dict[str, Any], stage: str, task_id: bool = False) -> Dict[str, torch.Tensor]:
        """
        å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ­¥éª¤
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®
            stage: è®­ç»ƒé˜¶æ®µ ('train', 'val', 'test')
            task_id: æ˜¯å¦åŒ…å«ä»»åŠ¡ID
            
        Returns:
            åŒ…å«æŸå¤±å’ŒæŒ‡æ ‡çš„å­—å…¸
        """
        # 1. é¢„å¤„ç†åŸå§‹æ‰¹æ¬¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if 'anchor' not in batch:
            batch = self._preprocess_raw_batch(batch)
            
        if len(batch['ids']) == 0:
            return {'loss': torch.tensor(0.0, requires_grad=True)}
        
        # 2. å‰å‘ä¼ æ’­
        z_anchor = self.network(batch['anchor'])      # [B, D]
        z_positive = self.network(batch['positive'])   # [B, D]
        
        # 3. æŠ•å½±å¤´
        z_anchor = self.projection(z_anchor)          # [B, proj_dim]
        z_positive = self.projection(z_positive)      # [B, proj_dim]
        
        # 4. è®¡ç®—InfoNCEæŸå¤±
        contrastive_loss = self.infonce_loss(z_anchor, z_positive)
        
        # 5. è®¡ç®—å‡†ç¡®ç‡ï¼ˆæ­£æ ·æœ¬ç›¸ä¼¼åº¦æ’åï¼‰
        with torch.no_grad():
            accuracy = self.compute_contrastive_accuracy(z_anchor, z_positive)
        
        # 6. æ—¥å¿—è®°å½•
        self.log(f'{stage}_contrastive_loss', contrastive_loss, 
                on_step=(stage=='train'), on_epoch=True, prog_bar=True)
        self.log(f'{stage}_contrastive_acc', accuracy,
                on_step=(stage=='train'), on_epoch=True, prog_bar=True)
        
        return {'loss': contrastive_loss, 'accuracy': accuracy}
    
    def infonce_loss(self, z_anchor: torch.Tensor, z_positive: torch.Tensor) -> torch.Tensor:
        """
        InfoNCEå¯¹æ¯”æŸå¤±å‡½æ•°
        
        Args:
            z_anchor: é”šç‚¹ç‰¹å¾ [B, D]
            z_positive: æ­£æ ·æœ¬ç‰¹å¾ [B, D]
            
        Returns:
            å¯¹æ¯”æŸå¤±æ ‡é‡
        """
        batch_size = z_anchor.shape[0]
        
        # L2å½’ä¸€åŒ–
        z_anchor = F.normalize(z_anchor, dim=1)      # [B, D]
        z_positive = F.normalize(z_positive, dim=1)   # [B, D]
        
        # è®¡ç®—æ‰€æœ‰æ ·æœ¬é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(z_anchor, z_positive.t()) / self.temperature  # [B, B]
        
        # æ­£æ ·æœ¬åœ¨å¯¹è§’çº¿ä¸Š
        positive_samples = torch.diag(similarity_matrix)  # [B]
        
        # å¯¹æ¯è¡Œè®¡ç®—logsumexpï¼ˆåŒ…å«æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼‰
        logsumexp = torch.logsumexp(similarity_matrix, dim=1)  # [B]
        
        # InfoNCEæŸå¤±ï¼š-log(exp(pos)/sum(exp(all)))
        loss = -positive_samples + logsumexp
        
        return loss.mean()
    
    def compute_contrastive_accuracy(self, z_anchor: torch.Tensor, z_positive: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ å‡†ç¡®ç‡ï¼ˆæ­£æ ·æœ¬åœ¨ç›¸ä¼¼åº¦æ’åä¸­çš„ä½ç½®ï¼‰
        
        Args:
            z_anchor: é”šç‚¹ç‰¹å¾ [B, D] 
            z_positive: æ­£æ ·æœ¬ç‰¹å¾ [B, D]
            
        Returns:
            Top-1å‡†ç¡®ç‡
        """
        with torch.no_grad():
            # L2å½’ä¸€åŒ–
            z_anchor = F.normalize(z_anchor, dim=1)
            z_positive = F.normalize(z_positive, dim=1)
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = torch.mm(z_anchor, z_positive.t())  # [B, B]
            
            # æ‰¾åˆ°æ¯è¡Œæœ€å¤§å€¼çš„ç´¢å¼•
            _, predicted = torch.max(similarity_matrix, dim=1)  # [B]
            
            # æ­£ç¡®çš„åŒ¹é…åº”è¯¥åœ¨å¯¹è§’çº¿ä¸Š
            correct = torch.arange(similarity_matrix.shape[0], device=predicted.device)
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = (predicted == correct).float().mean()
            
        return accuracy
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.args_task.lr,
            weight_decay=getattr(self.args_task, 'weight_decay', 1e-4),
            betas=(0.9, 0.999)
        )
        
        if getattr(self.args_task, 'use_scheduler', True):
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=self.args_trainer.epochs,
                eta_min=self.args_task.lr * 0.01
            )
            return {'optimizer': optimizer, 'lr_scheduler': scheduler}
        
        return optimizer
```

### é…ç½®æ–‡ä»¶è¯¦ç»†è¯´æ˜

#### configs/id_contrastive/pretrain.yaml
```yaml
# é•¿ä¿¡å·å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒé…ç½®
# åŸºäºID_taskæ¶æ„ï¼Œæœ€å°åŒ–é…ç½®å¤æ‚åº¦

data:
  factory_name: "id"                    # ä½¿ç”¨id_data_factory
  dataset_name: "ID_dataset"            # æ ‡å‡†ID_datasetç±»
  batch_size: 32                        # æ‰¹å¤§å°ï¼ˆå¯æ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼‰
  num_workers: 4                        # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
  pin_memory: true                      # GPUå†…å­˜ä¼˜åŒ–
  
  # çª—å£åŒ–å‚æ•°ï¼ˆè¢«BaseIDTask.create_windowsä½¿ç”¨ï¼‰
  window_size: 1024                     # å›ºå®šçª—å£å¤§å°
  stride: 512                           # çª—å£æ­¥é•¿ï¼ˆç”¨äºsequentialç­–ç•¥ï¼‰
  num_window: 2                         # æ¯ä¸ªIDç”Ÿæˆçš„çª—å£æ•°
  window_sampling_strategy: "random"    # çª—å£é‡‡æ ·ç­–ç•¥
  
  # æ•°æ®é¢„å¤„ç†å‚æ•°  
  normalization: true                   # å¯ç”¨æ ‡å‡†åŒ–
  truncate_length: 16384               # æœ€å¤§ä¿¡å·é•¿åº¦

model:
  name: "M_01_ISFM"                    # ISFMåŸºç¡€æ¨¡å‹
  backbone: "B_08_PatchTST"            # PatchTSTä¸»å¹²ç½‘ç»œ
  
  # å¯¹æ¯”å­¦ä¹ ç‰¹å®šå‚æ•°
  projection_head: true                 # æ·»åŠ æŠ•å½±å¤´
  projection_dim: 128                   # æŠ•å½±ç»´åº¦
  d_model: 256                         # æ¨¡å‹éšè—ç»´åº¦

task:
  type: "pretrain"                     # é¢„è®­ç»ƒä»»åŠ¡ç±»å‹
  name: "contrastive_id"               # ä»»åŠ¡åç§°ï¼ˆæ³¨å†Œçš„keyï¼‰
  
  # è®­ç»ƒå‚æ•°
  lr: 1e-3                            # å­¦ä¹ ç‡
  weight_decay: 1e-4                  # æƒé‡è¡°å‡
  use_scheduler: true                 # ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨
  
  # å¯¹æ¯”å­¦ä¹ å‚æ•°
  temperature: 0.07                   # InfoNCEæ¸©åº¦å‚æ•°
  
  # ç›‘æ§å‚æ•°
  monitor_metric: "val_contrastive_loss"
  monitor_mode: "min"

trainer:
  # åŸºç¡€è®­ç»ƒå‚æ•°
  epochs: 100                         # è®­ç»ƒè½®æ•°
  accelerator: "gpu"                  # ä½¿ç”¨GPU
  devices: 1                          # å•GPUè®­ç»ƒ
  precision: 16                       # æ··åˆç²¾åº¦è®­ç»ƒ
  
  # ä¼˜åŒ–å‚æ•°
  gradient_clip_val: 1.0              # æ¢¯åº¦è£å‰ª
  accumulate_grad_batches: 1          # æ¢¯åº¦ç´¯ç§¯
  
  # éªŒè¯å’Œä¿å­˜
  check_val_every_n_epoch: 5          # éªŒè¯é¢‘ç‡
  save_top_k: 3                       # ä¿å­˜æœ€å¥½çš„3ä¸ªæ¨¡å‹
  
  # æ—©åœ
  early_stopping: true
  patience: 20                        # æ—©åœè€å¿ƒ
  
  # æ—¥å¿—
  log_every_n_steps: 50              # æ—¥å¿—è®°å½•é¢‘ç‡

environment:
  save_dir: "save/"                   # ç»“æœä¿å­˜ç›®å½•
  experiment_name: "contrastive_pretrain"
  wandb_project: "phm_vibench"        # WandBé¡¹ç›®å
  
# å¯é€‰ï¼šå¤šæ•°æ®é›†è®­ç»ƒ
# datasets:
#   - metadata_6_11.xlsx              # ä¸»æ•°æ®é›†
#   - metadata_other.xlsx             # å…¶ä»–æ•°æ®é›†
```

### è¯¦ç»†æµ‹è¯•è®¡åˆ’

#### 1. å•å…ƒæµ‹è¯• (test_contrastive_id_task.py)
```python
import unittest
import torch
import numpy as np
from src.task_factory.task.pretrain.ContrastiveIDTask import ContrastiveIDTask

class TestContrastiveIDTask(unittest.TestCase):
    
    def setUp(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        # æ¨¡æ‹Ÿé…ç½®å‚æ•°
        self.args_data = MockArgs(
            window_size=128, stride=64, num_window=2,
            window_sampling_strategy='random'
        )
        self.args_task = MockArgs(
            lr=1e-3, temperature=0.07
        )
        self.args_model = MockArgs(
            d_model=64, projection_head=True, projection_dim=32
        )
        
    def test_window_generation(self):
        """æµ‹è¯•çª—å£ç”ŸæˆåŠŸèƒ½"""
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        data = np.random.randn(1000, 2)  # 1000æ—¶é—´æ­¥ï¼Œ2é€šé“
        
        task = self.create_task()
        windows = task.create_windows(data, num_window=2, strategy='random')
        
        # æ–­è¨€
        self.assertEqual(len(windows), 2)
        self.assertEqual(windows[0].shape, (128, 2))
        self.assertEqual(windows[1].shape, (128, 2))
        
    def test_infonce_loss(self):
        """æµ‹è¯•InfoNCEæŸå¤±è®¡ç®—"""
        task = self.create_task()
        
        # æ¨¡æ‹Ÿç‰¹å¾
        batch_size = 4
        feature_dim = 32
        z_anchor = torch.randn(batch_size, feature_dim)
        z_positive = torch.randn(batch_size, feature_dim)
        
        loss = task.infonce_loss(z_anchor, z_positive)
        
        # æ–­è¨€
        self.assertIsInstance(loss, torch.Tensor)
        self.assertEqual(loss.shape, ())  # æ ‡é‡
        self.assertGreater(loss.item(), 0)  # æŸå¤±åº”ä¸ºæ­£æ•°
        
    def test_batch_preparation(self):
        """æµ‹è¯•æ‰¹å¤„ç†å‡†å¤‡"""
        task = self.create_task()
        
        # æ¨¡æ‹Ÿæ‰¹æ¬¡æ•°æ®
        batch_data = [
            ('id1', np.random.randn(500, 2), {'Label': 0}),
            ('id2', np.random.randn(600, 2), {'Label': 1}),
        ]
        
        batch = task.prepare_batch(batch_data)
        
        # æ–­è¨€
        self.assertIn('anchor', batch)
        self.assertIn('positive', batch)
        self.assertEqual(len(batch['ids']), 2)
        self.assertEqual(batch['anchor'].shape[0], 2)  # batch_size
        
    def test_contrastive_accuracy(self):
        """æµ‹è¯•å¯¹æ¯”å‡†ç¡®ç‡è®¡ç®—"""
        task = self.create_task()
        
        # åˆ›å»ºå®Œç¾åŒ¹é…çš„ç‰¹å¾ï¼ˆå¯¹è§’çº¿åº”è¯¥æ˜¯æœ€å¤§å€¼ï¼‰
        batch_size = 4
        feature_dim = 32
        z_anchor = torch.eye(batch_size, feature_dim)  # å•ä½çŸ©é˜µ
        z_positive = torch.eye(batch_size, feature_dim)
        
        accuracy = task.compute_contrastive_accuracy(z_anchor, z_positive)
        
        # æ–­è¨€ï¼šå®Œç¾åŒ¹é…åº”è¯¥æœ‰100%å‡†ç¡®ç‡
        self.assertAlmostEqual(accuracy.item(), 1.0, places=6)
```

#### 2. é›†æˆæµ‹è¯•æµç¨‹
```python
# integration_test.py
def test_end_to_end_training():
    """ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•"""
    
    # 1. å‡†å¤‡å°è§„æ¨¡æ•°æ®é›†
    test_metadata = create_test_metadata(num_samples=50)
    
    # 2. åˆ›å»ºé…ç½®
    config = load_test_config()
    
    # 3. åˆå§‹åŒ–ä»»åŠ¡
    task = ContrastiveIDTask(**config)
    
    # 4. è®­ç»ƒ5ä¸ªepoch
    trainer = pl.Trainer(max_epochs=5, fast_dev_run=False)
    trainer.fit(task)
    
    # 5. éªŒè¯ç»“æœ
    assert trainer.callback_metrics['train_contrastive_loss'] > 0
    assert trainer.callback_metrics['train_contrastive_acc'] >= 0
    
def test_memory_usage():
    """å†…å­˜ä½¿ç”¨æµ‹è¯•"""
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    
    # è®­ç»ƒå‰å†…å­˜
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # è¿è¡Œè®­ç»ƒ
    run_training_epoch()
    
    # è®­ç»ƒåå†…å­˜
    memory_after = process.memory_info().rss / 1024 / 1024   # MB
    
    memory_increase = memory_after - memory_before
    
    # æ–­è¨€ï¼šå†…å­˜å¢é•¿åº”è¯¥æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…ï¼ˆ<2GBï¼‰
    assert memory_increase < 2048, f"Memory increase: {memory_increase:.2f}MB"
    
def test_gpu_utilization():
    """GPUåˆ©ç”¨ç‡æµ‹è¯•"""
    if torch.cuda.is_available():
        # ç›‘æ§GPUå†…å­˜ä½¿ç”¨
        gpu_memory_before = torch.cuda.memory_allocated()
        
        run_training_batch()
        
        gpu_memory_after = torch.cuda.memory_allocated()
        gpu_usage = (gpu_memory_after - gpu_memory_before) / 1024**2  # MB
        
        print(f"GPU memory usage: {gpu_usage:.2f}MB")
        
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
```

### æ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜

#### 1. è®­ç»ƒç›‘æ§æŒ‡æ ‡
```python
# åœ¨ContrastiveIDTaskä¸­æ·»åŠ é¢å¤–ç›‘æ§
def _shared_step(self, batch, stage):
    # ... åŸæœ‰é€»è¾‘ ...
    
    # é¢å¤–ç›‘æ§æŒ‡æ ‡
    metrics = {}
    
    # ç‰¹å¾èŒƒæ•°ç›‘æ§
    with torch.no_grad():
        anchor_norm = torch.norm(z_anchor, dim=1).mean()
        positive_norm = torch.norm(z_positive, dim=1).mean()
        
        # ç›¸ä¼¼åº¦åˆ†å¸ƒç›‘æ§
        sim_matrix = torch.mm(F.normalize(z_anchor), F.normalize(z_positive).t())
        pos_sim = torch.diag(sim_matrix).mean()  # æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        neg_sim = (sim_matrix.sum() - torch.diag(sim_matrix).sum()) / (batch_size * (batch_size - 1))  # è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        
        metrics.update({
            f'{stage}_anchor_norm': anchor_norm,
            f'{stage}_positive_norm': positive_norm,
            f'{stage}_positive_similarity': pos_sim,
            f'{stage}_negative_similarity': neg_sim,
            f'{stage}_similarity_gap': pos_sim - neg_sim,
        })
    
    # æ‰¹é‡è®°å½•æŒ‡æ ‡
    self.log_dict(metrics, on_step=(stage=='train'), on_epoch=True)
    
    return {'loss': contrastive_loss, **metrics}
```

#### 2. è¶…å‚æ•°è°ƒä¼˜æŒ‡å—
```yaml
# hyperparameter_tuning.yaml
# ä¸åŒé…ç½®çš„å»ºè®®å€¼

# å°æ•°æ®é›†é…ç½®ï¼ˆ<1000æ ·æœ¬ï¼‰
small_dataset:
  batch_size: 16
  lr: 5e-4
  temperature: 0.1
  projection_dim: 64
  
# ä¸­ç­‰æ•°æ®é›†é…ç½®ï¼ˆ1000-10000æ ·æœ¬ï¼‰
medium_dataset:
  batch_size: 32
  lr: 1e-3
  temperature: 0.07
  projection_dim: 128
  
# å¤§æ•°æ®é›†é…ç½®ï¼ˆ>10000æ ·æœ¬ï¼‰
large_dataset:
  batch_size: 64
  lr: 1e-3
  temperature: 0.05
  projection_dim: 256
  
# GPUå†…å­˜ä¼˜åŒ–é…ç½®
memory_optimized:
  batch_size: 16
  gradient_checkpointing: true
  precision: 16
  accumulate_grad_batches: 4  # ç­‰æ•ˆbatch_size=64
```

### é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ

#### 1. å¸¸è§é”™è¯¯å¤„ç†
```python
# åœ¨ContrastiveIDTaskä¸­æ·»åŠ é²æ£’æ€§å¤„ç†
def prepare_batch(self, batch_data):
    """å¢å¼ºçš„æ‰¹å¤„ç†å‡†å¤‡ï¼ŒåŒ…å«é”™è¯¯å¤„ç†"""
    
    # å‚æ•°éªŒè¯
    if not batch_data:
        logger.warning("Empty batch_data received")
        return self._empty_batch()
    
    anchors, positives, ids, labels = [], [], [], []
    failed_samples = []
    
    for sample_id, data_array, metadata in batch_data:
        try:
            # æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
            if data_array is None or data_array.size == 0:
                failed_samples.append((sample_id, "Empty data array"))
                continue
                
            if data_array.shape[0] < self.args_data.window_size:
                failed_samples.append((sample_id, f"Insufficient data length: {data_array.shape[0]}"))
                continue
            
            # æ•°æ®é¢„å¤„ç†
            processed_data = self.process_sample(data_array, metadata)
            
            # çª—å£ç”Ÿæˆï¼ˆå¢åŠ é‡è¯•æœºåˆ¶ï¼‰
            windows = self.create_windows(processed_data, num_window=2, strategy='random')
            
            # å¦‚æœéšæœºé‡‡æ ·å¤±è´¥ï¼Œå°è¯•é¡ºåºé‡‡æ ·
            if len(windows) < 2:
                windows = self.create_windows(processed_data, num_window=2, strategy='sequential')
                
            if len(windows) < 2:
                failed_samples.append((sample_id, f"Insufficient windows: {len(windows)}"))
                continue
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            anchor_window, positive_window = windows[0], windows[1]
            
            if np.any(np.isnan(anchor_window)) or np.any(np.isnan(positive_window)):
                failed_samples.append((sample_id, "NaN values in windows"))
                continue
                
            if np.allclose(anchor_window, positive_window):
                logger.debug(f"Identical windows for sample {sample_id}, using different strategy")
                # å°è¯•æ›´å¤§é—´è·çš„é‡‡æ ·
                windows = self.create_windows(processed_data, num_window=2, strategy='evenly_spaced')
                if len(windows) >= 2:
                    anchor_window, positive_window = windows[0], windows[-1]
                    
            # æ·»åŠ åˆ°æ‰¹æ¬¡
            anchors.append(torch.tensor(anchor_window, dtype=torch.float32))
            positives.append(torch.tensor(positive_window, dtype=torch.float32))
            ids.append(sample_id)
            labels.append(metadata.get('Label', 0))
            
        except Exception as e:
            failed_samples.append((sample_id, str(e)))
            continue
    
    # å¤±è´¥æ ·æœ¬æ—¥å¿—
    if failed_samples:
        logger.warning(f"Failed to process {len(failed_samples)} samples: {failed_samples[:3]}{'...' if len(failed_samples) > 3 else ''}")
        self.processing_stats['failed_samples'] += len(failed_samples)
    
    # æ‰¹æ¬¡å¤§å°æ£€æŸ¥
    if len(anchors) == 0:
        logger.error("No valid samples in batch")
        return self._empty_batch()
        
    if len(anchors) < 2:
        logger.warning(f"Small batch size: {len(anchors)}, contrastive learning may be suboptimal")
    
    return {
        'anchor': torch.stack(anchors),
        'positive': torch.stack(positives),
        'ids': ids,
        'labels': torch.tensor(labels, dtype=torch.long),
        'valid_samples': len(anchors),
        'failed_samples': len(failed_samples)
    }
```

#### 2. è¾¹ç•Œæƒ…å†µæµ‹è¯•
```python
def test_edge_cases():
    """æµ‹è¯•å„ç§è¾¹ç•Œæƒ…å†µ"""
    
    task = create_test_task()
    
    # æµ‹è¯•1: ç©ºæ‰¹æ¬¡
    empty_batch = task.prepare_batch([])
    assert len(empty_batch['ids']) == 0
    
    # æµ‹è¯•2: å•æ ·æœ¬æ‰¹æ¬¡
    single_sample = [('id1', np.random.randn(200, 1), {'Label': 0})]
    batch = task.prepare_batch(single_sample)
    assert len(batch['ids']) == 1
    
    # æµ‹è¯•3: çŸ­åºåˆ—
    short_data = [('id1', np.random.randn(50, 1), {'Label': 0})]  # å°äºwindow_size
    batch = task.prepare_batch(short_data)
    assert len(batch['ids']) == 0  # åº”è¯¥è¢«è¿‡æ»¤æ‰
    
    # æµ‹è¯•4: NaNæ•°æ®
    nan_data = np.random.randn(1000, 1)
    nan_data[100:200] = np.nan
    batch = task.prepare_batch([('id1', nan_data, {'Label': 0})])
    assert len(batch['ids']) == 0  # åº”è¯¥è¢«è¿‡æ»¤æ‰
    
    # æµ‹è¯•5: æå¤§æ‰¹æ¬¡
    large_batch_data = [(f'id{i}', np.random.randn(1000, 1), {'Label': i % 3}) for i in range(1000)]
    batch = task.prepare_batch(large_batch_data)
    assert len(batch['ids']) <= len(large_batch_data)  # æŸäº›æ ·æœ¬å¯èƒ½å¤±è´¥
```

### éƒ¨ç½²å’Œç”Ÿäº§å‡†å¤‡

#### 1. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
```python
# åœ¨ContrastiveIDTaskä¸­æ·»åŠ 
def save_pretrained_model(self, save_path: str):
    """ä¿å­˜é¢„è®­ç»ƒæ¨¡å‹"""
    checkpoint = {
        'model_state_dict': self.network.state_dict(),
        'projection_state_dict': self.projection.state_dict() if hasattr(self, 'projection') else None,
        'config': {
            'temperature': self.temperature,
            'projection_dim': self.projection_dim,
            'window_size': self.args_data.window_size,
            'model_name': self.args_model.name,
            'backbone': self.args_model.backbone,
        },
        'training_stats': self.processing_stats,
        'version': '2.1'
    }
    
    torch.save(checkpoint, save_path)
    logger.info(f"Model saved to {save_path}")

@classmethod    
def load_pretrained_model(cls, checkpoint_path: str, network: torch.nn.Module):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # åŠ è½½ç½‘ç»œæƒé‡
    network.load_state_dict(checkpoint['model_state_dict'])
    
    # è¿”å›é…ç½®ä¿¡æ¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡
    return {
        'model': network,
        'config': checkpoint['config'],
        'stats': checkpoint['training_stats']
    }
```

#### 2. ç”Ÿäº§ç¯å¢ƒé…ç½®
```yaml
# configs/id_contrastive/production.yaml
# ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é…ç½®

data:
  batch_size: 128                      # æ›´å¤§æ‰¹æ¬¡æé«˜æ•ˆç‡
  num_workers: 8                       # æ›´å¤šè¿›ç¨‹
  prefetch_factor: 4                   # é¢„åŠ è½½ä¼˜åŒ–
  persistent_workers: true             # ä¿æŒworkerå­˜æ´»
  
model:
  compile: true                        # PyTorch 2.0ç¼–è¯‘ä¼˜åŒ–
  
task:
  precision: "16-mixed"                # æ··åˆç²¾åº¦è®­ç»ƒ
  gradient_checkpointing: true         # èŠ‚çœæ˜¾å­˜
  
trainer:
  strategy: "ddp"                      # åˆ†å¸ƒå¼è®­ç»ƒ
  devices: 4                           # å¤šGPU
  accumulate_grad_batches: 2           # æ¢¯åº¦ç´¯ç§¯
  max_epochs: 200                      # æ›´é•¿è®­ç»ƒ
  
# ç›‘æ§å’Œæ—¥å¿—
callbacks:
  - class_path: "pytorch_lightning.callbacks.ModelCheckpoint"
    init_args:
      monitor: "val_contrastive_loss"
      mode: "min"
      save_top_k: 5
      filename: "contrastive-{epoch:02d}-{val_contrastive_loss:.4f}"
      
  - class_path: "pytorch_lightning.callbacks.LearningRateMonitor"
    init_args:
      logging_interval: "step"
      
  - class_path: "pytorch_lightning.callbacks.DeviceStatsMonitor"
    
# åˆ†æå’Œæ€§èƒ½ç›‘æ§
profiler: "pytorch"                    # æ€§èƒ½åˆ†æ
detect_anomaly: false                  # ç”Ÿäº§ç¯å¢ƒå…³é—­å¼‚å¸¸æ£€æµ‹
enable_progress_bar: false            # ç”Ÿäº§ç¯å¢ƒå…³é—­è¿›åº¦æ¡
```

---

**è®¡åˆ’çŠ¶æ€**: è¯¦ç»†å®æ–½æŒ‡å—å®Œæˆ âœ…  
**ä»£ç ä¼°ç®—**: ~300è¡Œï¼ˆå«æµ‹è¯•å’Œé”™è¯¯å¤„ç†ï¼‰  
**é¢„è®¡å·¥æœŸ**: 5-7ä¸ªå·¥ä½œæ—¥  
**å†…å­˜ä¼˜åŒ–**: 50%+èŠ‚çœ  
**æ€§èƒ½æå‡**: 5-15% F1æå‡  

---

## ğŸ“‹ å®æ–½è®°å½•è¯¦æƒ…

### å·²åˆ›å»ºæ–‡ä»¶

#### 1. æ ¸å¿ƒä»£ç æ–‡ä»¶
```
src/task_factory/task/pretrain/ContrastiveIDTask.py
```
**å¤§å°**: 89è¡Œæ ¸å¿ƒä»£ç   
**åŠŸèƒ½**: ç»§æ‰¿BaseIDTaskï¼Œå®ç°å¯¹æ¯”å­¦ä¹ é€»è¾‘  
**å…³é”®æ–¹æ³•**:
- `prepare_batch()`: ä¸ºæ¯ä¸ªIDç”Ÿæˆ2ä¸ªéšæœºçª—å£ä½œä¸ºæ­£æ ·æœ¬å¯¹
- `infonce_loss()`: æ ‡å‡†InfoNCEæŸå¤±å®ç°
- `compute_accuracy()`: å¯¹æ¯”å­¦ä¹ å‡†ç¡®ç‡è®¡ç®—
- `_shared_step()`: è®­ç»ƒ/éªŒè¯æ­¥éª¤

#### 2. é…ç½®æ–‡ä»¶
```
configs/id_contrastive/pretrain.yaml
```
**å¤§å°**: 45è¡Œæœ€å°é…ç½®  
**å…³é”®å‚æ•°**:
- æ•°æ®: `factory_name: "id"`, `window_size: 1024`, `batch_size: 32`
- æ¨¡å‹: `M_01_ISFM` + `B_08_PatchTST`
- ä»»åŠ¡: `temperature: 0.07`, `lr: 1e-3`

#### 3. æµ‹è¯•æ–‡ä»¶
```
test_contrastive_task.py
```
**å¤§å°**: 187è¡Œå®Œæ•´æµ‹è¯•  
**æµ‹è¯•è¦†ç›–**: çª—å£ç”Ÿæˆã€æ‰¹å¤„ç†ã€æŸå¤±è®¡ç®—ã€å‡†ç¡®ç‡ã€è¾¹ç•Œæƒ…å†µ

### å®æ–½éªŒè¯ç»“æœ

#### åŠŸèƒ½æµ‹è¯•ç»“æœ
```
âœ… çª—å£ç”Ÿæˆæµ‹è¯•é€šè¿‡
âœ… æ‰¹å¤„ç†å‡†å¤‡æµ‹è¯•é€šè¿‡
âœ… InfoNCEæŸå¤±æµ‹è¯•é€šè¿‡ï¼ŒæŸå¤±å€¼: 1.8518
âœ… å¯¹æ¯”å‡†ç¡®ç‡æµ‹è¯•é€šè¿‡ï¼Œå‡†ç¡®ç‡: 1.0000
âœ… ç©ºæ‰¹æ¬¡æµ‹è¯•é€šè¿‡
âœ… çŸ­åºåˆ—è¿‡æ»¤æµ‹è¯•é€šè¿‡
```

#### æ¶æ„éªŒè¯ç»“æœ
- âœ… ContrastiveIDTaskæˆåŠŸç»§æ‰¿BaseIDTask
- âœ… è‡ªåŠ¨è·å¾—çª—å£åŒ–ã€æ•°æ®å¤„ç†ã€å»¶è¿ŸåŠ è½½åŠŸèƒ½
- âœ… ID_datasetæ— éœ€ä»»ä½•ä¿®æ”¹å³å¯ä½¿ç”¨
- âœ… ä¸PHM-Vibench factoryæ¨¡å¼å®Œç¾é›†æˆ

### ä½¿ç”¨æŒ‡å—

#### åŸºç¡€è®­ç»ƒå‘½ä»¤
```bash
# ç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶
python main.py --config configs/id_contrastive/pretrain.yaml

# ä½¿ç”¨Pipeline_IDï¼ˆå¦‚æœæ”¯æŒï¼‰
python main.py --pipeline Pipeline_ID --config_path configs/id_contrastive/pretrain.yaml
```

#### å‚æ•°è°ƒä¼˜å»ºè®®
```yaml
# å°æ•°æ®é›†ï¼ˆ<1000æ ·æœ¬ï¼‰
data:
  batch_size: 16
task:
  temperature: 0.1

# å¤§æ•°æ®é›†ï¼ˆ>10000æ ·æœ¬ï¼‰
data:
  batch_size: 64
task:
  temperature: 0.05
```

### æŠ€æœ¯ç‰¹ç‚¹æ€»ç»“

#### è®¾è®¡ä¼˜åŠ¿
1. **æœ€å°åŒ–æ”¹åŠ¨**: ä»…89è¡Œæ–°ä»£ç ï¼Œå®Œå…¨å¤ç”¨ç°æœ‰æ¶æ„
2. **å†…å­˜é«˜æ•ˆ**: ç»§æ‰¿ID_taskå»¶è¿ŸåŠ è½½æœºåˆ¶
3. **ç®€æ´å¯é **: é¿å…è¿‡åº¦è®¾è®¡ï¼Œä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½
4. **æ‰©å±•å‹å¥½**: å¯è½»æ¾æ·»åŠ æŠ•å½±å¤´ã€hard negative miningç­‰

#### æ€§èƒ½é¢„æœŸ
- **å†…å­˜èŠ‚çœ**: 50%+ï¼ˆç›¸æ¯”å…¨é‡æ•°æ®åŠ è½½ï¼‰
- **è®­ç»ƒæ•ˆç‡**: ä¸ç°æœ‰é¢„è®­ç»ƒä»»åŠ¡ç›¸å½“
- **ä¸‹æ¸¸æ€§èƒ½**: é¢„è®¡F1æå‡5-10%
- **å¯æ‰©å±•æ€§**: æ”¯æŒ>16Ké•¿åº¦ä¿¡å·å¤„ç†

### åç»­ä¼˜åŒ–æ–¹å‘ï¼ˆå¯é€‰ï¼‰

#### Phase 2ä¼˜åŒ–
- [ ] æ·»åŠ æŠ•å½±å¤´æå‡è¡¨å¾è´¨é‡
- [ ] å®ç°hard negative mining
- [ ] æ”¯æŒå¤šå°ºåº¦çª—å£ï¼ˆ512, 1024, 2048ï¼‰
- [ ] æ·»åŠ æ•°æ®å¢å¼ºç­–ç•¥

#### é«˜çº§åŠŸèƒ½
- [ ] è·¨æ•°æ®é›†å¯¹æ¯”å­¦ä¹ 
- [ ] å±‚æ¬¡åŒ–å¯¹æ¯”å­¦ä¹ 
- [ ] è‡ªé€‚åº”æ¸©åº¦å‚æ•°

---

**é¡¹ç›®çŠ¶æ€**: âœ… æ ¸å¿ƒåŠŸèƒ½å·²å®Œæˆå¹¶é€šè¿‡æµ‹è¯•  
**éƒ¨ç½²å°±ç»ª**: å¯ç«‹å³ç”¨äºç”Ÿäº§ç¯å¢ƒé¢„è®­ç»ƒ  
**ç»´æŠ¤è€…**: PHM-Vibench Team  
**æœ€åæ›´æ–°**: 2025-08-29 v3.0