# Multi-Task PHM Foundation Model Configuration - B_08_PatchTST
# Production Mode: 200 epochs, wandb online, 2 iterations
# Author: PHM-Vibench Team
# Date: 2025-08-29

environment:
  VBENCH_HOME: "/home/lq/LQcode/2_project/PHMBench/Vbench"
  PYTHONPATH: "/home/lq/.conda/envs/lq"
  PHMBench: "/home/lq/LQcode/2_project/PHMBench"
  project: "MultiTask_Foundation_Model"
  seed: 42
  output_dir: "results/multitask_B_08_PatchTST"
  iterations: 2  # Production mode: 2 iterations for stability
  wandb: true  # Production mode: W&B logging enabled
  swanlab: False  # Test mode: disabled
  notes: 'Multi-task foundation model with B_08_PatchTST - PRODUCTION MODE'
  workspace: 'PHMbench'

# Data Configuration
data:
  data_dir: "/gpfs/gibbs/project/lu_lu/ql334/PHM-Vibench/data"
  metadata_file: "metadata_6_11.xlsx"
  batch_size: 512     # Single GPU optimized: A100 can handle large batches
  num_workers: 16    # Reduced workers
  window_size: 4096
  stride: 4
  truncate_length: 200
  num_window: 128    # Reduced from 512 to 128
  normalization: 'standardization'
  dtype: float32
  window_sampling_strategy: 'evenly_spaced'
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  
  # RUL-specific dataset configuration
  rul_dataset_id: 2  # XJTU bearing dataset for RUL

# Model Configuration
model:
  name: "M_01_ISFM"
  type: "ISFM"
  input_dim: 2
  
  # Architecture components
  embedding: E_01_HSE
  backbone: B_08_PatchTST  # PatchTST backbone
  task_head: MultiTaskHead  # Enhanced multi-task head with signal prediction
  
  # Model dimensions - MEMORY OPTIMIZED
  num_patches: 64    # Reduced from 128 to 64
  patch_size_L: 128  # Reduced from 256 to 128
  patch_size_C: 1
  output_dim: 256    # Reduced from 1024 to 256
  num_heads: 4       # Reduced from 8 to 4
  num_layers: 2      # Reduced from 3 to 2
  d_ff: 512         # Reduced from 2048 to 512
  dropout: 0.1
  
  # PatchTST specific parameters
  e_layers: 2        # Number of encoder layers (maps to num_layers)
  factor: 1          # Attention factor for efficiency
  activation: "gelu" # Activation function
  
  # PatchTST-specific parameters
  patch_len: 16
  stride: 8
  d_model: 128
  
  # Multi-task head configuration
  classification_head: H_02_distance_cla
  prediction_head: H_03_Linear_pred
  hidden_dim: 64     # MEMORY FIX: Reduced from 512 to 64 (99.9% param reduction in H_03_Linear_pred)
  activation: "gelu"
  use_batch_norm: true
  
  # H_03_Linear_pred parameters for signal prediction - MEMORY OPTIMIZED
  max_len: 4096      # USER REQUIREMENT: Keep max_len = 4096 
  max_out: 2         # MEMORY FIX: Reduced from 3 to 2
  act: "gelu"

# Task Configuration
task:
  name: "multi_task_phm"
  type: "In_distribution"
  
  # Enabled tasks
  enabled_tasks: 
    - 'classification'       # Fault diagnosis
    - 'anomaly_detection'   # Anomaly detection  
    - 'signal_prediction'   # Signal prediction
    - 'rul_prediction'      # RUL prediction
  
  # Task weights for loss balancing
  task_weights:
    classification: 1.0
    anomaly_detection: 0.6
    signal_prediction: 0.7
    rul_prediction: 0.8
  
  # Task-specific configurations
  classification:
    loss: "CE"  # Cross-entropy loss
    num_classes: auto  # Auto-determined from dataset
    label_smoothing: 0.1
    loss_weight: 1.0
  
  anomaly_detection:
    loss: "BCE"  # Binary cross-entropy loss
    threshold: 0.5
    class_weights: [1.0, 2.0]  # [normal, anomaly]
    loss_weight: 0.6
  
  signal_prediction:
    loss: "MSE"  # Mean squared error loss
    pred_len: 96  # Predict 96 timesteps
    use_mean_pooling: false  # Key: don't use mean pooling
    loss_weight: 0.7
  
  rul_prediction:
    loss: "MSE"  # Mean squared error loss
    max_rul_value: 1.0
    normalize_targets: true
    dataset_id: 2  # XJTU dataset
    loss_weight: 0.8
  
  # Target systems for evaluation
  target_system_id: [1, 2, 5, 6, 13, 19]
  
  # Optimization parameters
  optimizer: "adamw"
  lr: 0.0005
  weight_decay: 0.01
  momentum: 0.9  # For SGD if needed
  
  # Learning rate scheduling
  scheduler:
    name: "cosine"
    options:
      T_max: 200
      eta_min: 1e-6

# Trainer Configuration
trainer:
  name: "Default_trainer"
  num_epochs: 200  # Production mode: full 200 epochs
  gpus: 1
  precision: 16  # Mixed precision
  accumulate_grad_batches: 2  # Effective batch size: 512Ã—2=1024
  gradient_clip_val: 1.0
  
  # Early stopping (disabled for test mode)
  early_stopping: false  # Test mode: disabled
  patience: 25
  monitor: "val_loss"
  
  # Model checkpointing
  save_top_k: 3
  save_last: true
  
  # Logging
  log_every_n_steps: 50
  
  # Device configuration
  device: 'cuda'
  
  # Validation
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  
  # Validation Performance Optimization
  validation_verbose: false  # Disable verbose warnings during validation for speed

# Evaluation Configuration
evaluation:
  # Metrics to compute during evaluation
  compute_metrics:
    - "accuracy"      # For classification
    - "f1_score"      # For classification & anomaly detection
    - "precision"     # For classification & anomaly detection  
    - "recall"        # For classification & anomaly detection
    - "roc_auc"       # For anomaly detection
    - "mse"           # For RUL & signal prediction
    - "mae"           # For RUL & signal prediction
    - "r2_score"      # For RUL & signal prediction
  
  # Test after training
  test_after_training: true

# Reproducibility
reproducibility:
  deterministic: true
  benchmark: false
  seed_everything: true

# Advanced Configuration (Test Mode)
advanced:
  effective_batch_size: 256  # Smaller for testing
  use_amp: true  # Mixed precision
  compile_model: false
  memory_efficient: true
  profiler: null  # No profiling in test mode