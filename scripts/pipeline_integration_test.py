#!/usr/bin/env python3
"""
Pipelineé›†æˆéªŒè¯è„šæœ¬
éªŒè¯ContrastiveIDTaskä¸ŽçŽ°æœ‰Pipelineçš„å…¼å®¹æ€§
"""

import sys
import os
import yaml
import json
import tempfile
import shutil
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

from src.configs import load_config


class PipelineIntegrationValidator:
    """Pipelineé›†æˆéªŒè¯å™¨"""
    
    def __init__(self, test_dir: str = None):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.test_dir = test_dir or tempfile.mkdtemp(prefix="pipeline_test_")
        self.test_path = Path(self.test_dir)
        self.results = {
            'passed': [],
            'failed': [],
            'warnings': []
        }
        
        print(f"Pipelineé›†æˆæµ‹è¯•ç›®å½•: {self.test_dir}")
    
    def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç›®å½•"""
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)
            print(f"æ¸…ç†æµ‹è¯•ç›®å½•: {self.test_dir}")
    
    def create_test_config(self, config_name: str, overrides: Dict = None) -> str:
        """åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶"""
        base_config = {
            'data': {
                'factory_name': 'id',
                'dataset_name': 'ID_dataset',
                'batch_size': 16,
                'num_workers': 2,
                'window_size': 1024,
                'stride': 512,
                'num_window': 2,
                'window_sampling_strategy': 'random',
                'normalization': True,
                'truncate_length': 8192
            },
            'model': {
                'name': 'M_01_ISFM',
                'backbone': 'B_08_PatchTST',
                'd_model': 256
            },
            'task': {
                'type': 'pretrain',
                'name': 'contrastive_id',
                'lr': 1e-3,
                'weight_decay': 1e-4,
                'temperature': 0.07
            },
            'trainer': {
                'epochs': 5,
                'accelerator': 'cpu',
                'devices': 1,
                'precision': 32,
                'gradient_clip_val': 1.0,
                'check_val_every_n_epoch': 2,
                'log_every_n_steps': 10
            },
            'environment': {
                'save_dir': str(self.test_path / "results"),
                'experiment_name': 'pipeline_integration_test'
            }
        }
        
        # åº”ç”¨è¦†ç›–å‚æ•°
        if overrides:
            base_config = self._deep_update(base_config, overrides)
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = self.test_path / f"{config_name}.yaml"
        with open(config_path, 'w') as f:
            yaml.dump(base_config, f, default_flow_style=False)
        
        return str(config_path)
    
    def _deep_update(self, base_dict: Dict, update_dict: Dict) -> Dict:
        """æ·±åº¦æ›´æ–°å­—å…¸"""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                base_dict[key] = self._deep_update(base_dict[key], value)
            else:
                base_dict[key] = value
        return base_dict
    
    def test_pipeline_01_compatibility(self):
        """æµ‹è¯•Pipeline_01å…¼å®¹æ€§"""
        print("\n=== æµ‹è¯•Pipeline_01å…¼å®¹æ€§ ===")
        
        try:
            # Pipeline_01æ ‡å‡†é…ç½®è¦æ±‚
            pipeline_01_requirements = {
                'data': {
                    'factory_name': 'id',
                    'dataset_name': 'ID_dataset'
                },
                'task': {
                    'type': 'pretrain'
                },
                'trainer': {
                    'accelerator': ['cpu', 'gpu'],
                    'precision': [16, 32]
                }
            }
            
            config_path = self.create_test_config('pipeline_01_test')
            config = load_config(config_path)
            
            # éªŒè¯å…³é”®é…ç½®é¡¹
            assert config['data']['factory_name'] == 'id', "æ•°æ®å·¥åŽ‚åç§°ä¸å…¼å®¹"
            assert config['task']['type'] == 'pretrain', "ä»»åŠ¡ç±»åž‹ä¸å…¼å®¹"
            assert config['trainer']['accelerator'] in ['cpu', 'gpu'], "åŠ é€Ÿå™¨ç±»åž‹ä¸æ”¯æŒ"
            assert config['trainer']['precision'] in [16, 32], "ç²¾åº¦ç±»åž‹ä¸æ”¯æŒ"
            
            # éªŒè¯å¿…éœ€çš„é…ç½®ç»“æž„
            required_sections = ['data', 'model', 'task', 'trainer', 'environment']
            for section in required_sections:
                assert section in config, f"ç¼ºå°‘å¿…éœ€é…ç½®æ®µ: {section}"
            
            # éªŒè¯æ•°æ®ç±»åž‹
            assert isinstance(config['data']['batch_size'], int), "batch_sizeç±»åž‹é”™è¯¯"
            assert isinstance(config['task']['temperature'], (int, float)), "temperatureç±»åž‹é”™è¯¯"
            
            self.results['passed'].append('Pipeline_01å…¼å®¹æ€§')
            print("âœ… Pipeline_01å…¼å®¹æ€§éªŒè¯é€šè¿‡")
            
        except Exception as e:
            self.results['failed'].append(f'Pipeline_01å…¼å®¹æ€§: {str(e)}')
            print(f"âŒ Pipeline_01å…¼å®¹æ€§éªŒè¯å¤±è´¥: {e}")
    
    def test_pipeline_id_compatibility(self):
        """æµ‹è¯•Pipeline_IDå…¼å®¹æ€§"""
        print("\n=== æµ‹è¯•Pipeline_IDå…¼å®¹æ€§ ===")
        
        try:
            # Pipeline_IDç‰¹å®šè¦æ±‚
            id_config_overrides = {
                'data': {
                    'factory_name': 'id',  # å¿…é¡»ä½¿ç”¨idå·¥åŽ‚
                    'window_sampling_strategy': 'random',  # ID pipelineç‰¹å®š
                    'num_window': 2  # ID pipelineè¦æ±‚
                },
                'task': {
                    'name': 'contrastive_id'  # ç‰¹å®šä»»åŠ¡åç§°
                }
            }
            
            config_path = self.create_test_config('pipeline_id_test', id_config_overrides)
            config = load_config(config_path)
            
            # éªŒè¯ID pipelineç‰¹å®šè¦æ±‚
            assert config['data']['factory_name'] == 'id', "å¿…é¡»ä½¿ç”¨idæ•°æ®å·¥åŽ‚"
            assert config['data']['window_sampling_strategy'] == 'random', "çª—å£é‡‡æ ·ç­–ç•¥ä¸å…¼å®¹"
            assert config['data']['num_window'] >= 2, "çª—å£æ•°é‡è¦æ±‚ä¸æ»¡è¶³"
            assert config['task']['name'] == 'contrastive_id', "ä»»åŠ¡åç§°ä¸åŒ¹é…"
            
            # éªŒè¯çª—å£åŒ–å‚æ•°
            assert 'window_size' in config['data'], "ç¼ºå°‘çª—å£å¤§å°é…ç½®"
            assert 'stride' in config['data'], "ç¼ºå°‘æ­¥é•¿é…ç½®"
            assert config['data']['window_size'] > 0, "çª—å£å¤§å°æ— æ•ˆ"
            assert config['data']['stride'] > 0, "æ­¥é•¿æ— æ•ˆ"
            
            self.results['passed'].append('Pipeline_IDå…¼å®¹æ€§')
            print("âœ… Pipeline_IDå…¼å®¹æ€§éªŒè¯é€šè¿‡")
            
        except Exception as e:
            self.results['failed'].append(f'Pipeline_IDå…¼å®¹æ€§: {str(e)}')
            print(f"âŒ Pipeline_IDå…¼å®¹æ€§éªŒè¯å¤±è´¥: {e}")
    
    def test_pipeline_02_pretrain_finetune_compatibility(self):
        """æµ‹è¯•Pipeline_02é¢„è®­ç»ƒ+å¾®è°ƒå…¼å®¹æ€§"""
        print("\n=== æµ‹è¯•Pipeline_02é¢„è®­ç»ƒ+å¾®è°ƒå…¼å®¹æ€§ ===")
        
        try:
            # åˆ›å»ºé¢„è®­ç»ƒé˜¶æ®µé…ç½®
            pretrain_config_path = self.create_test_config('pipeline_02_pretrain', {
                'task': {'type': 'pretrain', 'name': 'contrastive_id'},
                'trainer': {'epochs': 10},
                'environment': {'experiment_name': 'pretrain_stage'}
            })
            
            # åˆ›å»ºå¾®è°ƒé˜¶æ®µé…ç½®
            finetune_overrides = {
                'task': {
                    'type': 'finetune',
                    'name': 'classification',
                    'pretrain_checkpoint': 'path/to/pretrain/checkpoint.ckpt'
                },
                'trainer': {'epochs': 5},
                'environment': {'experiment_name': 'finetune_stage'}
            }
            finetune_config_path = self.create_test_config('pipeline_02_finetune', finetune_overrides)
            
            # éªŒè¯é¢„è®­ç»ƒé…ç½®
            pretrain_config = load_config(pretrain_config_path)
            assert pretrain_config['task']['type'] == 'pretrain', "é¢„è®­ç»ƒä»»åŠ¡ç±»åž‹é”™è¯¯"
            assert pretrain_config['task']['name'] == 'contrastive_id', "é¢„è®­ç»ƒä»»åŠ¡åç§°é”™è¯¯"
            
            # éªŒè¯å¾®è°ƒé…ç½®
            finetune_config = load_config(finetune_config_path)
            assert finetune_config['task']['type'] == 'finetune', "å¾®è°ƒä»»åŠ¡ç±»åž‹é”™è¯¯"
            assert 'pretrain_checkpoint' in finetune_config['task'], "ç¼ºå°‘é¢„è®­ç»ƒæ£€æŸ¥ç‚¹é…ç½®"
            
            # éªŒè¯é…ç½®ä¸€è‡´æ€§ï¼ˆæ¨¡åž‹æž¶æž„åº”è¯¥å…¼å®¹ï¼‰
            assert (pretrain_config['model']['name'] == finetune_config['model']['name'] or
                    'pretrain_checkpoint' in finetune_config['task']), "æ¨¡åž‹æž¶æž„ä¸å…¼å®¹"
            
            self.results['passed'].append('Pipeline_02å…¼å®¹æ€§')
            print("âœ… Pipeline_02é¢„è®­ç»ƒ+å¾®è°ƒå…¼å®¹æ€§éªŒè¯é€šè¿‡")
            
        except Exception as e:
            self.results['failed'].append(f'Pipeline_02å…¼å®¹æ€§: {str(e)}')
            print(f"âŒ Pipeline_02å…¼å®¹æ€§éªŒè¯å¤±è´¥: {e}")
    
    def test_config_system_integration(self):
        """æµ‹è¯•é…ç½®ç³»ç»Ÿé›†æˆ"""
        print("\n=== æµ‹è¯•é…ç½®ç³»ç»Ÿé›†æˆ ===")
        
        try:
            # æµ‹è¯•v5.0é…ç½®ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½
            base_config_path = self.create_test_config('config_system_test')
            
            # æµ‹è¯•é…ç½®åŠ è½½
            config = load_config(base_config_path)
            assert isinstance(config, dict), "é…ç½®åŠ è½½ç»“æžœä¸æ˜¯å­—å…¸"
            
            # æµ‹è¯•å‚æ•°è¦†ç›–
            overrides = {
                'task.temperature': 0.05,
                'data.batch_size': 64,
                'trainer.epochs': 20
            }
            config_with_overrides = load_config(base_config_path, overrides)
            
            assert config_with_overrides['task']['temperature'] == 0.05, "æ¸©åº¦å‚æ•°è¦†ç›–å¤±è´¥"
            assert config_with_overrides['data']['batch_size'] == 64, "æ‰¹é‡å¤§å°è¦†ç›–å¤±è´¥"
            assert config_with_overrides['trainer']['epochs'] == 20, "epochè¦†ç›–å¤±è´¥"
            
            # æµ‹è¯•æ·±åº¦è¦†ç›–
            deep_overrides = {
                'model': {'d_model': 512, 'backbone': 'B_04_Dlinear'},
                'data': {'window_size': 2048}
            }
            config_deep = load_config(base_config_path, deep_overrides)
            
            assert config_deep['model']['d_model'] == 512, "æ·±åº¦è¦†ç›–å¤±è´¥"
            assert config_deep['model']['backbone'] == 'B_04_Dlinear', "ä¸»å¹²ç½‘ç»œè¦†ç›–å¤±è´¥"
            assert config_deep['data']['window_size'] == 2048, "çª—å£å¤§å°è¦†ç›–å¤±è´¥"
            
            # éªŒè¯æœªè¦†ç›–çš„å€¼ä¿æŒä¸å˜
            assert config_deep['task']['name'] == 'contrastive_id', "æœªè¦†ç›–å€¼è¢«æ„å¤–ä¿®æ”¹"
            
            self.results['passed'].append('é…ç½®ç³»ç»Ÿé›†æˆ')
            print("âœ… é…ç½®ç³»ç»Ÿé›†æˆéªŒè¯é€šè¿‡")
            
        except Exception as e:
            self.results['failed'].append(f'é…ç½®ç³»ç»Ÿé›†æˆ: {str(e)}')
            print(f"âŒ é…ç½®ç³»ç»Ÿé›†æˆéªŒè¯å¤±è´¥: {e}")
    
    def test_results_format_compatibility(self):
        """æµ‹è¯•ç»“æžœä¿å­˜æ ¼å¼å…¼å®¹æ€§"""
        print("\n=== æµ‹è¯•ç»“æžœä¿å­˜æ ¼å¼å…¼å®¹æ€§ ===")
        
        try:
            config_path = self.create_test_config('results_format_test')
            config = load_config(config_path)
            
            # éªŒè¯ä¿å­˜ç›®å½•ç»“æž„ç¬¦åˆPHM-Vibenchè§„èŒƒ
            save_dir = Path(config['environment']['save_dir'])
            experiment_name = config['environment']['experiment_name']
            
            # åˆ›å»ºé¢„æœŸçš„ç›®å½•ç»“æž„
            expected_dirs = [
                save_dir / experiment_name / 'checkpoints',
                save_dir / experiment_name / 'figures', 
                save_dir / experiment_name / 'logs'
            ]
            
            for dir_path in expected_dirs:
                dir_path.mkdir(parents=True, exist_ok=True)
            
            # éªŒè¯ç›®å½•å­˜åœ¨
            for dir_path in expected_dirs:
                assert dir_path.exists(), f"é¢„æœŸç›®å½•ä¸å­˜åœ¨: {dir_path}"
            
            # åˆ›å»ºç¤ºä¾‹ç»“æžœæ–‡ä»¶
            metrics_file = save_dir / experiment_name / 'metrics.json'
            config_backup = save_dir / experiment_name / 'config.yaml'
            log_file = save_dir / experiment_name / 'logs' / 'training.log'
            
            # ä¿å­˜ç¤ºä¾‹æŒ‡æ ‡
            metrics = {
                'train_loss': [0.8, 0.6, 0.4, 0.3],
                'val_loss': [0.9, 0.7, 0.5, 0.4],
                'train_contrastive_acc': [0.5, 0.7, 0.8, 0.85],
                'val_contrastive_acc': [0.4, 0.6, 0.75, 0.8],
                'epoch': [1, 2, 3, 4]
            }
            
            with open(metrics_file, 'w') as f:
                json.dump(metrics, f, indent=2)
            
            # å¤‡ä»½é…ç½®æ–‡ä»¶
            with open(config_backup, 'w') as f:
                yaml.dump(config, f, default_flow_style=False)
            
            # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
            with open(log_file, 'w') as f:
                f.write("Epoch 1: train_loss=0.8, val_loss=0.9, train_acc=0.5, val_acc=0.4\n")
                f.write("Epoch 2: train_loss=0.6, val_loss=0.7, train_acc=0.7, val_acc=0.6\n")
                f.write("Epoch 3: train_loss=0.4, val_loss=0.5, train_acc=0.8, val_acc=0.75\n")
                f.write("Epoch 4: train_loss=0.3, val_loss=0.4, train_acc=0.85, val_acc=0.8\n")
            
            # éªŒè¯æ–‡ä»¶æ ¼å¼
            assert metrics_file.exists(), "æŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨"
            assert config_backup.exists(), "é…ç½®å¤‡ä»½ä¸å­˜åœ¨" 
            assert log_file.exists(), "æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨"
            
            # éªŒè¯æŒ‡æ ‡æ–‡ä»¶å†…å®¹
            with open(metrics_file, 'r') as f:
                saved_metrics = json.load(f)
                required_metrics = ['train_loss', 'val_loss', 'train_contrastive_acc', 'val_contrastive_acc']
                for metric in required_metrics:
                    assert metric in saved_metrics, f"ç¼ºå°‘å¿…éœ€æŒ‡æ ‡: {metric}"
            
            self.results['passed'].append('ç»“æžœæ ¼å¼å…¼å®¹æ€§')
            print("âœ… ç»“æžœä¿å­˜æ ¼å¼å…¼å®¹æ€§éªŒè¯é€šè¿‡")
            
        except Exception as e:
            self.results['failed'].append(f'ç»“æžœæ ¼å¼å…¼å®¹æ€§: {str(e)}')
            print(f"âŒ ç»“æžœæ ¼å¼å…¼å®¹æ€§éªŒè¯å¤±è´¥: {e}")
    
    def test_multitask_pipeline_compatibility(self):
        """æµ‹è¯•å¤šä»»åŠ¡Pipelineå…¼å®¹æ€§"""
        print("\n=== æµ‹è¯•å¤šä»»åŠ¡Pipelineå…¼å®¹æ€§ ===")
        
        try:
            # å¤šä»»åŠ¡é…ç½®ï¼ˆå¯¹æ¯”é¢„è®­ç»ƒä½œä¸ºå…¶ä¸­ä¸€ä¸ªä»»åŠ¡ï¼‰
            multitask_overrides = {
                'task': {
                    'type': 'multitask',
                    'subtasks': [
                        {
                            'name': 'contrastive_id',
                            'type': 'pretrain',
                            'weight': 1.0,
                            'temperature': 0.07
                        },
                        {
                            'name': 'classification',
                            'type': 'supervised',
                            'weight': 0.5,
                            'num_classes': 10
                        }
                    ]
                },
                'trainer': {
                    'epochs': 15,
                    'multitask_balancing': 'weighted'
                }
            }
            
            config_path = self.create_test_config('multitask_test', multitask_overrides)
            config = load_config(config_path)
            
            # éªŒè¯å¤šä»»åŠ¡é…ç½®ç»“æž„
            assert config['task']['type'] == 'multitask', "å¤šä»»åŠ¡ç±»åž‹é…ç½®é”™è¯¯"
            assert 'subtasks' in config['task'], "ç¼ºå°‘å­ä»»åŠ¡é…ç½®"
            assert len(config['task']['subtasks']) >= 2, "å­ä»»åŠ¡æ•°é‡ä¸è¶³"
            
            # éªŒè¯å¯¹æ¯”å­¦ä¹ å­ä»»åŠ¡
            contrastive_task = None
            for subtask in config['task']['subtasks']:
                if subtask['name'] == 'contrastive_id':
                    contrastive_task = subtask
                    break
            
            assert contrastive_task is not None, "ç¼ºå°‘å¯¹æ¯”å­¦ä¹ å­ä»»åŠ¡"
            assert contrastive_task['type'] == 'pretrain', "å¯¹æ¯”å­¦ä¹ å­ä»»åŠ¡ç±»åž‹é”™è¯¯"
            assert 'temperature' in contrastive_task, "ç¼ºå°‘æ¸©åº¦å‚æ•°"
            assert 'weight' in contrastive_task, "ç¼ºå°‘ä»»åŠ¡æƒé‡"
            
            # éªŒè¯ä»»åŠ¡æƒé‡å’Œ
            total_weight = sum(subtask['weight'] for subtask in config['task']['subtasks'])
            if abs(total_weight - 1.0) > 0.1:  # å…è®¸ä¸€å®šçš„æƒé‡åˆ†é…çµæ´»æ€§
                self.results['warnings'].append('ä»»åŠ¡æƒé‡å’Œä¸ä¸º1.0ï¼Œå¯èƒ½å½±å“è®­ç»ƒå¹³è¡¡')
            
            self.results['passed'].append('å¤šä»»åŠ¡Pipelineå…¼å®¹æ€§')
            print("âœ… å¤šä»»åŠ¡Pipelineå…¼å®¹æ€§éªŒè¯é€šè¿‡")
            
        except Exception as e:
            self.results['failed'].append(f'å¤šä»»åŠ¡Pipelineå…¼å®¹æ€§: {str(e)}')
            print(f"âŒ å¤šä»»åŠ¡Pipelineå…¼å®¹æ€§éªŒè¯å¤±è´¥: {e}")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
        print("å¼€å§‹Pipelineé›†æˆå…¼å®¹æ€§éªŒè¯...")
        
        test_methods = [
            self.test_pipeline_01_compatibility,
            self.test_pipeline_id_compatibility,
            self.test_pipeline_02_pretrain_finetune_compatibility,
            self.test_config_system_integration,
            self.test_results_format_compatibility,
            self.test_multitask_pipeline_compatibility
        ]
        
        for test_method in test_methods:
            try:
                test_method()
            except Exception as e:
                print(f"æµ‹è¯•æ–¹æ³• {test_method.__name__} æ‰§è¡Œå¤±è´¥: {e}")
        
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("Pipelineé›†æˆå…¼å®¹æ€§éªŒè¯æ‘˜è¦")
        print("="*60)
        
        print(f"âœ… é€šè¿‡æµ‹è¯• ({len(self.results['passed'])}):")
        for test in self.results['passed']:
            print(f"  - {test}")
        
        if self.results['warnings']:
            print(f"\nâš ï¸  è­¦å‘Š ({len(self.results['warnings'])}):")
            for warning in self.results['warnings']:
                print(f"  - {warning}")
        
        if self.results['failed']:
            print(f"\nâŒ å¤±è´¥æµ‹è¯• ({len(self.results['failed'])}):")
            for failure in self.results['failed']:
                print(f"  - {failure}")
        else:
            print(f"\nðŸŽ‰ æ‰€æœ‰æ ¸å¿ƒæµ‹è¯•é€šè¿‡ï¼ContrastiveIDTaskä¸ŽçŽ°æœ‰Pipelineå®Œå…¨å…¼å®¹")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_tests = len(self.results['passed']) + len(self.results['failed'])
        success_rate = len(self.results['passed']) / total_tests * 100 if total_tests > 0 else 0
        
        print(f"\nðŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"  é€šè¿‡çŽ‡: {success_rate:.1f}%")
        print(f"  è­¦å‘Šæ•°: {len(self.results['warnings'])}")
        
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    validator = PipelineIntegrationValidator()
    
    try:
        validator.run_all_tests()
    finally:
        validator.cleanup()


if __name__ == "__main__":
    main()