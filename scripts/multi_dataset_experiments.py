#!/usr/bin/env python3
"""
å¤šæ•°æ®é›†å®éªŒè„šæœ¬
åŸºäºmetadataè‡ªåŠ¨æ‰¹é‡è¿è¡Œå¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå®éªŒ
"""

import os
import sys
import json
import yaml
import argparse
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import subprocess
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

from src.configs import load_config


class MultiDatasetExperimentRunner:
    """å¤šæ•°æ®é›†å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, 
                 base_config_path: str,
                 metadata_dir: str = "data",
                 results_dir: str = "save/multi_dataset",
                 dry_run: bool = False):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨
        
        Args:
            base_config_path: åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„
            metadata_dir: metadataæ–‡ä»¶ç›®å½•
            results_dir: ç»“æœä¿å­˜ç›®å½•
            dry_run: æ˜¯å¦åªè¾“å‡ºå®éªŒè®¡åˆ’è€Œä¸å®é™…è¿è¡Œ
        """
        self.base_config_path = base_config_path
        self.metadata_dir = Path(metadata_dir)
        self.results_dir = Path(results_dir)
        self.dry_run = dry_run
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # å®éªŒè®°å½•
        self.experiments = []
        self.results = {
            'completed': [],
            'failed': [],
            'skipped': []
        }
        
        print(f"å¤šæ•°æ®é›†å®éªŒè¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"åŸºç¡€é…ç½®: {base_config_path}")
        print(f"ç»“æœç›®å½•: {results_dir}")
        print(f"å¹²è¿è¡Œæ¨¡å¼: {dry_run}")
    
    def discover_datasets(self) -> List[Dict[str, Any]]:
        """å‘ç°æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
        print(f"\næ­£åœ¨æ‰«æmetadataç›®å½•: {self.metadata_dir}")
        
        datasets = []
        metadata_files = list(self.metadata_dir.glob("metadata_*.xlsx"))
        
        if not metadata_files:
            print(f"âš ï¸  åœ¨ {self.metadata_dir} ä¸­æœªæ‰¾åˆ°metadataæ–‡ä»¶")
            return datasets
        
        for metadata_file in metadata_files:
            try:
                # æå–æ•°æ®é›†åç§°
                dataset_name = metadata_file.stem.replace('metadata_', '')
                
                # è¯»å–metadataæ–‡ä»¶è·å–åŸºæœ¬ä¿¡æ¯
                df = pd.read_excel(metadata_file, sheet_name=0)
                
                dataset_info = {
                    'name': dataset_name,
                    'metadata_file': str(metadata_file),
                    'num_samples': len(df),
                    'h5_file': self.metadata_dir / f"{dataset_name}.h5"
                }
                
                # æ£€æŸ¥H5æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if dataset_info['h5_file'].exists():
                    dataset_info['h5_size_mb'] = dataset_info['h5_file'].stat().st_size / (1024 * 1024)
                    dataset_info['ready'] = True
                else:
                    dataset_info['h5_size_mb'] = 0
                    dataset_info['ready'] = False
                    print(f"âš ï¸  æ•°æ®é›† {dataset_name} çš„H5æ–‡ä»¶ä¸å­˜åœ¨: {dataset_info['h5_file']}")
                
                # å°è¯•è·å–æ›´å¤šä¿¡æ¯
                if 'Label' in df.columns:
                    dataset_info['num_classes'] = df['Label'].nunique()
                    dataset_info['class_distribution'] = df['Label'].value_counts().to_dict()
                
                if 'ID' in df.columns:
                    dataset_info['num_ids'] = df['ID'].nunique()
                
                datasets.append(dataset_info)
                print(f"âœ… å‘ç°æ•°æ®é›†: {dataset_name} ({dataset_info['num_samples']} æ ·æœ¬)")
                
            except Exception as e:
                print(f"âŒ å¤„ç†metadataæ–‡ä»¶å¤±è´¥ {metadata_file}: {e}")
        
        print(f"\nå…±å‘ç° {len(datasets)} ä¸ªæ•°æ®é›†")
        return datasets
    
    def filter_datasets(self, 
                       datasets: List[Dict], 
                       include_patterns: List[str] = None,
                       exclude_patterns: List[str] = None,
                       min_samples: int = 100,
                       ready_only: bool = True) -> List[Dict]:
        """è¿‡æ»¤æ•°æ®é›†"""
        print(f"\næ­£åœ¨è¿‡æ»¤æ•°æ®é›†...")
        
        filtered_datasets = []
        
        for dataset in datasets:
            # æ£€æŸ¥æ˜¯å¦å‡†å¤‡å°±ç»ª
            if ready_only and not dataset.get('ready', False):
                print(f"è·³è¿‡æœªå‡†å¤‡æ•°æ®é›†: {dataset['name']}")
                continue
            
            # æ£€æŸ¥æ ·æœ¬æ•°é‡
            if dataset['num_samples'] < min_samples:
                print(f"è·³è¿‡æ ·æœ¬æ•°é‡ä¸è¶³çš„æ•°æ®é›†: {dataset['name']} ({dataset['num_samples']} < {min_samples})")
                continue
            
            # æ£€æŸ¥åŒ…å«æ¨¡å¼
            if include_patterns:
                if not any(pattern.lower() in dataset['name'].lower() for pattern in include_patterns):
                    print(f"è·³è¿‡ä¸åŒ¹é…åŒ…å«æ¨¡å¼çš„æ•°æ®é›†: {dataset['name']}")
                    continue
            
            # æ£€æŸ¥æ’é™¤æ¨¡å¼
            if exclude_patterns:
                if any(pattern.lower() in dataset['name'].lower() for pattern in exclude_patterns):
                    print(f"è·³è¿‡åŒ¹é…æ’é™¤æ¨¡å¼çš„æ•°æ®é›†: {dataset['name']}")
                    continue
            
            filtered_datasets.append(dataset)
            print(f"âœ… ä¿ç•™æ•°æ®é›†: {dataset['name']}")
        
        print(f"\nè¿‡æ»¤åä¿ç•™ {len(filtered_datasets)} ä¸ªæ•°æ®é›†")
        return filtered_datasets
    
    def generate_experiment_configs(self, 
                                   datasets: List[Dict],
                                   config_variants: List[Dict] = None) -> List[Dict]:
        """ä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆå®éªŒé…ç½®"""
        print(f"\næ­£åœ¨ç”Ÿæˆå®éªŒé…ç½®...")
        
        if config_variants is None:
            # å¯¹æ¯”å­¦ä¹ é»˜è®¤é…ç½®å˜ä½“
            config_variants = [
                {'name': 'default', 'overrides': {}},
                {'name': 'large_window', 'overrides': {'data.window_size': 2048, 'data.stride': 1024}},
                {'name': 'small_window', 'overrides': {'data.window_size': 512, 'data.stride': 256}},
                {'name': 'low_temp', 'overrides': {'task.temperature': 0.01}},
                {'name': 'high_temp', 'overrides': {'task.temperature': 0.5}},
                {'name': 'high_lr', 'overrides': {'task.lr': 5e-3}},
                {'name': 'large_batch', 'overrides': {'data.batch_size': 64}},
                {'name': 'small_batch', 'overrides': {'data.batch_size': 8}}
            ]
        
        experiments = []
        
        for dataset in datasets:
            for variant in config_variants:
                # åŸºç¡€é…ç½®è¦†ç›–
                base_overrides = {
                    'data.metadata_file': dataset['metadata_file'],
                    'environment.experiment_name': f"contrastive_{dataset['name']}_{variant['name']}",
                    'environment.save_dir': str(self.results_dir / dataset['name'] / variant['name'])
                }
                
                # åˆå¹¶å˜ä½“è¦†ç›–
                final_overrides = {**base_overrides, **variant['overrides']}
                
                # æ•°æ®é›†ç‰¹å®šè°ƒæ•´
                dataset_specific_overrides = self._get_dataset_specific_overrides(dataset)
                final_overrides.update(dataset_specific_overrides)
                
                experiment = {
                    'id': f"{dataset['name']}_{variant['name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    'dataset': dataset['name'],
                    'variant': variant['name'], 
                    'config_overrides': final_overrides,
                    'expected_duration_hours': self._estimate_experiment_duration(dataset, variant),
                    'status': 'pending'
                }
                
                experiments.append(experiment)
        
        print(f"ç”Ÿæˆäº† {len(experiments)} ä¸ªå®éªŒé…ç½®")
        return experiments
    
    def _get_dataset_specific_overrides(self, dataset: Dict) -> Dict:
        """æ ¹æ®æ•°æ®é›†ç‰¹æ€§ç”Ÿæˆç‰¹å®šçš„é…ç½®è¦†ç›–"""
        overrides = {}
        
        # æ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´æ‰¹é‡å¤§å°
        if dataset['num_samples'] < 500:
            overrides['data.batch_size'] = 8
            overrides['trainer.epochs'] = 1  # å¿«é€Ÿæµ‹è¯•ç”¨å•epoch
        elif dataset['num_samples'] < 2000:
            overrides['data.batch_size'] = 16
            overrides['trainer.epochs'] = 1  # å¿«é€Ÿæµ‹è¯•ç”¨å•epoch
        else:
            overrides['data.batch_size'] = 32
            overrides['trainer.epochs'] = 1  # å¿«é€Ÿæµ‹è¯•ç”¨å•epoch
        
        # æ ¹æ®H5æ–‡ä»¶å¤§å°è°ƒæ•´num_workers
        if dataset.get('h5_size_mb', 0) > 1000:  # å¤§æ–‡ä»¶
            overrides['data.num_workers'] = 8
        elif dataset.get('h5_size_mb', 0) > 100:
            overrides['data.num_workers'] = 4
        else:
            overrides['data.num_workers'] = 2
        
        # æ ¹æ®ç±»åˆ«æ•°é‡è°ƒæ•´æ¨¡å‹å¤§å°
        if dataset.get('num_classes', 0) > 10:
            overrides['model.d_model'] = 512
        elif dataset.get('num_classes', 0) > 5:
            overrides['model.d_model'] = 256
        else:
            overrides['model.d_model'] = 128
        
        return overrides
    
    def _estimate_experiment_duration(self, dataset: Dict, variant: Dict) -> float:
        """ä¼°è®¡å®éªŒæŒç»­æ—¶é—´ï¼ˆå°æ—¶ï¼‰"""
        base_duration = 0.5  # åŸºç¡€æ—¶é—´
        
        # æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´
        size_factor = dataset['num_samples'] / 1000
        base_duration *= (1 + size_factor * 0.1)
        
        # æ ¹æ®é…ç½®å˜ä½“è°ƒæ•´
        if 'large_window' in variant['name']:
            base_duration *= 1.5
        if 'high_lr' in variant['name']:
            base_duration *= 0.8  # å¯èƒ½æ”¶æ•›æ›´å¿«
        
        return round(base_duration, 2)
    
    def run_experiments(self, 
                       experiments: List[Dict],
                       parallel: bool = False,
                       max_parallel: int = 2,
                       timeout_hours: int = 24) -> Dict:
        """è¿è¡Œå®éªŒ"""
        print(f"\nå¼€å§‹è¿è¡Œ {len(experiments)} ä¸ªå®éªŒ")
        
        if self.dry_run:
            print("ğŸ”„ å¹²è¿è¡Œæ¨¡å¼ï¼Œåªè¾“å‡ºå®éªŒè®¡åˆ’:")
            self._print_experiment_plan(experiments)
            return {'completed': [], 'failed': [], 'skipped': experiments}
        
        if parallel:
            return self._run_experiments_parallel(experiments, max_parallel, timeout_hours)
        else:
            return self._run_experiments_sequential(experiments, timeout_hours)
    
    def _run_experiments_sequential(self, experiments: List[Dict], timeout_hours: int) -> Dict:
        """é¡ºåºè¿è¡Œå®éªŒ"""
        results = {'completed': [], 'failed': [], 'skipped': []}
        
        total_experiments = len(experiments)
        
        for i, experiment in enumerate(experiments, 1):
            print(f"\n{'='*60}")
            print(f"è¿è¡Œå®éªŒ {i}/{total_experiments}: {experiment['id']}")
            print(f"æ•°æ®é›†: {experiment['dataset']}, å˜ä½“: {experiment['variant']}")
            print(f"é¢„è®¡è€—æ—¶: {experiment['expected_duration_hours']} å°æ—¶")
            print(f"{'='*60}")
            
            start_time = time.time()
            
            try:
                # åˆ›å»ºå®éªŒé…ç½®æ–‡ä»¶
                config_path = self._create_experiment_config(experiment)
                
                # è¿è¡Œå®éªŒ
                success = self._run_single_experiment(config_path, experiment, timeout_hours)
                
                end_time = time.time()
                actual_duration = (end_time - start_time) / 3600  # è½¬æ¢ä¸ºå°æ—¶
                
                experiment['actual_duration_hours'] = round(actual_duration, 2)
                experiment['status'] = 'completed' if success else 'failed'
                
                if success:
                    results['completed'].append(experiment)
                    print(f"âœ… å®éªŒå®Œæˆ: {experiment['id']} (è€—æ—¶: {actual_duration:.2f} å°æ—¶)")
                else:
                    results['failed'].append(experiment)
                    print(f"âŒ å®éªŒå¤±è´¥: {experiment['id']} (è€—æ—¶: {actual_duration:.2f} å°æ—¶)")
                
            except Exception as e:
                print(f"âŒ å®éªŒå¼‚å¸¸: {experiment['id']} - {e}")
                experiment['status'] = 'failed'
                experiment['error'] = str(e)
                results['failed'].append(experiment)
        
        return results
    
    def _run_experiments_parallel(self, experiments: List[Dict], max_parallel: int, timeout_hours: int) -> Dict:
        """å¹¶è¡Œè¿è¡Œå®éªŒ"""
        # è¿™é‡Œå¯ä»¥å®ç°å¹¶è¡Œæ‰§è¡Œé€»è¾‘
        # ä¸ºç®€åŒ–ï¼Œæš‚æ—¶ä½¿ç”¨é¡ºåºæ‰§è¡Œ
        print(f"âš ï¸  å¹¶è¡Œæ‰§è¡Œå°šæœªå®ç°ï¼Œä½¿ç”¨é¡ºåºæ‰§è¡Œ (max_parallel={max_parallel})")
        return self._run_experiments_sequential(experiments, timeout_hours)
    
    def _create_experiment_config(self, experiment: Dict) -> str:
        """åˆ›å»ºå®éªŒé…ç½®æ–‡ä»¶"""
        # åŠ è½½åŸºç¡€é…ç½®å¹¶åº”ç”¨è¦†ç›–
        config = load_config(self.base_config_path, experiment['config_overrides'])
        
        # ä¿å­˜å®éªŒé…ç½®
        experiment_dir = Path(experiment['config_overrides']['environment.save_dir'])
        experiment_dir.mkdir(parents=True, exist_ok=True)
        
        config_path = experiment_dir / f"config_{experiment['id']}.yaml"
        
        with open(config_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        # ä¿å­˜å®éªŒå…ƒä¿¡æ¯
        experiment_info = {
            'experiment_id': experiment['id'],
            'dataset': experiment['dataset'],
            'variant': experiment['variant'],
            'created_at': datetime.now().isoformat(),
            'config_overrides': experiment['config_overrides'],
            'expected_duration_hours': experiment['expected_duration_hours']
        }
        
        info_path = experiment_dir / f"experiment_info_{experiment['id']}.json"
        with open(info_path, 'w') as f:
            json.dump(experiment_info, f, indent=2)
        
        return str(config_path)
    
    def _run_single_experiment(self, config_path: str, experiment: Dict, timeout_hours: int) -> bool:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        try:
            # æ„å»ºå‘½ä»¤
            cmd = [
                'python', 'main.py',
                '--config', config_path
            ]
            
            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆè½¬æ¢ä¸ºç§’ï¼‰
            timeout_seconds = timeout_hours * 3600
            
            # è¿è¡Œå®éªŒ
            result = subprocess.run(
                cmd,
                timeout=timeout_seconds,
                capture_output=True,
                text=True
            )
            
            # ä¿å­˜æ—¥å¿—
            experiment_dir = Path(experiment['config_overrides']['environment.save_dir'])
            
            stdout_path = experiment_dir / f"stdout_{experiment['id']}.log"
            stderr_path = experiment_dir / f"stderr_{experiment['id']}.log"
            
            with open(stdout_path, 'w') as f:
                f.write(result.stdout)
            
            with open(stderr_path, 'w') as f:
                f.write(result.stderr)
            
            # æ£€æŸ¥è¿”å›ç 
            if result.returncode == 0:
                print(f"å®éªŒæˆåŠŸå®Œæˆ")
                return True
            else:
                print(f"å®éªŒå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                print(f"é”™è¯¯ä¿¡æ¯: {result.stderr[-500:]}")  # åªæ˜¾ç¤ºæœ€å500ä¸ªå­—ç¬¦
                return False
                
        except subprocess.TimeoutExpired:
            print(f"å®éªŒè¶…æ—¶ ({timeout_hours} å°æ—¶)")
            return False
        except Exception as e:
            print(f"è¿è¡Œå®éªŒæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return False
    
    def _print_experiment_plan(self, experiments: List[Dict]):
        """æ‰“å°å®éªŒè®¡åˆ’"""
        print(f"\nå®éªŒè®¡åˆ’æ‘˜è¦:")
        print(f"æ€»å®éªŒæ•°: {len(experiments)}")
        
        # æŒ‰æ•°æ®é›†åˆ†ç»„
        datasets_groups = {}
        for exp in experiments:
            dataset = exp['dataset']
            if dataset not in datasets_groups:
                datasets_groups[dataset] = []
            datasets_groups[dataset].append(exp)
        
        for dataset, exps in datasets_groups.items():
            print(f"\næ•°æ®é›†: {dataset} ({len(exps)} ä¸ªå®éªŒ)")
            total_time = sum(exp['expected_duration_hours'] for exp in exps)
            print(f"  é¢„è®¡æ€»è€—æ—¶: {total_time:.1f} å°æ—¶")
            
            for exp in exps:
                print(f"    - {exp['variant']}: {exp['expected_duration_hours']} å°æ—¶")
        
        total_time = sum(exp['expected_duration_hours'] for exp in experiments)
        print(f"\næ€»é¢„è®¡è€—æ—¶: {total_time:.1f} å°æ—¶")
    
    def generate_report(self, results: Dict) -> str:
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        print(f"\nç”Ÿæˆå®éªŒæŠ¥å‘Š...")
        
        report_data = {
            'summary': {
                'total_experiments': len(results['completed']) + len(results['failed']) + len(results['skipped']),
                'completed': len(results['completed']),
                'failed': len(results['failed']),
                'skipped': len(results['skipped']),
                'success_rate': len(results['completed']) / (len(results['completed']) + len(results['failed'])) * 100 if (len(results['completed']) + len(results['failed'])) > 0 else 0
            },
            'completed_experiments': results['completed'],
            'failed_experiments': results['failed'],
            'skipped_experiments': results['skipped'],
            'generated_at': datetime.now().isoformat()
        }
        
        # è®¡ç®—æ€»è€—æ—¶
        total_duration = sum(exp.get('actual_duration_hours', 0) for exp in results['completed'])
        report_data['summary']['total_duration_hours'] = round(total_duration, 2)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_path = self.results_dir / f"experiment_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w') as f:
            json.dump(report_data, f, indent=2)
        
        # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        readable_report_path = self.results_dir / f"experiment_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(readable_report_path, 'w') as f:
            f.write("å¤šæ•°æ®é›†å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå®éªŒæŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"å®éªŒæ€»ç»“:\n")
            f.write(f"  æ€»å®éªŒæ•°: {report_data['summary']['total_experiments']}\n")
            f.write(f"  æˆåŠŸå®Œæˆ: {report_data['summary']['completed']}\n")
            f.write(f"  å¤±è´¥: {report_data['summary']['failed']}\n")
            f.write(f"  è·³è¿‡: {report_data['summary']['skipped']}\n")
            f.write(f"  æˆåŠŸç‡: {report_data['summary']['success_rate']:.1f}%\n")
            f.write(f"  æ€»è€—æ—¶: {report_data['summary']['total_duration_hours']:.2f} å°æ—¶\n\n")
            
            if results['completed']:
                f.write("æˆåŠŸå®Œæˆçš„å®éªŒ:\n")
                for exp in results['completed']:
                    f.write(f"  - {exp['dataset']}_{exp['variant']}: {exp.get('actual_duration_hours', 'N/A')} å°æ—¶\n")
                f.write("\n")
            
            if results['failed']:
                f.write("å¤±è´¥çš„å®éªŒ:\n")
                for exp in results['failed']:
                    error_msg = exp.get('error', 'æœªçŸ¥é”™è¯¯')
                    f.write(f"  - {exp['dataset']}_{exp['variant']}: {error_msg}\n")
                f.write("\n")
            
            f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report_data['generated_at']}\n")
        
        print(f"å®éªŒæŠ¥å‘Šå·²ä¿å­˜:")
        print(f"  è¯¦ç»†æŠ¥å‘Š: {report_path}")
        print(f"  æ‘˜è¦æŠ¥å‘Š: {readable_report_path}")
        
        return str(readable_report_path)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤šæ•°æ®é›†å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå®éªŒ")
    
    parser.add_argument(
        '--base_config',
        default='configs/id_contrastive/pretrain.yaml',
        help='åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--metadata_dir',
        default='data',
        help='metadataæ–‡ä»¶ç›®å½•'
    )
    
    parser.add_argument(
        '--results_dir',
        default='save/multi_dataset_experiments',
        help='ç»“æœä¿å­˜ç›®å½•'
    )
    
    parser.add_argument(
        '--include_datasets',
        nargs='*',
        help='åŒ…å«çš„æ•°æ®é›†åç§°æ¨¡å¼'
    )
    
    parser.add_argument(
        '--exclude_datasets', 
        nargs='*',
        help='æ’é™¤çš„æ•°æ®é›†åç§°æ¨¡å¼'
    )
    
    parser.add_argument(
        '--min_samples',
        type=int,
        default=100,
        help='æœ€å°æ ·æœ¬æ•°è¦æ±‚'
    )
    
    parser.add_argument(
        '--variants',
        nargs='*',
        choices=['default', 'large_window', 'low_temp', 'high_lr'],
        default=['default'],
        help='é…ç½®å˜ä½“'
    )
    
    parser.add_argument(
        '--parallel',
        action='store_true',
        help='å¹¶è¡Œè¿è¡Œå®éªŒ'
    )
    
    parser.add_argument(
        '--max_parallel',
        type=int,
        default=2,
        help='æœ€å¤§å¹¶è¡Œæ•°'
    )
    
    parser.add_argument(
        '--timeout',
        type=int,
        default=24,
        help='å•ä¸ªå®éªŒè¶…æ—¶æ—¶é—´ï¼ˆå°æ—¶ï¼‰'
    )
    
    parser.add_argument(
        '--dry_run',
        action='store_true',
        help='åªè¾“å‡ºå®éªŒè®¡åˆ’ï¼Œä¸å®é™…è¿è¡Œ'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = MultiDatasetExperimentRunner(
        base_config_path=args.base_config,
        metadata_dir=args.metadata_dir,
        results_dir=args.results_dir,
        dry_run=args.dry_run
    )
    
    try:
        # å‘ç°æ•°æ®é›†
        datasets = runner.discover_datasets()
        
        if not datasets:
            print("âŒ æ²¡æœ‰å‘ç°å¯ç”¨çš„æ•°æ®é›†")
            return
        
        # è¿‡æ»¤æ•°æ®é›†
        filtered_datasets = runner.filter_datasets(
            datasets,
            include_patterns=args.include_datasets,
            exclude_patterns=args.exclude_datasets,
            min_samples=args.min_samples,
            ready_only=True
        )
        
        if not filtered_datasets:
            print("âŒ è¿‡æ»¤åæ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†")
            return
        
        # ç”Ÿæˆé…ç½®å˜ä½“
        config_variants = []
        variant_configs = {
            'default': {'name': 'default', 'overrides': {}},
            'large_window': {'name': 'large_window', 'overrides': {'data.window_size': 2048, 'data.stride': 1024}},
            'low_temp': {'name': 'low_temp', 'overrides': {'task.temperature': 0.05}},
            'high_lr': {'name': 'high_lr', 'overrides': {'task.lr': 5e-3}}
        }
        
        for variant in args.variants:
            if variant in variant_configs:
                config_variants.append(variant_configs[variant])
        
        # ç”Ÿæˆå®éªŒé…ç½®
        experiments = runner.generate_experiment_configs(filtered_datasets, config_variants)
        
        if not experiments:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•å®éªŒé…ç½®")
            return
        
        # è¿è¡Œå®éªŒ
        results = runner.run_experiments(
            experiments,
            parallel=args.parallel,
            max_parallel=args.max_parallel,
            timeout_hours=args.timeout
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = runner.generate_report(results)
        
        print(f"\nğŸ‰ å¤šæ•°æ®é›†å®éªŒå®Œæˆ!")
        print(f"æŠ¥å‘Šä¿å­˜åœ¨: {report_path}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()