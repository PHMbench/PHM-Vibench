# é˜¶æ®µ4: ç»“æœåˆ†ææŒ‡å—

å®éªŒç»“æœæ·±åº¦åˆ†æã€æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œæ´å¯ŸæŒ–æ˜çš„å®Œæ•´æŒ‡å—ã€‚

## ğŸ“‹ æœ¬é˜¶æ®µç›®æ ‡

- [x] è¿›è¡Œå…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•
- [x] æ·±å…¥åˆ†æå®éªŒç»“æœå’Œè¶‹åŠ¿
- [x] ç”Ÿæˆé«˜è´¨é‡çš„å¯è§†åŒ–å›¾è¡¨
- [x] æŒ–æ˜å…³é”®æŠ€æœ¯æ´å¯Ÿ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
python performance_benchmark.py \
    --experiments_dir ../03_experiments/results/ \
    --output_dir benchmarks/ \
    --all_metrics
```

### 2. ç»“æœå¯è§†åŒ–åˆ†æ
```bash
python performance_benchmark.py \
    --visualize \
    --experiments_dir ../03_experiments/results/ \
    --figures training_curves,heatmaps,scatter_plots \
    --output_dir figures/
```

### 3. æ€§èƒ½å¯¹æ¯”åˆ†æ
```bash
python performance_benchmark.py \
    --compare_methods \
    --method_dirs method_A/,method_B/,method_C/ \
    --statistical_test \
    --output_report comparison_report.html
```

## ğŸ› ï¸ æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### performance_benchmark.py
**ä¸»è¦åŠŸèƒ½**: å…¨é¢çš„æ€§èƒ½åˆ†æå’ŒåŸºå‡†æµ‹è¯•å·¥å…·

#### åŸºç¡€æ€§èƒ½åˆ†æ
```bash
# å•å®éªŒè¯¦ç»†åˆ†æ
python performance_benchmark.py \
    --experiment_dir ../03_experiments/results/single_cwru/ \
    --detailed_analysis

# æ‰¹é‡å®éªŒåˆ†æ
python performance_benchmark.py \
    --experiments_dir ../03_experiments/results/ \
    --batch_analysis

# å®æ—¶æ€§èƒ½ç›‘æ§
python performance_benchmark.py \
    --monitor \
    --refresh_interval 30 \
    --experiments_dir ../03_experiments/results/
```

#### é«˜çº§æ€§èƒ½åŸºå‡†
```bash
# å†…å­˜ä½¿ç”¨åˆ†æ
python performance_benchmark.py \
    --memory_profiling \
    --batch_sizes 8,16,32,64 \
    --sequence_lengths 1024,2048,4096

# GPUæ€§èƒ½åˆ†æ
python performance_benchmark.py \
    --gpu_profiling \
    --profile_ops infonce,accuracy,forward_pass

# å¯æ‰©å±•æ€§æµ‹è¯•
python performance_benchmark.py \
    --scalability_test \
    --dataset_sizes 100,500,1000,5000 \
    --parallel_workers 1,2,4,8
```

## ğŸ“Š åˆ†æç»´åº¦è¯¦è§£

### ğŸ¯ æ€§èƒ½æŒ‡æ ‡åˆ†æ

#### 1. æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
```python
# æ ¸å¿ƒæŒ‡æ ‡è®¡ç®—
metrics_config = {
    'accuracy': 'classification_accuracy',
    'precision': 'macro_precision',
    'recall': 'macro_recall',
    'f1_score': 'macro_f1',
    'auc_roc': 'multiclass_auc',
    'confusion_matrix': 'normalized_confusion'
}
```

#### 2. è®­ç»ƒæ•ˆç‡æŒ‡æ ‡
```python
# æ•ˆç‡æŒ‡æ ‡ç›‘æ§
efficiency_metrics = {
    'training_time': 'seconds_per_epoch',
    'inference_speed': 'samples_per_second',
    'memory_usage': 'peak_memory_mb',
    'gpu_utilization': 'average_gpu_percent',
    'convergence_speed': 'epochs_to_convergence'
}
```

### ğŸ“ˆ è¶‹åŠ¿åˆ†æåŠŸèƒ½

#### è®­ç»ƒæ›²çº¿åˆ†æ
```bash
# ç”Ÿæˆè®­ç»ƒè¶‹åŠ¿å›¾
python performance_benchmark.py \
    --plot_training_curves \
    --metrics loss,accuracy,lr \
    --smooth_window 10 \
    --compare_experiments

# æ”¶æ•›æ€§åˆ†æ
python performance_benchmark.py \
    --convergence_analysis \
    --patience_threshold 10 \
    --min_improvement 0.001
```

#### è¶…å‚æ•°å½±å“åˆ†æ
```bash
# å‚æ•°æ•æ„Ÿæ€§åˆ†æ
python performance_benchmark.py \
    --parameter_analysis \
    --ablation_dir ../03_experiments/results/ablation/ \
    --parameters temperature,window_size,batch_size \
    --interaction_effects
```

## ğŸ”¬ æ·±åº¦åˆ†æå·¥å…·

### ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
```python
# statistical_analysis.py
from performance_benchmark import StatisticalAnalyzer

analyzer = StatisticalAnalyzer()

# åŠ è½½å®éªŒç»“æœ
results = analyzer.load_multiple_experiments([
    'results/method_A/',
    'results/method_B/',
    'results/method_C/'
])

# æ‰§è¡Œç»Ÿè®¡æ£€éªŒ
stats_results = analyzer.run_statistical_tests(
    results,
    metrics=['accuracy', 'f1_score'],
    test_type='anova',  # 'ttest', 'mannwhitney', 'anova'
    correction='bonferroni'  # 'fdr', 'bonferroni'
)

print(f"ANOVA p-value: {stats_results['p_value']:.6f}")
print(f"Effect size (etaÂ²): {stats_results['effect_size']:.4f}")
```

### ç½®ä¿¡åŒºé—´ä¼°ç®—
```python
# Bootstrapç½®ä¿¡åŒºé—´
ci_results = analyzer.bootstrap_confidence_intervals(
    results,
    metric='accuracy',
    n_bootstrap=1000,
    confidence_level=0.95
)

print(f"95% CI: [{ci_results['lower']:.4f}, {ci_results['upper']:.4f}]")
```

### å¤šé‡æ¯”è¾ƒåˆ†æ
```python
# Post-hocå¤šé‡æ¯”è¾ƒ
posthoc_results = analyzer.posthoc_analysis(
    results,
    method='tukey_hsd'  # 'bonferroni', 'holm', 'tukey_hsd'
)
```

## ğŸ“Š å¯è§†åŒ–åˆ†æ

### é«˜è´¨é‡å›¾è¡¨ç”Ÿæˆ
```bash
# IEEEè®ºæ–‡çº§å›¾è¡¨
python performance_benchmark.py \
    --generate_figures \
    --style ieee \
    --dpi 300 \
    --format pdf,png \
    --font_size 12

# è‡ªå®šä¹‰æ ·å¼å›¾è¡¨
python performance_benchmark.py \
    --generate_figures \
    --style_config custom_style.json \
    --color_palette viridis \
    --figure_size 10,6
```

### å¤šç»´åº¦å¯è§†åŒ–
```python
# visualization_tools.py
import matplotlib.pyplot as plt
import seaborn as sns
from performance_benchmark import VisualizationEngine

viz = VisualizationEngine()

# 1. æ€§èƒ½å¯¹æ¯”çƒ­å›¾
viz.plot_performance_heatmap(
    results_dict,
    metrics=['accuracy', 'f1_score'],
    methods=['Method_A', 'Method_B', 'Method_C'],
    datasets=['CWRU', 'XJTU', 'PU']
)

# 2. å‚æ•°æ•æ„Ÿæ€§å›¾
viz.plot_parameter_sensitivity(
    ablation_results,
    parameter='temperature',
    metric='accuracy',
    confidence_intervals=True
)

# 3. è®­ç»ƒåŠ¨æ€å›¾
viz.plot_training_dynamics(
    training_logs,
    metrics=['loss', 'accuracy'],
    comparison_baselines=['Random', 'Traditional_ML']
)
```

## ğŸ“ˆ åŸºå‡†æµ‹è¯•å¥—ä»¶

### æ ‡å‡†åŸºå‡†æµ‹è¯•
```bash
# è¿è¡Œå®Œæ•´åŸºå‡†å¥—ä»¶
python performance_benchmark.py \
    --benchmark_suite comprehensive \
    --include_baselines \
    --save_results benchmarks/comprehensive_benchmark.json

# å¿«é€ŸåŸºå‡†æµ‹è¯•
python performance_benchmark.py \
    --benchmark_suite quick \
    --essential_metrics_only
```

### è‡ªå®šä¹‰åŸºå‡†æµ‹è¯•
```python
# custom_benchmark.py
from performance_benchmark import BenchmarkSuite

# å®šä¹‰è‡ªå®šä¹‰åŸºå‡†
benchmark = BenchmarkSuite()

# æ·»åŠ åŸºçº¿æ–¹æ³•
benchmark.add_baseline('Random Classifier', random_classifier_results)
benchmark.add_baseline('SVM', svm_results)
benchmark.add_baseline('CNN', cnn_results)

# æ·»åŠ æµ‹è¯•æ–¹æ³•
benchmark.add_method('ContrastiveID', contrastive_results)

# è¿è¡Œå¯¹æ¯”
comparison = benchmark.run_comparison(
    metrics=['accuracy', 'precision', recall', 'f1_score'],
    statistical_tests=True,
    effect_size_calculation=True
)
```

## ğŸ” æ€§èƒ½æ´å¯ŸæŒ–æ˜

### è‡ªåŠ¨æ´å¯Ÿæå–
```python
# insight_extractor.py
from performance_benchmark import InsightExtractor

extractor = InsightExtractor()

# è‡ªåŠ¨æå–å…³é”®æ´å¯Ÿ
insights = extractor.extract_insights(experiment_results)

for insight in insights:
    print(f"ğŸ“Š {insight.category}: {insight.description}")
    print(f"   ç½®ä¿¡åº¦: {insight.confidence:.2f}")
    print(f"   æ”¯æ’‘æ•°æ®: {insight.evidence}")
```

### æ¨¡å¼è¯†åˆ«åˆ†æ
```python
# è¯†åˆ«æ€§èƒ½æ¨¡å¼
patterns = extractor.identify_patterns(
    results=experiment_results,
    pattern_types=['convergence', 'overfitting', 'underfitting', 'optimal_region']
)

# å¼‚å¸¸æ£€æµ‹
anomalies = extractor.detect_anomalies(
    results=experiment_results,
    threshold=2.0  # æ ‡å‡†å·®é˜ˆå€¼
)
```

## ğŸ“‹ åˆ†ææŠ¥å‘Šç”Ÿæˆ

### è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆ
```bash
# ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š
python performance_benchmark.py \
    --generate_report \
    --template analysis_template.html \
    --include_figures \
    --output_format html,pdf \
    --output analysis_report

# ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
python performance_benchmark.py \
    --executive_summary \
    --key_findings_only \
    --output executive_summary.pdf
```

### æŠ¥å‘Šå†…å®¹ç»“æ„
```python
# report_generator.py
report_sections = {
    'executive_summary': {
        'key_findings': [],
        'performance_highlights': [],
        'recommendations': []
    },
    'detailed_analysis': {
        'method_comparison': {},
        'statistical_analysis': {},
        'parameter_sensitivity': {}
    },
    'visualizations': {
        'performance_charts': [],
        'trend_analysis': [],
        'comparison_plots': []
    },
    'appendix': {
        'raw_data': {},
        'statistical_details': {},
        'configuration_files': []
    }
}
```

## ğŸ¯ åŸºå‡†å¯¹æ¯”å‚è€ƒ

### å·¥ä¸šæŒ¯åŠ¨åˆ†æåŸºå‡†
```python
# æ ‡å‡†åŸºå‡†æ€§èƒ½å‚è€ƒ
benchmark_references = {
    'CWRU': {
        'Random': 0.25,
        'Traditional_ML': 0.65,
        'CNN': 0.78,
        'LSTM': 0.75,
        'Transformer': 0.82,
        'ContrastiveID_Target': 0.85  # ç›®æ ‡æ€§èƒ½
    },
    'XJTU': {
        'Random': 0.20,
        'Traditional_ML': 0.58,
        'CNN': 0.71,
        'LSTM': 0.68,
        'Transformer': 0.76,
        'ContrastiveID_Target': 0.80
    }
}
```

### è·¨æ•°æ®é›†æ³›åŒ–åŸºå‡†
```python
# åŸŸæ³›åŒ–æ€§èƒ½å‚è€ƒ
domain_generalization_benchmarks = {
    'CWRUâ†’XJTU': {
        'Direct_Transfer': 0.35,
        'Fine_Tuning': 0.58,
        'Domain_Adaptation': 0.65,
        'ContrastiveID_Target': 0.70
    },
    'XJTUâ†’PU': {
        'Direct_Transfer': 0.32,
        'Fine_Tuning': 0.55,
        'Domain_Adaptation': 0.62,
        'ContrastiveID_Target': 0.68
    }
}
```

## ğŸ”§ é«˜çº§åˆ†ææŠ€æœ¯

### æ³¨æ„åŠ›å¯è§†åŒ–
```python
# attention_analysis.py
def analyze_attention_patterns(model, test_data):
    """åˆ†ææ¨¡å‹æ³¨æ„åŠ›æ¨¡å¼"""
    attention_weights = model.get_attention_weights(test_data)

    # æ—¶é—´ç»´åº¦æ³¨æ„åŠ›
    temporal_attention = attention_weights.mean(dim=1)

    # é¢‘ç‡ç»´åº¦æ³¨æ„åŠ›
    frequency_attention = fft_analysis(attention_weights)

    return {
        'temporal_patterns': temporal_attention,
        'frequency_patterns': frequency_attention
    }
```

### ç‰¹å¾è¡¨ç¤ºåˆ†æ
```python
# representation_analysis.py
def analyze_learned_representations(model, datasets):
    """åˆ†æå­¦ä¹ åˆ°çš„ç‰¹å¾è¡¨ç¤º"""

    # æå–ç‰¹å¾
    features = model.extract_features(datasets)

    # t-SNEå¯è§†åŒ–
    tsne_embedding = TSNE(n_components=2).fit_transform(features)

    # èšç±»åˆ†æ
    clustering_score = silhouette_score(features, labels)

    # ç‰¹å¾é‡è¦æ€§
    importance_scores = feature_importance_analysis(features, labels)

    return {
        'embeddings': tsne_embedding,
        'clustering_quality': clustering_score,
        'feature_importance': importance_scores
    }
```

## ğŸ¯ è¿›å…¥ä¸‹ä¸€é˜¶æ®µ

### åˆ†æè´¨é‡æ£€æŸ¥æ¸…å•
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆä¸”ç»“æœåˆç†
- [ ] ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒé€šè¿‡
- [ ] å…³é”®æ´å¯Ÿå·²æå–å¹¶éªŒè¯
- [ ] é«˜è´¨é‡å›¾è¡¨å·²ç”Ÿæˆ
- [ ] åˆ†ææŠ¥å‘Šå®Œæ•´ä¸”å‡†ç¡®

### åˆ†æç»“æœéªŒè¯
```bash
# éªŒè¯åˆ†æç»“æœçš„ä¸€è‡´æ€§
python validate_analysis.py \
    --analysis_results benchmarks/ \
    --cross_validation \
    --reproducibility_check

# ç”Ÿæˆåˆ†æè´¨é‡æŠ¥å‘Š
python analysis_quality_check.py \
    --results_dir benchmarks/ \
    --check_completeness \
    --validate_statistics
```

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨
```bash
# è¿›å…¥è®ºæ–‡æ”¯æ’‘é˜¶æ®µ
cd ../05_paper_support/

# å¼€å§‹å‡†å¤‡è®ºæ–‡ææ–™
python baseline_comparison.py --analysis_results ../04_analysis/benchmarks/
```

## ğŸ“š æ·±å…¥å­¦ä¹ èµ„æº

### ç»Ÿè®¡åˆ†ææ–¹æ³•
- **å‡è®¾æ£€éªŒ**: t-test, ANOVA, Mann-Whitney U
- **å¤šé‡æ¯”è¾ƒ**: Bonferroni, FDR, Tukey HSD
- **æ•ˆåº”é‡**: Cohen's d, eta squared, Cliff's delta
- **ç½®ä¿¡åŒºé—´**: Bootstrap, è´å¶æ–¯æ–¹æ³•

### å¯è§†åŒ–æœ€ä½³å®è·µ
- **é¢œè‰²é€‰æ‹©**: è‰²ç›²å‹å¥½è°ƒè‰²æ¿
- **å›¾è¡¨ç±»å‹**: æ ¹æ®æ•°æ®ç‰¹å¾é€‰æ‹©
- **ç»Ÿè®¡æ ‡æ³¨**: æ˜¾è‘—æ€§æ ‡è®°æ–¹æ³•
- **å›¾ä¾‹è®¾è®¡**: æ¸…æ™°ä¸”å®Œæ•´çš„æ ‡æ³¨

### æ€§èƒ½åˆ†æå·¥å…·
- **Profiling**: PyTorch Profiler, cProfile
- **å†…å­˜åˆ†æ**: memory_profiler, py-spy
- **GPUç›‘æ§**: nvidia-smi, gpustat
- **å¯è§†åŒ–**: matplotlib, seaborn, plotly

---

**ğŸ‰ æ­å–œï¼æ‚¨å·²æŒæ¡æ·±åº¦ç»“æœåˆ†ææŠ€èƒ½ã€‚**

æ•°æ®ä¸ä¼šè¯´è°ï¼Œä½†éœ€è¦æ­£ç¡®çš„æ–¹æ³•æ¥å€¾å¬å®ƒçš„å£°éŸ³ã€‚é€šè¿‡ä¸¥è°¨çš„åˆ†æï¼Œæ‚¨å°†è·å¾—æœ‰è¯´æœåŠ›çš„ç§‘å­¦æ´å¯Ÿã€‚

è®©æˆ‘ä»¬è¿›å…¥[è®ºæ–‡æ”¯æ’‘é˜¶æ®µ](../05_paper_support/README.md)å°†åˆ†æç»“æœè½¬åŒ–ä¸ºå­¦æœ¯æˆæœã€‚