#!/usr/bin/env python3
"""
æ•°æ®éªŒè¯å·¥å…· - ContrastiveIDTaskä¸“ç”¨

å…¨é¢éªŒè¯PHM-Vibenchæ•°æ®é›†çš„å®Œæ•´æ€§å’Œå…¼å®¹æ€§ï¼ŒåŒ…æ‹¬ï¼š
- Metadataæ–‡ä»¶æ ¼å¼éªŒè¯
- H5æ•°æ®æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
- æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯åˆ†æ
- IDåˆ†å¸ƒå’Œçª—å£ç”Ÿæˆå¯è¡Œæ€§éªŒè¯
- ContrastiveIDTaskå…¼å®¹æ€§æµ‹è¯•

Usage:
    # å¿«é€Ÿæ•°æ®éªŒè¯
    python data_validation.py --data_dir data

    # è¯¦ç»†éªŒè¯åŒ…å«ç»Ÿè®¡åˆ†æ
    python data_validation.py --data_dir data --detailed --stats

    # ä¿®å¤å¸¸è§æ•°æ®é—®é¢˜
    python data_validation.py --data_dir data --fix

Author: PHM-Vibench Team
Version: 1.0 (Data Quality Assurance)
"""

import os
import sys
import pandas as pd
import numpy as np
import h5py
from pathlib import Path
import argparse
import json
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import warnings
from collections import defaultdict, Counter
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

class DataValidator:
    """PHM-Vibenchæ•°æ®éªŒè¯å™¨

    ä¸“ä¸ºContrastiveIDTaskè®¾è®¡çš„æ•°æ®è´¨é‡ä¿è¯å·¥å…·ã€‚
    """

    def __init__(self, data_dir: str, detailed: bool = False, enable_fix: bool = False):
        self.data_dir = Path(data_dir)
        self.detailed = detailed
        self.enable_fix = enable_fix

        # éªŒè¯ç»“æœå­˜å‚¨
        self.validation_results = {
            'metadata_files': [],
            'h5_files': [],
            'dataset_statistics': {},
            'compatibility_tests': {},
            'issues_found': [],
            'recommendations': []
        }

        # æ•°æ®è´¨é‡æ ‡å‡†
        self.quality_standards = {
            'min_samples_per_dataset': 100,
            'min_ids_per_dataset': 10,
            'min_samples_per_id': 5,
            'max_missing_rate': 0.1,  # æœ€å¤§ç¼ºå¤±æ•°æ®æ¯”ä¾‹
            'min_signal_length': 1024,  # æœ€å°ä¿¡å·é•¿åº¦
            'required_metadata_columns': ['ID', 'Label', 'Sample_length']
        }

        print("ğŸ” PHM-Vibenchæ•°æ®éªŒè¯å·¥å…·")
        print(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"ğŸ“Š è¯¦ç»†æ¨¡å¼: {detailed}")
        print("=" * 60)

    def validate_metadata_files(self) -> List[Dict[str, Any]]:
        """éªŒè¯metadataæ–‡ä»¶"""
        print("ğŸ“‹ éªŒè¯Metadataæ–‡ä»¶...")

        metadata_results = []

        # æŸ¥æ‰¾metadataæ–‡ä»¶
        metadata_files = list(self.data_dir.glob("metadata_*.xlsx"))

        if not metadata_files:
            issue = "æœªæ‰¾åˆ°metadata_*.xlsxæ–‡ä»¶"
            self.validation_results['issues_found'].append(issue)
            print(f"âŒ {issue}")
            return metadata_results

        print(f"ğŸ“„ æ‰¾åˆ° {len(metadata_files)} ä¸ªmetadataæ–‡ä»¶")

        for metadata_file in metadata_files:
            print(f"\nğŸ“„ éªŒè¯: {metadata_file.name}")

            result = {
                'file_name': metadata_file.name,
                'file_path': str(metadata_file),
                'file_size_mb': metadata_file.stat().st_size / (1024 * 1024),
                'validation_status': 'unknown',
                'issues': [],
                'statistics': {}
            }

            try:
                # è¯»å–Excelæ–‡ä»¶
                df = pd.read_excel(metadata_file, sheet_name=0)
                result['total_samples'] = len(df)

                # æ£€æŸ¥å¿…éœ€çš„åˆ—
                missing_columns = [col for col in self.quality_standards['required_metadata_columns'] if col not in df.columns]
                if missing_columns:
                    issue = f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}"
                    result['issues'].append(issue)
                    print(f"  âŒ {issue}")

                # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
                if 'ID' in df.columns:
                    unique_ids = df['ID'].nunique()
                    result['statistics']['unique_ids'] = unique_ids
                    print(f"  ğŸ“Š å”¯ä¸€IDæ•°: {unique_ids}")

                    if unique_ids < self.quality_standards['min_ids_per_dataset']:
                        issue = f"IDæ•°é‡è¿‡å°‘: {unique_ids} < {self.quality_standards['min_ids_per_dataset']}"
                        result['issues'].append(issue)
                        print(f"  âš ï¸ {issue}")

                    # æ£€æŸ¥æ¯ä¸ªIDçš„æ ·æœ¬æ•°åˆ†å¸ƒ
                    id_counts = df['ID'].value_counts()
                    min_samples_per_id = id_counts.min()
                    result['statistics']['min_samples_per_id'] = int(min_samples_per_id)
                    result['statistics']['avg_samples_per_id'] = float(id_counts.mean())

                    if min_samples_per_id < self.quality_standards['min_samples_per_id']:
                        issue = f"æŸäº›IDæ ·æœ¬æ•°è¿‡å°‘: {min_samples_per_id} < {self.quality_standards['min_samples_per_id']}"
                        result['issues'].append(issue)
                        print(f"  âš ï¸ {issue}")

                # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
                if 'Label' in df.columns:
                    unique_labels = df['Label'].nunique()
                    result['statistics']['unique_labels'] = unique_labels
                    label_distribution = df['Label'].value_counts().to_dict()
                    result['statistics']['label_distribution'] = {str(k): int(v) for k, v in label_distribution.items()}
                    print(f"  ğŸ·ï¸ ç±»åˆ«æ•°: {unique_labels}")

                # æ£€æŸ¥ä¿¡å·é•¿åº¦
                if 'Sample_length' in df.columns:
                    avg_length = df['Sample_length'].mean()
                    min_length = df['Sample_length'].min()
                    max_length = df['Sample_length'].max()

                    result['statistics']['avg_signal_length'] = float(avg_length)
                    result['statistics']['min_signal_length'] = int(min_length)
                    result['statistics']['max_signal_length'] = int(max_length)

                    print(f"  ğŸ“ ä¿¡å·é•¿åº¦: å¹³å‡={avg_length:.0f}, èŒƒå›´=[{min_length}, {max_length}]")

                    if min_length < self.quality_standards['min_signal_length']:
                        issue = f"ä¿¡å·é•¿åº¦è¿‡çŸ­: {min_length} < {self.quality_standards['min_signal_length']}"
                        result['issues'].append(issue)
                        print(f"  âš ï¸ {issue}")

                # æ£€æŸ¥ç¼ºå¤±å€¼
                missing_data_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
                result['statistics']['missing_data_ratio'] = float(missing_data_ratio)

                if missing_data_ratio > self.quality_standards['max_missing_rate']:
                    issue = f"ç¼ºå¤±æ•°æ®è¿‡å¤š: {missing_data_ratio:.3f} > {self.quality_standards['max_missing_rate']}"
                    result['issues'].append(issue)
                    print(f"  âš ï¸ {issue}")

                # æ€»ä½“è¯„ä¼°
                if not result['issues']:
                    result['validation_status'] = 'pass'
                    print(f"  âœ… éªŒè¯é€šè¿‡")
                else:
                    result['validation_status'] = 'warning'
                    print(f"  âš ï¸ å‘ç° {len(result['issues'])} ä¸ªé—®é¢˜")

            except Exception as e:
                result['validation_status'] = 'fail'
                result['issues'].append(f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
                print(f"  âŒ è¯»å–å¤±è´¥: {e}")

            metadata_results.append(result)

        return metadata_results

    def validate_h5_files(self, metadata_results: List[Dict]) -> List[Dict[str, Any]]:
        """éªŒè¯H5æ•°æ®æ–‡ä»¶"""
        print(f"\nğŸ’¾ éªŒè¯H5æ•°æ®æ–‡ä»¶...")

        h5_results = []

        # æŸ¥æ‰¾H5æ–‡ä»¶
        h5_files = list(self.data_dir.glob("*.h5"))
        print(f"ğŸ“„ æ‰¾åˆ° {len(h5_files)} ä¸ªH5æ–‡ä»¶")

        if not h5_files:
            issue = "æœªæ‰¾åˆ°H5æ•°æ®æ–‡ä»¶"
            self.validation_results['issues_found'].append(issue)
            print(f"âŒ {issue}")
            return h5_results

        for h5_file in h5_files:
            print(f"\nğŸ’¾ éªŒè¯: {h5_file.name}")

            result = {
                'file_name': h5_file.name,
                'file_path': str(h5_file),
                'file_size_mb': h5_file.stat().st_size / (1024 * 1024),
                'validation_status': 'unknown',
                'issues': [],
                'statistics': {}
            }

            try:
                with h5py.File(h5_file, 'r') as f:
                    # æ£€æŸ¥æ–‡ä»¶ç»“æ„
                    keys = list(f.keys())
                    result['statistics']['top_level_keys'] = keys
                    print(f"  ğŸ”‘ é¡¶çº§é”®: {keys}")

                    # æ£€æŸ¥æ•°æ®é›†ç»“æ„ï¼ˆå‡è®¾ä½¿ç”¨æ ‡å‡†çš„PHM-Vibenchæ ¼å¼ï¼‰
                    total_size_mb = 0
                    dataset_count = 0

                    def visit_datasets(name, obj):
                        nonlocal total_size_mb, dataset_count
                        if isinstance(obj, h5py.Dataset):
                            dataset_count += 1
                            dataset_size = obj.size * obj.dtype.itemsize / (1024 * 1024)
                            total_size_mb += dataset_size

                    f.visititems(visit_datasets)

                    result['statistics']['dataset_count'] = dataset_count
                    result['statistics']['total_data_size_mb'] = float(total_size_mb)
                    print(f"  ğŸ“Š æ•°æ®é›†æ•°é‡: {dataset_count}")
                    print(f"  ğŸ“¦ æ•°æ®å¤§å°: {total_size_mb:.1f} MB")

                    # å°è¯•è¯»å–ä¸€å°éƒ¨åˆ†æ•°æ®éªŒè¯å¯è®¿é—®æ€§
                    sample_read_success = False
                    if keys:
                        try:
                            first_key = keys[0]
                            if isinstance(f[first_key], h5py.Dataset):
                                sample_data = f[first_key][:10] if len(f[first_key]) > 0 else f[first_key][:]
                                sample_read_success = True
                                result['statistics']['sample_shape'] = list(sample_data.shape)
                                result['statistics']['sample_dtype'] = str(sample_data.dtype)
                        except Exception as e:
                            result['issues'].append(f"æ— æ³•è¯»å–æ ·æœ¬æ•°æ®: {str(e)}")

                    if sample_read_success:
                        print(f"  âœ… æ•°æ®å¯è¯»å–")
                    else:
                        result['issues'].append("æ— æ³•è¯»å–æ•°æ®å†…å®¹")
                        print(f"  âŒ æ•°æ®è¯»å–å¤±è´¥")

                # æ€»ä½“è¯„ä¼°
                if not result['issues']:
                    result['validation_status'] = 'pass'
                    print(f"  âœ… éªŒè¯é€šè¿‡")
                else:
                    result['validation_status'] = 'warning'
                    print(f"  âš ï¸ å‘ç° {len(result['issues'])} ä¸ªé—®é¢˜")

            except Exception as e:
                result['validation_status'] = 'fail'
                result['issues'].append(f"æ‰“å¼€æ–‡ä»¶å¤±è´¥: {str(e)}")
                print(f"  âŒ æ‰“å¼€å¤±è´¥: {e}")

            h5_results.append(result)

        return h5_results

    def test_contrastive_compatibility(self, metadata_results: List[Dict]) -> Dict[str, Any]:
        """æµ‹è¯•ContrastiveIDTaskå…¼å®¹æ€§"""
        print(f"\nğŸ”¬ æµ‹è¯•ContrastiveIDTaskå…¼å®¹æ€§...")

        compatibility_result = {
            'overall_compatible': True,
            'tests': [],
            'recommendations': []
        }

        for metadata_result in metadata_results:
            if metadata_result['validation_status'] == 'fail':
                continue

            test_name = f"ContrastiveIDå…¼å®¹æ€§ - {metadata_result['file_name']}"
            test_result = {
                'test_name': test_name,
                'status': 'pass',
                'issues': []
            }

            stats = metadata_result.get('statistics', {})

            # æ£€æŸ¥IDæ•°é‡
            unique_ids = stats.get('unique_ids', 0)
            if unique_ids < 10:
                test_result['status'] = 'warning'
                test_result['issues'].append(f"IDæ•°é‡è¾ƒå°‘({unique_ids})ï¼Œå¯èƒ½å½±å“å¯¹æ¯”å­¦ä¹ æ•ˆæœ")

            # æ£€æŸ¥æ¯IDæ ·æœ¬æ•°
            min_samples_per_id = stats.get('min_samples_per_id', 0)
            if min_samples_per_id < 2:
                test_result['status'] = 'fail'
                test_result['issues'].append(f"æŸäº›IDæ ·æœ¬æ•°ä¸è¶³2ä¸ªï¼Œæ— æ³•ç”Ÿæˆæ­£æ ·æœ¬å¯¹")
                compatibility_result['overall_compatible'] = False

            # æ£€æŸ¥ä¿¡å·é•¿åº¦
            min_signal_length = stats.get('min_signal_length', 0)
            if min_signal_length < 1024:
                test_result['status'] = 'warning'
                test_result['issues'].append(f"ä¿¡å·é•¿åº¦è¾ƒçŸ­({min_signal_length})ï¼Œå»ºè®®çª—å£å¤§å°â‰¤{min_signal_length//2}")

            # è¯„ä¼°çª—å£ç”Ÿæˆå¯è¡Œæ€§
            avg_signal_length = stats.get('avg_signal_length', 0)
            if avg_signal_length > 0:
                recommended_window_sizes = []
                for window_size in [256, 512, 1024, 2048]:
                    if avg_signal_length >= window_size * 2:  # èƒ½ç”Ÿæˆè‡³å°‘2ä¸ªçª—å£
                        recommended_window_sizes.append(window_size)

                if recommended_window_sizes:
                    compatibility_result['recommendations'].append(
                        f"{metadata_result['file_name']}: æ¨èçª—å£å¤§å° {recommended_window_sizes}"
                    )
                else:
                    test_result['status'] = 'warning'
                    test_result['issues'].append("ä¿¡å·é•¿åº¦ä¸è¶³ä»¥ç”Ÿæˆæ ‡å‡†çª—å£")

            if test_result['status'] == 'pass':
                print(f"  âœ… {test_name}: å…¼å®¹")
            elif test_result['status'] == 'warning':
                print(f"  âš ï¸ {test_name}: éƒ¨åˆ†å…¼å®¹ï¼Œæœ‰å»ºè®®")
                for issue in test_result['issues']:
                    print(f"     â€¢ {issue}")
            else:
                print(f"  âŒ {test_name}: ä¸å…¼å®¹")
                for issue in test_result['issues']:
                    print(f"     â€¢ {issue}")

            compatibility_result['tests'].append(test_result)

        return compatibility_result

    def generate_statistics(self, metadata_results: List[Dict], h5_results: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if not self.detailed:
            return {}

        print(f"\nğŸ“Š ç”Ÿæˆæ•°æ®ç»Ÿè®¡...")

        statistics = {
            'summary': {},
            'datasets': {},
            'data_quality_score': 0.0
        }

        # æ€»ä½“ç»Ÿè®¡
        total_samples = sum(r.get('total_samples', 0) for r in metadata_results)
        total_ids = sum(r.get('statistics', {}).get('unique_ids', 0) for r in metadata_results)
        total_datasets = len([r for r in metadata_results if r['validation_status'] != 'fail'])

        statistics['summary'] = {
            'total_datasets': total_datasets,
            'total_samples': total_samples,
            'total_unique_ids': total_ids,
            'avg_samples_per_dataset': total_samples / max(1, total_datasets),
            'avg_ids_per_dataset': total_ids / max(1, total_datasets)
        }

        # æ•°æ®é›†è¯¦ç»†ç»Ÿè®¡
        for metadata_result in metadata_results:
            if metadata_result['validation_status'] == 'fail':
                continue

            dataset_name = metadata_result['file_name'].replace('metadata_', '').replace('.xlsx', '')
            statistics['datasets'][dataset_name] = metadata_result['statistics']

        # è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°
        quality_factors = []

        # å› å­1: å®Œæ•´æ€§ï¼ˆæ˜¯å¦æœ‰ç¼ºå¤±æ–‡ä»¶ï¼‰
        metadata_count = len([r for r in metadata_results if r['validation_status'] != 'fail'])
        h5_count = len([r for r in h5_results if r['validation_status'] != 'fail'])
        completeness_score = min(1.0, (metadata_count + h5_count) / (2 * len(metadata_results)))
        quality_factors.append(('completeness', completeness_score, 0.3))

        # å› å­2: æ•°æ®é‡å……è¶³æ€§
        avg_samples = statistics['summary']['avg_samples_per_dataset']
        sample_adequacy = min(1.0, avg_samples / self.quality_standards['min_samples_per_dataset'])
        quality_factors.append(('sample_adequacy', sample_adequacy, 0.3))

        # å› å­3: IDåˆ†å¸ƒåˆç†æ€§
        avg_ids = statistics['summary']['avg_ids_per_dataset']
        id_adequacy = min(1.0, avg_ids / self.quality_standards['min_ids_per_dataset'])
        quality_factors.append(('id_adequacy', id_adequacy, 0.2))

        # å› å­4: æ•°æ®è´¨é‡ï¼ˆæ— é”™è¯¯ï¼‰
        total_issues = sum(len(r['issues']) for r in metadata_results + h5_results)
        error_penalty = max(0.0, 1.0 - total_issues / 10)  # æ¯10ä¸ªé—®é¢˜æ‰£é™¤100%
        quality_factors.append(('error_penalty', error_penalty, 0.2))

        # è®¡ç®—åŠ æƒå¹³å‡åˆ†
        total_score = sum(score * weight for _, score, weight in quality_factors)
        statistics['data_quality_score'] = total_score

        print(f"ğŸ“ˆ æ•°æ®è´¨é‡è¯„åˆ†: {total_score:.2f}/1.00")
        for factor_name, score, weight in quality_factors:
            print(f"   â€¢ {factor_name}: {score:.2f} (æƒé‡: {weight:.1f})")

        return statistics

    def create_visualizations(self, metadata_results: List[Dict], statistics: Dict):
        """åˆ›å»ºæ•°æ®å¯è§†åŒ–"""
        if not self.detailed or not statistics:
            return

        print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('Data Validation Report', fontsize=16)

            # 1. æ•°æ®é›†æ ·æœ¬æ•°åˆ†å¸ƒ
            dataset_names = []
            sample_counts = []

            for result in metadata_results:
                if result['validation_status'] != 'fail':
                    name = result['file_name'].replace('metadata_', '').replace('.xlsx', '')
                    count = result.get('total_samples', 0)
                    dataset_names.append(name)
                    sample_counts.append(count)

            if dataset_names:
                axes[0, 0].bar(range(len(dataset_names)), sample_counts)
                axes[0, 0].set_title('Samples per Dataset')
                axes[0, 0].set_xlabel('Dataset')
                axes[0, 0].set_ylabel('Sample Count')
                axes[0, 0].set_xticks(range(len(dataset_names)))
                axes[0, 0].set_xticklabels(dataset_names, rotation=45, ha='right')

            # 2. IDæ•°é‡åˆ†å¸ƒ
            id_counts = [result.get('statistics', {}).get('unique_ids', 0) for result in metadata_results
                        if result['validation_status'] != 'fail']

            if id_counts:
                axes[0, 1].bar(range(len(dataset_names)), id_counts)
                axes[0, 1].set_title('Unique IDs per Dataset')
                axes[0, 1].set_xlabel('Dataset')
                axes[0, 1].set_ylabel('Unique ID Count')
                axes[0, 1].set_xticks(range(len(dataset_names)))
                axes[0, 1].set_xticklabels(dataset_names, rotation=45, ha='right')

            # 3. ä¿¡å·é•¿åº¦åˆ†å¸ƒ
            signal_lengths = []
            for result in metadata_results:
                if result['validation_status'] != 'fail':
                    avg_length = result.get('statistics', {}).get('avg_signal_length', 0)
                    if avg_length > 0:
                        signal_lengths.append(avg_length)

            if signal_lengths:
                axes[1, 0].hist(signal_lengths, bins=20, alpha=0.7)
                axes[1, 0].set_title('Signal Length Distribution')
                axes[1, 0].set_xlabel('Average Signal Length')
                axes[1, 0].set_ylabel('Frequency')

            # 4. æ•°æ®è´¨é‡è¯„åˆ†é›·è¾¾å›¾ï¼ˆç®€åŒ–ä¸ºæ¡å½¢å›¾ï¼‰
            quality_aspects = ['Completeness', 'Sample Adequacy', 'ID Adequacy', 'Error-free']
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä»quality_factorsä¸­æå–
            quality_scores = [0.9, 0.8, 0.7, 0.85]  # ç¤ºä¾‹åˆ†æ•°

            axes[1, 1].bar(quality_aspects, quality_scores)
            axes[1, 1].set_title('Data Quality Aspects')
            axes[1, 1].set_ylabel('Score')
            axes[1, 1].set_ylim(0, 1)
            axes[1, 1].tick_params(axis='x', rotation=45)

            plt.tight_layout()

            # ä¿å­˜å›¾è¡¨
            plot_file = Path(__file__).parent / f"data_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            plt.savefig(plot_file, dpi=150, bbox_inches='tight')
            print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜: {plot_file}")

            plt.close()

        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")

    def generate_recommendations(self, metadata_results: List[Dict], h5_results: List[Dict],
                               compatibility_result: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºéªŒè¯ç»“æœç”Ÿæˆå»ºè®®
        failed_metadata = [r for r in metadata_results if r['validation_status'] == 'fail']
        if failed_metadata:
            recommendations.append(f"ä¿®å¤ {len(failed_metadata)} ä¸ªæŸåçš„metadataæ–‡ä»¶")

        failed_h5 = [r for r in h5_results if r['validation_status'] == 'fail']
        if failed_h5:
            recommendations.append(f"ä¿®å¤ {len(failed_h5)} ä¸ªæŸåçš„H5æ–‡ä»¶")

        # æ•°æ®é‡å»ºè®®
        low_sample_datasets = [r for r in metadata_results
                              if r.get('total_samples', 0) < self.quality_standards['min_samples_per_dataset']]
        if low_sample_datasets:
            recommendations.append(f"å¢åŠ  {len(low_sample_datasets)} ä¸ªæ•°æ®é›†çš„æ ·æœ¬æ•°é‡")

        # ContrastiveIDTaskç‰¹å®šå»ºè®®
        if not compatibility_result.get('overall_compatible', True):
            recommendations.append("è§£å†³ContrastiveIDTaskå…¼å®¹æ€§é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªIDè‡³å°‘æœ‰2ä¸ªæ ·æœ¬")

        # çª—å£å¤§å°å»ºè®®
        short_signal_datasets = [r for r in metadata_results
                               if r.get('statistics', {}).get('min_signal_length', float('inf')) < 1024]
        if short_signal_datasets:
            recommendations.append("å¯¹äºçŸ­ä¿¡å·æ•°æ®é›†ï¼Œè€ƒè™‘ä½¿ç”¨è¾ƒå°çš„çª—å£å¤§å°ï¼ˆ256æˆ–512ï¼‰")

        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        large_datasets = [r for r in metadata_results if r.get('total_samples', 0) > 10000]
        if large_datasets:
            recommendations.append("å¤§å‹æ•°æ®é›†å»ºè®®ä½¿ç”¨æ•°æ®å¹¶è¡Œè®­ç»ƒä»¥æé«˜æ•ˆç‡")

        return recommendations

    def run_complete_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®éªŒè¯"""
        print("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®éªŒè¯...\n")

        start_time = datetime.now()

        # 1. éªŒè¯metadataæ–‡ä»¶
        metadata_results = self.validate_metadata_files()
        self.validation_results['metadata_files'] = metadata_results

        # 2. éªŒè¯H5æ–‡ä»¶
        h5_results = self.validate_h5_files(metadata_results)
        self.validation_results['h5_files'] = h5_results

        # 3. å…¼å®¹æ€§æµ‹è¯•
        compatibility_result = self.test_contrastive_compatibility(metadata_results)
        self.validation_results['compatibility_tests'] = compatibility_result

        # 4. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        statistics = self.generate_statistics(metadata_results, h5_results)
        self.validation_results['dataset_statistics'] = statistics

        # 5. åˆ›å»ºå¯è§†åŒ–
        if self.detailed:
            self.create_visualizations(metadata_results, statistics)

        # 6. ç”Ÿæˆå»ºè®®
        recommendations = self.generate_recommendations(metadata_results, h5_results, compatibility_result)
        self.validation_results['recommendations'] = recommendations

        # 7. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        end_time = datetime.now()
        validation_duration = (end_time - start_time).total_seconds()

        print(f"\n{'='*60}")
        print("ğŸ“‹ æ•°æ®éªŒè¯æ€»ç»“")
        print(f"{'='*60}")

        # æ€»ä½“çŠ¶æ€
        total_issues = len(self.validation_results['issues_found'])
        metadata_passed = len([r for r in metadata_results if r['validation_status'] == 'pass'])
        h5_passed = len([r for r in h5_results if r['validation_status'] == 'pass'])

        print(f"â±ï¸  éªŒè¯è€—æ—¶: {validation_duration:.1f}ç§’")
        print(f"ğŸ“„ Metadataæ–‡ä»¶: {metadata_passed}/{len(metadata_results)} é€šè¿‡")
        print(f"ğŸ’¾ H5æ–‡ä»¶: {h5_passed}/{len(h5_results)} é€šè¿‡")
        print(f"ğŸ”¬ ContrastiveIDå…¼å®¹æ€§: {'âœ… å…¼å®¹' if compatibility_result['overall_compatible'] else 'âŒ ä¸å…¼å®¹'}")

        if statistics and 'data_quality_score' in statistics:
            quality_score = statistics['data_quality_score']
            print(f"ğŸ† æ•°æ®è´¨é‡è¯„åˆ†: {quality_score:.2f}/1.00")

            if quality_score >= 0.8:
                print("ğŸ‰ æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå¯ç›´æ¥ç”¨äºè®­ç»ƒ")
            elif quality_score >= 0.6:
                print("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®å…³æ³¨éƒ¨åˆ†é—®é¢˜")
            else:
                print("âš ï¸ æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›")

        # æ˜¾ç¤ºå»ºè®®
        if recommendations:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")

        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        self.validation_results['validation_summary'] = {
            'timestamp': end_time.isoformat(),
            'duration_seconds': validation_duration,
            'total_metadata_files': len(metadata_results),
            'passed_metadata_files': metadata_passed,
            'total_h5_files': len(h5_results),
            'passed_h5_files': h5_passed,
            'contrastive_compatible': compatibility_result['overall_compatible'],
            'data_quality_score': statistics.get('data_quality_score', 0.0) if statistics else 0.0
        }

        report_file = Path(__file__).parent / f"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(self.validation_results, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        return self.validation_results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="PHM-Vibenchæ•°æ®éªŒè¯å·¥å…·")

    parser.add_argument('--data_dir', default='data',
                       help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--detailed', action='store_true',
                       help='è¯¦ç»†éªŒè¯åŒ…å«ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–')
    parser.add_argument('--stats', action='store_true',
                       help='ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--fix', action='store_true',
                       help='è‡ªåŠ¨ä¿®å¤å¸¸è§é—®é¢˜ï¼ˆåŠŸèƒ½å¼€å‘ä¸­ï¼‰')

    args = parser.parse_args()

    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print(f"ğŸ’¡ è¯·ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶åŒ…å«metadata_*.xlsxå’Œ*.h5æ–‡ä»¶")
        return 1

    # åˆ›å»ºéªŒè¯å™¨
    validator = DataValidator(
        data_dir=str(data_dir),
        detailed=args.detailed or args.stats,
        enable_fix=args.fix
    )

    try:
        # è¿è¡ŒéªŒè¯
        results = validator.run_complete_validation()

        # æ ¹æ®ç»“æœè¿”å›é€‚å½“çš„é€€å‡ºç 
        compatibility_ok = results['compatibility_tests']['overall_compatible']
        quality_score = results['dataset_statistics'].get('data_quality_score', 0.0)

        if compatibility_ok and quality_score >= 0.6:
            print(f"\nğŸ‰ æ•°æ®éªŒè¯æˆåŠŸï¼æ•°æ®å·²å‡†å¤‡å°±ç»ªã€‚")
            return 0
        elif compatibility_ok:
            print(f"\nâš ï¸ æ•°æ®åŸºæœ¬å¯ç”¨ï¼Œä½†å»ºè®®æ”¹è¿›è´¨é‡ã€‚")
            return 0
        else:
            print(f"\nâŒ æ•°æ®å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œéœ€è¦ä¿®å¤åæ‰èƒ½ä½¿ç”¨ã€‚")
            return 1

    except KeyboardInterrupt:
        print(f"\nâš ï¸ éªŒè¯è¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        print(f"\nâŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())