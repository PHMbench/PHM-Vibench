# é˜¶æ®µ2: æ•°æ®å‡†å¤‡æŒ‡å—

å·¥ä¸šæŒ¯åŠ¨æ•°æ®é›†å‡†å¤‡ã€éªŒè¯å’Œé¢„å¤„ç†çš„å®Œæ•´æŒ‡å—ã€‚

## ğŸ“‹ æœ¬é˜¶æ®µç›®æ ‡

- [x] éªŒè¯æ•°æ®é›†å®Œæ•´æ€§å’Œæ ¼å¼
- [x] æ£€æŸ¥ContrastiveIDTaskå…¼å®¹æ€§
- [x] åˆ†ææ•°æ®è´¨é‡å’Œåˆ†å¸ƒ
- [x] ç”Ÿæˆæ•°æ®å‡†å¤‡æŠ¥å‘Š

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å•æ•°æ®é›†éªŒè¯
```bash
python data_validation.py --dataset CWRU --metadata_path /path/to/metadata_CWRU.xlsx
```

### 2. å¤šæ•°æ®é›†æ‰¹é‡éªŒè¯
```bash
# éªŒè¯å¤šä¸ªæ•°æ®é›†
for dataset in CWRU XJTU PU FEMTO; do
    python data_validation.py --dataset $dataset --quick
done
```

### 3. å¿«é€Ÿå…¼å®¹æ€§æ£€æŸ¥
```bash
python data_validation.py --dataset CWRU --compatibility_only
```

## ğŸ› ï¸ æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### data_validation.py
**ä¸»è¦åŠŸèƒ½**: å…¨é¢çš„æ•°æ®é›†éªŒè¯å’Œåˆ†æå·¥å…·

#### åŸºæœ¬éªŒè¯
```bash
# æ ‡å‡†éªŒè¯æµç¨‹
python data_validation.py --dataset CWRU

# å¿«é€Ÿæ£€æŸ¥ï¼ˆè·³è¿‡è¯¦ç»†åˆ†æï¼‰
python data_validation.py --dataset CWRU --quick

# åªæ£€æŸ¥å…¼å®¹æ€§
python data_validation.py --dataset CWRU --compatibility_only
```

#### é«˜çº§åˆ†æ
```bash
# è¯¦ç»†æ•°æ®åˆ†æ
python data_validation.py --dataset CWRU --analyze --verbose

# ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
python data_validation.py --dataset CWRU --visualize --output_dir reports/

# å¤šæ•°æ®é›†å¯¹æ¯”åˆ†æ
python data_validation.py --datasets CWRU,XJTU,PU --compare
```

## ğŸ“Š éªŒè¯æ£€æŸ¥é¡¹ç›®

### ğŸ” åŸºç¡€å®Œæ•´æ€§æ£€æŸ¥
- [x] **H5æ–‡ä»¶å­˜åœ¨æ€§**: æ£€æŸ¥æ‰€æœ‰H5æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
- [x] **å…ƒæ•°æ®ä¸€è‡´æ€§**: éªŒè¯metadata.xlsxä¸å®é™…æ•°æ®åŒ¹é…
- [x] **æ–‡ä»¶å®Œæ•´æ€§**: æ£€æŸ¥H5æ–‡ä»¶æ˜¯å¦å¯æ­£å¸¸è¯»å–
- [x] **æ•°æ®ç»“æ„**: éªŒè¯ä¿¡å·ç»´åº¦å’Œæ ¼å¼

### ğŸ“ˆ æ•°æ®è´¨é‡åˆ†æ
- [x] **ä¿¡å·é•¿åº¦åˆ†å¸ƒ**: åˆ†ææ—¶é—´åºåˆ—é•¿åº¦ç»Ÿè®¡
- [x] **é€šé“æ•°éªŒè¯**: ç¡®è®¤ä¿¡å·é€šé“æ•°ä¸€è‡´æ€§
- [x] **æ•°å€¼èŒƒå›´æ£€æŸ¥**: æ£€æµ‹å¼‚å¸¸å€¼å’Œæ•°æ®èŒƒå›´
- [x] **æ ‡ç­¾åˆ†å¸ƒ**: åˆ†æç±»åˆ«æ ‡ç­¾çš„å¹³è¡¡æ€§

### ğŸ§ª ContrastiveIDTaskå…¼å®¹æ€§
- [x] **æœ€å°é•¿åº¦éªŒè¯**: ç¡®ä¿ä¿¡å·é•¿åº¦â‰¥window_size
- [x] **çª—å£é‡‡æ ·æµ‹è¯•**: éªŒè¯çª—å£ç”ŸæˆåŠŸèƒ½
- [x] **æ‰¹å¤„ç†æµ‹è¯•**: æµ‹è¯•æ‰¹æ¬¡å‡†å¤‡æµç¨‹
- [x] **å†…å­˜éœ€æ±‚ä¼°ç®—**: é¢„ä¼°è®­ç»ƒå†…å­˜å ç”¨

## ğŸ“‹ éªŒè¯æŠ¥å‘Šè§£è¯»

### âœ… æ­£å¸¸è¾“å‡ºç¤ºä¾‹
```
ğŸ“Š æ•°æ®é›†åˆ†ææŠ¥å‘Š: CWRU
=====================================
âœ… åŸºç¡€æ£€æŸ¥:
   - H5æ–‡ä»¶: 2,400ä¸ª (100%å®Œæ•´)
   - å…ƒæ•°æ®åŒ¹é…: 2,400/2,400
   - å¹³å‡ä¿¡å·é•¿åº¦: 121,945 Â± 15,234
   - é€šé“æ•°: 2 (ä¸€è‡´)

âœ… è´¨é‡åˆ†æ:
   - æ•°å€¼èŒƒå›´: [-5.23, 4.87] (æ­£å¸¸)
   - å¼‚å¸¸å€¼æ¯”ä¾‹: 0.02% (å¯æ¥å—)
   - æ ‡ç­¾åˆ†å¸ƒ: å‡è¡¡ (æœ€å¤§åå·®<10%)

âœ… ContrastiveIDTaskå…¼å®¹æ€§:
   - æœ€å°é•¿åº¦: 8,192 > window_size(256) âœ“
   - çª—å£é‡‡æ ·: æˆåŠŸç‡100%
   - æ‰¹å¤„ç†æµ‹è¯•: é€šè¿‡
   - é¢„ä¼°å†…å­˜: 1.2GB (batch_size=32)

ğŸ‰ æ•°æ®é›†éªŒè¯é€šè¿‡ï¼Œå¯ç”¨äºContrastiveIDTaskè®­ç»ƒ
```

### âš ï¸ é—®é¢˜æŠ¥å‘Šç¤ºä¾‹
```
âš ï¸ æ•°æ®é›†é—®é¢˜æŠ¥å‘Š: EXAMPLE_DATASET
=====================================
âŒ å‘ç°é—®é¢˜:
   1. ç¼ºå¤±H5æ–‡ä»¶: 15ä¸ªæ ·æœ¬æ— å¯¹åº”æ•°æ®æ–‡ä»¶
   2. é•¿åº¦ä¸è¶³æ ·æœ¬: 23ä¸ªæ ·æœ¬ < æœ€å°çª—å£å¤§å°
   3. å¼‚å¸¸å€¼è¿‡å¤š: Channel_1ä¸­8.3%æ•°æ®ä¸ºNaN
   4. æ ‡ç­¾ä¸å¹³è¡¡: Class_0å 83.2%ï¼Œå»ºè®®é‡æ–°å¹³è¡¡

ğŸ“ ä¿®å¤å»ºè®®:
   - è¡¥å……ç¼ºå¤±çš„H5æ–‡ä»¶æˆ–ä»å…ƒæ•°æ®ä¸­ç§»é™¤
   - è°ƒæ•´window_sizeâ‰¤2048é€‚åº”çŸ­ä¿¡å·
   - æ¸…ç†æˆ–æ’å€¼å¤„ç†NaNå€¼
   - è€ƒè™‘æ•°æ®å¢å¼ºæˆ–é‡é‡‡æ ·å¹³è¡¡æ ‡ç­¾
```

## ğŸ”§ é«˜çº§åŠŸèƒ½ä½¿ç”¨

### æ‰¹é‡æ•°æ®é›†å¤„ç†
```bash
# åˆ›å»ºæ•°æ®é›†å¤„ç†è„šæœ¬
cat > validate_all.sh << EOF
#!/bin/bash
datasets=("CWRU" "XJTU" "PU" "FEMTO" "IMS" "MFPT")
for dataset in "\${datasets[@]}"; do
    echo "éªŒè¯æ•°æ®é›†: \$dataset"
    python data_validation.py --dataset \$dataset --analyze --output_dir "reports/\$dataset/"
done
EOF

chmod +x validate_all.sh
./validate_all.sh
```

### è‡ªå®šä¹‰éªŒè¯è§„åˆ™
```python
# custom_validation.py
from data_validation import DatasetValidator

# åˆ›å»ºè‡ªå®šä¹‰éªŒè¯å™¨
validator = DatasetValidator(
    min_signal_length=512,
    max_nan_ratio=0.05,
    required_channels=2,
    min_samples_per_class=100
)

# è¿è¡Œè‡ªå®šä¹‰éªŒè¯
result = validator.validate_dataset("CUSTOM_DATASET")
print(result.summary())
```

### è·¨æ•°æ®é›†å…¼å®¹æ€§åˆ†æ
```bash
# åˆ†æå¤šæ•°æ®é›†çš„å…¼å®¹æ€§
python data_validation.py \
    --datasets CWRU,XJTU,PU \
    --cross_compatibility \
    --output_report cross_dataset_analysis.json
```

## ğŸ“Š æ•°æ®é¢„å¤„ç†å»ºè®®

### ä¿¡å·é•¿åº¦æ ‡å‡†åŒ–
```python
# æ ¹æ®åˆ†æç»“æœè°ƒæ•´å‚æ•°
recommended_config = {
    'window_size': 256,    # åŸºäºæœ€çŸ­ä¿¡å·é•¿åº¦
    'stride': 128,         # 50%é‡å 
    'truncate_length': 4096,  # åŸºäº95%åˆ†ä½æ•°
}
```

### æ‰¹å¤§å°ä¼˜åŒ–
```bash
# åŸºäºå†…å­˜åˆ†æè°ƒæ•´æ‰¹å¤§å°
python data_validation.py --dataset CWRU --memory_analysis --batch_sizes 8,16,32,64
```

**è¾“å‡ºç¤ºä¾‹**:
```
ğŸ’¾ å†…å­˜ä½¿ç”¨åˆ†æ:
   batch_size=8:  0.3GB (æ¨è)
   batch_size=16: 0.6GB (æ¨è)
   batch_size=32: 1.2GB (å¯è¡Œ)
   batch_size=64: 2.4GB (éœ€è¦>4GB GPU)
```

## ğŸ¯ æ•°æ®è´¨é‡ä¼˜åŒ–

### å¼‚å¸¸å€¼å¤„ç†
```python
# æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼
from data_validation import DataCleaner

cleaner = DataCleaner()

# æ£€æµ‹å¼‚å¸¸å€¼
outliers = cleaner.detect_outliers(dataset_path, method='iqr')

# æ¸…ç†ç­–ç•¥é€‰æ‹©
clean_dataset = cleaner.clean(
    dataset_path,
    strategy='interpolate',  # 'remove', 'interpolate', 'clip'
    outlier_threshold=3.0
)
```

### æ•°æ®å¢å¼ºå»ºè®®
```python
# æ ¹æ®åˆ†æç»“æœåˆ¶å®šå¢å¼ºç­–ç•¥
augmentation_config = {
    'noise_injection': 0.02,      # åŸºäºSNRåˆ†æ
    'time_warping': 0.1,          # åŸºäºé•¿åº¦å˜å¼‚æ€§
    'frequency_masking': 0.15,     # åŸºäºé¢‘åŸŸç‰¹å¾
    'mixup_alpha': 0.2            # æ ‡ç­¾å¹³è¡¡ç­–ç•¥
}
```

## ğŸ” æ•…éšœæ’é™¤

### âŒ H5æ–‡ä»¶è¯»å–å¤±è´¥
```bash
# æ£€æŸ¥H5æ–‡ä»¶å®Œæ•´æ€§
python -c "
import h5py
try:
    with h5py.File('problem_file.h5', 'r') as f:
        print(f'Keys: {list(f.keys())}')
except Exception as e:
    print(f'Error: {e}')
"

# ä¿®å¤æŸåçš„H5æ–‡ä»¶
python data_validation.py --dataset DATASET --repair_h5
```

### âŒ å†…å­˜ä¸è¶³
```bash
# ä½¿ç”¨æµå¼å¤„ç†æ¨¡å¼
python data_validation.py --dataset LARGE_DATASET --streaming --chunk_size 100
```

### âŒ å…ƒæ•°æ®ä¸åŒ¹é…
```bash
# ç”Ÿæˆæ–°çš„å…ƒæ•°æ®æ–‡ä»¶
python data_validation.py --dataset DATASET --generate_metadata --output metadata_new.xlsx
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### å¹¶è¡Œå¤„ç†
```bash
# å¤šè¿›ç¨‹éªŒè¯
python data_validation.py --dataset LARGE_DATASET --parallel --num_workers 4

# GPUåŠ é€Ÿåˆ†æ
python data_validation.py --dataset DATASET --use_gpu --gpu_id 0
```

### ç¼“å­˜æœºåˆ¶
```python
# å¯ç”¨éªŒè¯ç»“æœç¼“å­˜
export DATA_VALIDATION_CACHE=1
python data_validation.py --dataset DATASET  # é¦–æ¬¡è¿è¡Œï¼Œå»ºç«‹ç¼“å­˜
python data_validation.py --dataset DATASET  # åç»­è¿è¡Œä½¿ç”¨ç¼“å­˜
```

## ğŸ¯ è¿›å…¥ä¸‹ä¸€é˜¶æ®µ

### æ£€æŸ¥æ¸…å•
- [ ] æ‰€æœ‰ç›®æ ‡æ•°æ®é›†éªŒè¯é€šè¿‡
- [ ] ContrastiveIDTaskå…¼å®¹æ€§ç¡®è®¤
- [ ] æ•°æ®è´¨é‡é—®é¢˜å·²ä¿®å¤
- [ ] æœ€ä¼˜å‚æ•°é…ç½®å·²ç¡®å®š

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨
```bash
# è¿›å…¥å®éªŒæ‰§è¡Œé˜¶æ®µ
cd ../03_experiments/

# ä½¿ç”¨éªŒè¯åçš„é…ç½®è¿è¡Œå®éªŒ
python multi_dataset_runner.py \
    --datasets CWRU \
    --config validated_config.yaml
```

## ğŸ“š æ·±å…¥å­¦ä¹ 

### æ•°æ®é›†ç‰¹æ€§å‚è€ƒ
| æ•°æ®é›† | æ ·æœ¬æ•° | ä¿¡å·é•¿åº¦ | é€šé“æ•° | æ•…éšœç±»å‹ | ç‰¹ç‚¹ |
|--------|--------|----------|--------|----------|------|
| CWRU | 2,400 | ~120K | 2 | 4 | æ ‡å‡†åŸºå‡†æ•°æ®é›† |
| XJTU | 15,000 | ~32K | 2 | 5 | çœŸå®å·¥å†µæ•°æ® |
| PU | 26,400 | ~64K | 2 | 6 | å¤šå·¥å†µç»„åˆ |
| FEMTO | 17,000 | ~2.5K | 2 | 3 | åŠ é€Ÿå¯¿å‘½è¯•éªŒ |

### ç›¸å…³æŠ€æœ¯æ–‡æ¡£
- [H5DataDictæ–‡æ¡£](../docs/technical_guide.md#h5datadict) - æ•°æ®åŠ è½½æœºåˆ¶
- [BaseReaderæ¨¡å¼](../docs/technical_guide.md#basereader) - æ•°æ®è¯»å–å™¨
- [æ•°æ®å·¥å‚æ¶æ„](../docs/technical_guide.md#data-factory) - æ•´ä½“æ•°æ®å¤„ç†

---

**ğŸ‰ æ­å–œï¼æ‚¨çš„æ•°æ®å·²å‡†å¤‡å°±ç»ªã€‚**

æ•°æ®è´¨é‡ç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½ï¼Œå¥½çš„å¼€å§‹æ˜¯æˆåŠŸçš„ä¸€åŠï¼

è®©æˆ‘ä»¬è¿›å…¥[å®éªŒæ‰§è¡Œé˜¶æ®µ](../03_experiments/README.md)å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚