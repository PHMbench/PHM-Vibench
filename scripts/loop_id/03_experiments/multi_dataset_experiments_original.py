#!/usr/bin/env python3
"""
å¤šæ•°æ®é›†å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå®éªŒè„šæœ¬

å®Œæ•´çš„å¤šæ•°æ®é›†å®éªŒç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒï¼š
- å¤šæ•°æ®é›†ç»„åˆé…ç½®å’ŒåŸŸæ³›åŒ–å®éªŒ
- å¹¶è¡Œå®éªŒæ‰§è¡Œå’Œèµ„æºç®¡ç†
- ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•é›†æˆ
- æ™ºèƒ½é…ç½®ç®¡ç†å’Œå‚æ•°æ‰«æ
- è¯¦ç»†çš„å®éªŒæŠ¥å‘Šå’Œå¯è§†åŒ–åˆ†æ

ä½œè€…: Claude Code
ç‰ˆæœ¬: 2.0 (Task-014å®ç°)
"""

import os
import sys
import json
import yaml
import argparse
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Union
import subprocess
import time
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import itertools
import shutil
import logging
from dataclasses import dataclass, asdict
import numpy as np
from collections import defaultdict, Counter
import hashlib
import psutil
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

from src.configs import load_config
from src.data_factory.id_data_factory import id_data_factory

# å°è¯•å¯¼å…¥åŸºå‡†æµ‹è¯•æ¨¡å—ï¼ˆå¯é€‰ï¼‰
# å°è¯•å¯¼å…¥åŸºå‡†æµ‹è¯•æ¨¡å—ï¼ˆå¯é€‰ï¼‰
try:
    from benchmarks.contrastive_benchmark import PerformanceBenchmark
    BENCHMARK_AVAILABLE = True
except ImportError:
    BENCHMARK_AVAILABLE = False
    print("Warning: Benchmark module not available. Benchmark features will be disabled.")


@dataclass
class DatasetInfo:
    """æ•°æ®é›†ä¿¡æ¯çš„ç»“æ„åŒ–è¡¨ç¤º"""
    name: str
    metadata_file: str
    h5_file: Path
    num_samples: int
    num_classes: int = 0
    num_ids: int = 0
    h5_size_mb: float = 0.0
    ready: bool = False
    class_distribution: Dict[int, int] = None
    signal_length: int = 0
    sampling_rate: int = 0
    domain_type: str = "unknown"  # bearing, gear, motor, etc.
    
    def __post_init__(self):
        if self.class_distribution is None:
            self.class_distribution = {}


@dataclass  
class ExperimentConfig:
    """å®éªŒé…ç½®çš„ç»“æ„åŒ–è¡¨ç¤º"""
    id: str
    name: str
    dataset_combination: List[str]  # æ•°æ®é›†ç»„åˆ
    variant_name: str
    config_overrides: Dict[str, Any]
    expected_duration_hours: float
    priority: int = 0  # å®éªŒä¼˜å…ˆçº§
    status: str = "pending"
    dependencies: List[str] = None  # ä¾èµ–çš„å®éªŒ ID
    resource_requirements: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []
        if self.resource_requirements is None:
            self.resource_requirements = {
                'gpu_memory_mb': 8000,
                'ram_mb': 16000,
                'cpu_cores': 4
            }


@dataclass
class ExperimentResult:
    """å®éªŒç»“æœçš„ç»“æ„åŒ–è¡¨ç¤º"""
    experiment_id: str
    status: str  # completed, failed, skipped, running
    start_time: datetime = None
    end_time: datetime = None
    actual_duration_hours: float = 0.0
    metrics: Dict[str, float] = None
    error_message: str = ""
    resource_usage: Dict[str, float] = None
    output_files: List[str] = None
    
    def __post_init__(self):
        if self.metrics is None:
            self.metrics = {}
        if self.resource_usage is None:
            self.resource_usage = {}
        if self.output_files is None:
            self.output_files = []


class MultiDatasetExperimentRunner:
    """
    å¤šæ•°æ®é›†å¯¹æ¯”å­¦ä¹ å®éªŒè¿è¡Œå™¨
    
    ç‰¹æ€§:
    - æ™ºèƒ½æ•°æ®é›†ç»„åˆå’ŒåŸŸæ³›åŒ–å®éªŒ
    - å¹¶è¡Œæ‰§è¡Œå’Œèµ„æºç®¡ç†
    - åŸºå‡†æµ‹è¯•é›†æˆ
    - ç»¼åˆå¯è§†åŒ–æŠ¥å‘Š
    - å®éªŒæ¢å¤å’Œæ–­ç‚¹ç»­ä¼ 
    """
    
    def __init__(self, 
                 base_config_path: str,
                 metadata_dir: str = "data",
                 results_dir: str = "save/multi_dataset",
                 dry_run: bool = False,
                 enable_benchmarking: bool = True,
                 max_concurrent_experiments: int = None):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨
        
        Args:
            base_config_path: åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„
            metadata_dir: metadataæ–‡ä»¶ç›®å½•
            results_dir: ç»“æœä¿å­˜ç›®å½•
            dry_run: æ˜¯å¦åªè¾“å‡ºå®éªŒè®¡åˆ’è€Œä¸å®é™…è¿è¡Œ
            enable_benchmarking: æ˜¯å¦å¯ç”¨åŸºå‡†æµ‹è¯•
            max_concurrent_experiments: æœ€å¤§å¹¶å‘å®éªŒæ•°ï¼ˆé»˜è®¤ä¸ºCPUæ ¸æ•°çš„ä¸€åŠï¼‰
        """
        self.base_config_path = base_config_path
        self.metadata_dir = Path(metadata_dir)
        self.results_dir = Path(results_dir)
        self.dry_run = dry_run
        self.enable_benchmarking = enable_benchmarking and BENCHMARK_AVAILABLE
        self.max_concurrent_experiments = max_concurrent_experiments or max(1, mp.cpu_count() // 2)
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # å®éªŒçŠ¶æ€ç®¡ç†
        self.experiments: List[ExperimentConfig] = []
        self.results: Dict[str, List[ExperimentResult]] = {
            'completed': [],
            'failed': [],
            'skipped': [],
            'running': []
        }
        
        # èµ„æºç®¡ç†
        self.resource_manager = ResourceManager()
        
        # åŸºå‡†æµ‹è¯•å™¨ï¼ˆå¯é€‰ï¼‰
        self.benchmark = None
        if self.enable_benchmarking:
            try:
                self.benchmark = PerformanceBenchmark(
                    save_dir=self.results_dir / "benchmarks"
                )
            except Exception as e:
                self.logger.warning(f"Failed to initialize benchmark: {e}")
                self.enable_benchmarking = False
        
        # å®éªŒçŠ¶æ€æ–‡ä»¶
        self.state_file = self.results_dir / "experiment_state.json"
        self._load_experiment_state()
        
        self.logger.info(f"å¤šæ•°æ®é›†å¯¹æ¯”å­¦ä¹ å®éªŒè¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"åŸºç¡€é…ç½®: {base_config_path}")
        self.logger.info(f"ç»“æœç›®å½•: {results_dir}")
        self.logger.info(f"å¹²è¿è¡Œæ¨¡å¼: {dry_run}")
        self.logger.info(f"åŸºå‡†æµ‹è¯•: {self.enable_benchmarking}")
        self.logger.info(f"æœ€å¤§å¹¶å‘å®éªŒæ•°: {self.max_concurrent_experiments}")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_file = self.results_dir / "experiment.log"
        
        # åˆ›å»º logger
        self.logger = logging.getLogger('MultiDatasetExperiment')
        self.logger.setLevel(logging.INFO)
        
        # é¿å…é‡å¤æ·»åŠ  handler
        if not self.logger.handlers:
            # æ–‡ä»¶ handler
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(logging.INFO)
            
            # æ§åˆ¶å° handler
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            
            # è®¾ç½®æ ¼å¼
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)
            
            # æ·»åŠ  handlers
            self.logger.addHandler(file_handler)
            self.logger.addHandler(console_handler)
    
    def _load_experiment_state(self):
        """åŠ è½½å®éªŒçŠ¶æ€ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r') as f:
                    state = json.load(f)
                
                # æ¢å¤å®éªŒçŠ¶æ€
                for exp_data in state.get('experiments', []):
                    exp = ExperimentConfig(**exp_data)
                    self.experiments.append(exp)
                
                # æ¢å¤ç»“æœ
                for status, results in state.get('results', {}).items():
                    if status in self.results:
                        for result_data in results:
                            result = ExperimentResult(**result_data)
                            if result.start_time:
                                result.start_time = datetime.fromisoformat(result.start_time)
                            if result.end_time:
                                result.end_time = datetime.fromisoformat(result.end_time)
                            self.results[status].append(result)
                
                self.logger.info(f"Loaded experiment state with {len(self.experiments)} experiments")
                
            except Exception as e:
                self.logger.warning(f"Failed to load experiment state: {e}")
    
    def _save_experiment_state(self):
        """ä¿å­˜å®éªŒçŠ¶æ€"""
        try:
            state = {
                'experiments': [asdict(exp) for exp in self.experiments],
                'results': {},
                'last_updated': datetime.now().isoformat()
            }
            
            # è½¬æ¢ç»“æœæ ¼å¼
            for status, results in self.results.items():
                state['results'][status] = []
                for result in results:
                    result_dict = asdict(result)
                    # è½¬æ¢æ—¥æœŸç±»å‹
                    if result_dict['start_time']:
                        result_dict['start_time'] = result.start_time.isoformat()
                    if result_dict['end_time']:
                        result_dict['end_time'] = result.end_time.isoformat()
                    state['results'][status].append(result_dict)
            
            with open(self.state_file, 'w') as f:
                json.dump(state, f, indent=2)
                
        except Exception as e:
            self.logger.error(f"Failed to save experiment state: {e}")
    
    def discover_datasets(self) -> List[DatasetInfo]:
        """å‘ç°æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
        self.logger.info(f"æ­£åœ¨æ‰«æmetadataç›®å½•: {self.metadata_dir}")
        
        datasets = []
        metadata_files = list(self.metadata_dir.glob("metadata_*.xlsx"))
        
        if not metadata_files:
            self.logger.warning(f"åœ¨ {self.metadata_dir} ä¸­æœªæ‰¾åˆ°metadataæ–‡ä»¶")
            return datasets
        
        for metadata_file in metadata_files:
            try:
                # æå–æ•°æ®é›†åç§°
                dataset_name = metadata_file.stem.replace('metadata_', '')
                
                # è¯»å–metadataæ–‡ä»¶è·å–åŸºæœ¬ä¿¡æ¯
                df = pd.read_excel(metadata_file, sheet_name=0)
                
                # åˆ›å»ºæ•°æ®é›†ä¿¡æ¯å¯¹è±¡
                dataset_info = DatasetInfo(
                    name=dataset_name,
                    metadata_file=str(metadata_file),
                    h5_file=self.metadata_dir / f"{dataset_name}.h5",
                    num_samples=len(df)
                )
                
                # æ£€æŸ¥H5æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if dataset_info.h5_file.exists():
                    dataset_info.h5_size_mb = dataset_info.h5_file.stat().st_size / (1024 * 1024)
                    dataset_info.ready = True
                else:
                    dataset_info.h5_size_mb = 0
                    dataset_info.ready = False
                    self.logger.warning(f"æ•°æ®é›† {dataset_name} çš„H5æ–‡ä»¶ä¸å­˜åœ¨: {dataset_info.h5_file}")
                
                # å°è¯•è·å–æ›´å¤šä¿¡æ¯
                if 'Label' in df.columns:
                    dataset_info.num_classes = df['Label'].nunique()
                    dataset_info.class_distribution = df['Label'].value_counts().to_dict()
                
                if 'ID' in df.columns:
                    dataset_info.num_ids = df['ID'].nunique()
                
                # æ¨æ–­åŸŸç±»å‹
                dataset_info.domain_type = self._infer_domain_type(dataset_name)
                
                # è·å–ä¿¡å·é•¿åº¦ä¿¡æ¯
                if 'Length' in df.columns:
                    dataset_info.signal_length = int(df['Length'].iloc[0]) if len(df) > 0 else 0
                elif 'signal_length' in df.columns:
                    dataset_info.signal_length = int(df['signal_length'].iloc[0]) if len(df) > 0 else 0
                
                datasets.append(dataset_info)
                self.logger.info(f"å‘ç°æ•°æ®é›†: {dataset_name} ({dataset_info.num_samples} æ ·æœ¬, {dataset_info.num_classes} ç±»åˆ«)")
                
            except Exception as e:
                self.logger.error(f"å¤„ç†metadataæ–‡ä»¶å¤±è´¥ {metadata_file}: {e}")
        
        self.logger.info(f"å…±å‘ç° {len(datasets)} ä¸ªæ•°æ®é›†")
        return datasets
    
    def _infer_domain_type(self, dataset_name: str) -> str:
        """æ ¹æ®æ•°æ®é›†åç§°æ¨æ–­åŸŸç±»å‹"""
        name_lower = dataset_name.lower()
        
        if any(keyword in name_lower for keyword in ['cwru', 'bearing', 'ball']):
            return 'bearing'
        elif any(keyword in name_lower for keyword in ['gear', 'gearbox']):
            return 'gear'
        elif any(keyword in name_lower for keyword in ['motor', 'rotor']):
            return 'motor'
        elif any(keyword in name_lower for keyword in ['pump']):
            return 'pump'
        elif any(keyword in name_lower for keyword in ['fan', 'compressor']):
            return 'fan'
        else:
            return 'unknown'
    
    def filter_datasets(self, 
                       datasets: List[DatasetInfo], 
                       include_patterns: List[str] = None,
                       exclude_patterns: List[str] = None,
                       min_samples: int = 100,
                       ready_only: bool = True,
                       domain_types: List[str] = None) -> List[DatasetInfo]:
        """è¿‡æ»¤æ•°æ®é›†"""
        self.logger.info("æ­£åœ¨è¿‡æ»¤æ•°æ®é›†...")
        
        filtered_datasets = []
        
        for dataset in datasets:
            # æ£€æŸ¥æ˜¯å¦å‡†å¤‡å°±ç»ª
            if ready_only and not dataset.ready:
                self.logger.debug(f"è·³è¿‡æœªå‡†å¤‡æ•°æ®é›†: {dataset.name}")
                continue
            
            # æ£€æŸ¥æ ·æœ¬æ•°é‡
            if dataset.num_samples < min_samples:
                self.logger.debug(f"è·³è¿‡æ ·æœ¬æ•°é‡ä¸è¶³çš„æ•°æ®é›†: {dataset.name} ({dataset.num_samples} < {min_samples})")
                continue
            
            # æ£€æŸ¥åŒ…å«æ¨¡å¼
            if include_patterns:
                if not any(pattern.lower() in dataset.name.lower() for pattern in include_patterns):
                    self.logger.debug(f"è·³è¿‡ä¸åŒ¹é…åŒ…å«æ¨¡å¼çš„æ•°æ®é›†: {dataset.name}")
                    continue
            
            # æ£€æŸ¥æ’é™¤æ¨¡å¼
            if exclude_patterns:
                if any(pattern.lower() in dataset.name.lower() for pattern in exclude_patterns):
                    self.logger.debug(f"è·³è¿‡åŒ¹é…æ’é™¤æ¨¡å¼çš„æ•°æ®é›†: {dataset.name}")
                    continue
            
            # æ£€æŸ¥åŸŸç±»å‹
            if domain_types:
                if dataset.domain_type not in domain_types:
                    self.logger.debug(f"è·³è¿‡ä¸åŒ¹é…åŸŸç±»å‹çš„æ•°æ®é›†: {dataset.name} ({dataset.domain_type} not in {domain_types})")
                    continue
            
            filtered_datasets.append(dataset)
            self.logger.info(f"ä¿ç•™æ•°æ®é›†: {dataset.name} (åŸŸ: {dataset.domain_type})")
        
        self.logger.info(f"è¿‡æ»¤åä¿ç•™ {len(filtered_datasets)} ä¸ªæ•°æ®é›†")
        return filtered_datasets
    
    def generate_dataset_combinations(self, 
                                     datasets: List[DatasetInfo],
                                     combination_strategies: List[str] = None) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆæ•°æ®é›†ç»„åˆç­–ç•¥
        
        Args:
            datasets: æ•°æ®é›†åˆ—è¡¨
            combination_strategies: ç»„åˆç­–ç•¥åˆ—è¡¨
        
        Returns:
            æ•°æ®é›†ç»„åˆåˆ—è¡¨
        """
        if combination_strategies is None:
            combination_strategies = [
                'single',  # å•æ•°æ®é›†
                'cross_domain',  # è·¨åŸŸ
                'same_domain',  # åŒåŸŸå¤šæ•°æ®é›†
                'all_datasets',  # æ‰€æœ‰æ•°æ®é›†
            ]
        
        combinations = []
        
        # æŒ‰åŸŸç±»å‹åˆ†ç»„
        domain_groups = defaultdict(list)
        for dataset in datasets:
            domain_groups[dataset.domain_type].append(dataset)
        
        for strategy in combination_strategies:
            if strategy == 'single':
                # å•æ•°æ®é›†å®éªŒ
                for dataset in datasets:
                    combinations.append({
                        'name': f'single_{dataset.name}',
                        'strategy': 'single',
                        'datasets': [dataset.name],
                        'description': f'å•æ•°æ®é›†å®éªŒ: {dataset.name}',
                        'balancing': 'single',
                        'expected_samples': dataset.num_samples
                    })
            
            elif strategy == 'cross_domain':
                # è·¨åŸŸç»„åˆï¼šä¸åŒåŸŸç±»å‹çš„æ•°æ®é›†
                domain_types = list(domain_groups.keys())
                for i, source_domain in enumerate(domain_types):
                    for target_domain in domain_types[i+1:]:
                        if source_domain != target_domain and source_domain != 'unknown' and target_domain != 'unknown':
                            source_datasets = [ds.name for ds in domain_groups[source_domain]]
                            target_datasets = [ds.name for ds in domain_groups[target_domain]]
                            
                            # åˆ›å»ºè·¨åŸŸç»„åˆ
                            for source_ds in source_datasets[:2]:  # æœ€å¤š2ä¸ªæºåŸŸ
                                for target_ds in target_datasets[:2]:  # æœ€å¤š2ä¸ªç›®æ ‡åŸŸ
                                    combinations.append({
                                        'name': f'cross_{source_domain}_to_{target_domain}_{source_ds}_{target_ds}',
                                        'strategy': 'cross_domain',
                                        'datasets': [source_ds, target_ds],
                                        'source_datasets': [source_ds],
                                        'target_datasets': [target_ds],
                                        'description': f'è·¨åŸŸå®éªŒ: {source_domain} â†’ {target_domain}',
                                        'balancing': 'weighted',
                                        'expected_samples': sum(ds.num_samples for ds in datasets if ds.name in [source_ds, target_ds])
                                    })
            
            elif strategy == 'same_domain':
                # åŒåŸŸå¤šæ•°æ®é›†
                for domain_type, domain_datasets in domain_groups.items():
                    if len(domain_datasets) >= 2 and domain_type != 'unknown':
                        dataset_names = [ds.name for ds in domain_datasets]
                        combinations.append({
                            'name': f'same_domain_{domain_type}_all',
                            'strategy': 'same_domain',
                            'datasets': dataset_names,
                            'description': f'åŒåŸŸå¤šæ•°æ®é›†: {domain_type}',
                            'balancing': 'proportional',
                            'expected_samples': sum(ds.num_samples for ds in domain_datasets)
                        })
            
            elif strategy == 'all_datasets':
                # æ‰€æœ‰æ•°æ®é›†ï¼ˆå¦‚æœæ•°é‡ä¸å¤ªå¤šï¼‰
                if len(datasets) <= 8:  # é™åˆ¶æ•°æ®é›†æ•°é‡
                    dataset_names = [ds.name for ds in datasets]
                    combinations.append({
                        'name': 'all_datasets_combined',
                        'strategy': 'all_datasets',
                        'datasets': dataset_names,
                        'description': 'æ‰€æœ‰æ•°æ®é›†ç»„åˆå®éªŒ',
                        'balancing': 'equal',
                        'expected_samples': sum(ds.num_samples for ds in datasets)
                    })
        
        self.logger.info(f"ç”Ÿæˆäº† {len(combinations)} ä¸ªæ•°æ®é›†ç»„åˆ")
        return combinations


class ResourceManager:
    """èµ„æºç®¡ç†å™¨ï¼Œç”¨äºç®¡ç†å®éªŒæ‰€éœ€çš„è®¡ç®—èµ„æº"""
    
    def __init__(self):
        self.available_gpus = self._get_available_gpus()
        self.total_ram_mb = psutil.virtual_memory().total / (1024 * 1024)
        self.available_ram_mb = psutil.virtual_memory().available / (1024 * 1024)
        self.cpu_cores = mp.cpu_count()
        
        # å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ
        self.running_experiments: Dict[str, Dict[str, float]] = {}
        
    def _get_available_gpus(self) -> List[Dict[str, Any]]:
        """è·å–å¯ç”¨GPUä¿¡æ¯"""
        gpus = []
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_info = {
                    'id': i,
                    'name': torch.cuda.get_device_name(i),
                    'memory_mb': torch.cuda.get_device_properties(i).total_memory / (1024 * 1024)
                }
                gpus.append(gpu_info)
        return gpus
    
    def can_run_experiment(self, resource_req: Dict[str, float]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„èµ„æºè¿è¡Œå®éªŒ"""
        # è®¡ç®—å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ
        used_ram = sum(exp['ram_mb'] for exp in self.running_experiments.values())
        used_gpu_memory = sum(exp['gpu_memory_mb'] for exp in self.running_experiments.values())
        used_cpu_cores = sum(exp['cpu_cores'] for exp in self.running_experiments.values())
        
        # æ£€æŸ¥èµ„æºé™åˆ¶
        if used_ram + resource_req.get('ram_mb', 0) > self.available_ram_mb * 0.8:  # ç•™å‡º20%ç¼“å†²
            return False
        
        if used_cpu_cores + resource_req.get('cpu_cores', 0) > self.cpu_cores * 0.8:
            return False
        
        if self.available_gpus and used_gpu_memory + resource_req.get('gpu_memory_mb', 0) > max(gpu['memory_mb'] for gpu in self.available_gpus) * 0.8:
            return False
        
        return True
    
    def allocate_resources(self, experiment_id: str, resource_req: Dict[str, float]) -> bool:
        """ä¸ºå®éªŒåˆ†é…èµ„æº"""
        if self.can_run_experiment(resource_req):
            self.running_experiments[experiment_id] = resource_req.copy()
            return True
        return False
    
    def release_resources(self, experiment_id: str):
        """é‡Šæ”¾å®éªŒèµ„æº"""
        if experiment_id in self.running_experiments:
            del self.running_experiments[experiment_id]
    
    def get_resource_status(self) -> Dict[str, Any]:
        """è·å–èµ„æºä½¿ç”¨çŠ¶æ€"""
        used_ram = sum(exp['ram_mb'] for exp in self.running_experiments.values())
        used_gpu_memory = sum(exp['gpu_memory_mb'] for exp in self.running_experiments.values())
        used_cpu_cores = sum(exp['cpu_cores'] for exp in self.running_experiments.values())
        
        return {
            'total_ram_mb': self.total_ram_mb,
            'available_ram_mb': self.available_ram_mb,
            'used_ram_mb': used_ram,
            'total_cpu_cores': self.cpu_cores,
            'used_cpu_cores': used_cpu_cores,
            'gpus': self.available_gpus,
            'used_gpu_memory_mb': used_gpu_memory,
            'running_experiments': len(self.running_experiments)
        }
    
    def generate_dataset_combinations(self, 
                                     datasets: List[DatasetInfo],
                                     combination_strategies: List[str] = None) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆæ•°æ®é›†ç»„åˆç­–ç•¥
        
        Args:
            datasets: æ•°æ®é›†åˆ—è¡¨
            combination_strategies: ç»„åˆç­–ç•¥åˆ—è¡¨
        
        Returns:
            æ•°æ®é›†ç»„åˆåˆ—è¡¨
        """
        if combination_strategies is None:
            combination_strategies = [
                'single',  # å•æ•°æ®é›†
                'cross_domain',  # è·¨åŸŸ
                'same_domain',  # åŒåŸŸå¤šæ•°æ®é›†
                'all_datasets',  # æ‰€æœ‰æ•°æ®é›†
            ]
        
        combinations = []
        
        # æŒ‰åŸŸç±»å‹åˆ†ç»„
        domain_groups = defaultdict(list)
        for dataset in datasets:
            domain_groups[dataset.domain_type].append(dataset)
        
        for strategy in combination_strategies:
            if strategy == 'single':
                # å•æ•°æ®é›†å®éªŒ
                for dataset in datasets:
                    combinations.append({
                        'name': f'single_{dataset.name}',
                        'strategy': 'single',
                        'datasets': [dataset.name],
                        'description': f'å•æ•°æ®é›†å®éªŒ: {dataset.name}',
                        'balancing': 'single',
                        'expected_samples': dataset.num_samples
                    })
            
            elif strategy == 'cross_domain':
                # è·¨åŸŸç»„åˆï¼šä¸åŒåŸŸç±»å‹çš„æ•°æ®é›†
                domain_types = list(domain_groups.keys())
                for i, source_domain in enumerate(domain_types):
                    for target_domain in domain_types[i+1:]:
                        if source_domain != target_domain and source_domain != 'unknown' and target_domain != 'unknown':
                            source_datasets = [ds.name for ds in domain_groups[source_domain]]
                            target_datasets = [ds.name for ds in domain_groups[target_domain]]
                            
                            # åˆ›å»ºè·¨åŸŸç»„åˆ
                            for source_ds in source_datasets[:2]:  # æœ€å¤š2ä¸ªæºåŸŸ
                                for target_ds in target_datasets[:2]:  # æœ€å¤š2ä¸ªç›®æ ‡åŸŸ
                                    combinations.append({
                                        'name': f'cross_{source_domain}_to_{target_domain}_{source_ds}_{target_ds}',
                                        'strategy': 'cross_domain',
                                        'datasets': [source_ds, target_ds],
                                        'source_datasets': [source_ds],
                                        'target_datasets': [target_ds],
                                        'description': f'è·¨åŸŸå®éªŒ: {source_domain} â†’ {target_domain}',
                                        'balancing': 'weighted',
                                        'expected_samples': sum(ds.num_samples for ds in datasets if ds.name in [source_ds, target_ds])
                                    })
            
            elif strategy == 'same_domain':
                # åŒåŸŸå¤šæ•°æ®é›†
                for domain_type, domain_datasets in domain_groups.items():
                    if len(domain_datasets) >= 2 and domain_type != 'unknown':
                        dataset_names = [ds.name for ds in domain_datasets]
                        combinations.append({
                            'name': f'same_domain_{domain_type}_all',
                            'strategy': 'same_domain',
                            'datasets': dataset_names,
                            'description': f'åŒåŸŸå¤šæ•°æ®é›†: {domain_type}',
                            'balancing': 'proportional',
                            'expected_samples': sum(ds.num_samples for ds in domain_datasets)
                        })
            
            elif strategy == 'all_datasets':
                # æ‰€æœ‰æ•°æ®é›†ï¼ˆå¦‚æœæ•°é‡ä¸å¤ªå¤šï¼‰
                if len(datasets) <= 8:  # é™åˆ¶æ•°æ®é›†æ•°é‡
                    dataset_names = [ds.name for ds in datasets]
                    combinations.append({
                        'name': 'all_datasets_combined',
                        'strategy': 'all_datasets',
                        'datasets': dataset_names,
                        'description': 'æ‰€æœ‰æ•°æ®é›†ç»„åˆå®éªŒ',
                        'balancing': 'equal',
                        'expected_samples': sum(ds.num_samples for ds in datasets)
                    })
        
        self.logger.info(f"ç”Ÿæˆäº† {len(combinations)} ä¸ªæ•°æ®é›†ç»„åˆ")
        return combinations
    
    def generate_experiment_configs(self, 
                                   datasets: List[DatasetInfo],
                                   config_variants: List[Dict] = None,
                                   dataset_combinations: List[Dict] = None,
                                   enable_ablation: bool = True) -> List[ExperimentConfig]:
        """
        ç”Ÿæˆå®éªŒé…ç½®
        
        Args:
            datasets: æ•°æ®é›†åˆ—è¡¨
            config_variants: é…ç½®å˜ä½“åˆ—è¡¨
            dataset_combinations: æ•°æ®é›†ç»„åˆåˆ—è¡¨
            enable_ablation: æ˜¯å¦å¯ç”¨æ¶ˆèå®éªŒ
        
        Returns:
            å®éªŒé…ç½®åˆ—è¡¨
        """
        self.logger.info("æ­£åœ¨ç”Ÿæˆå®éªŒé…ç½®...")
        
        # ç”Ÿæˆæ•°æ®é›†ç»„åˆï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if dataset_combinations is None:
            dataset_combinations = self.generate_dataset_combinations(datasets)
        
        if config_variants is None:
            # å¯¹æ¯”å­¦ä¹ é»˜è®¤é…ç½®å˜ä½“
            config_variants = [
                {'name': 'default', 'overrides': {}, 'priority': 1},
                {'name': 'large_window', 'overrides': {'data.window_size': 2048, 'data.stride': 1024}, 'priority': 2},
                {'name': 'small_window', 'overrides': {'data.window_size': 512, 'data.stride': 256}, 'priority': 2},
                {'name': 'low_temp', 'overrides': {'task.temperature': 0.05}, 'priority': 3},
                {'name': 'high_temp', 'overrides': {'task.temperature': 0.3}, 'priority': 3},
                {'name': 'high_lr', 'overrides': {'task.lr': 5e-3}, 'priority': 3},
                {'name': 'large_batch', 'overrides': {'data.batch_size': 64}, 'priority': 2},
                {'name': 'small_batch', 'overrides': {'data.batch_size': 8}, 'priority': 2}
            ]
            
            # æ¶ˆèå®éªŒç‰¹å®šå˜ä½“
            if enable_ablation:
                ablation_variants = [
                    {'name': 'no_augmentation', 'overrides': {'data.enable_augmentation': False}, 'priority': 4},
                    {'name': 'different_backbone', 'overrides': {'model.backbone': 'B_04_Dlinear'}, 'priority': 4},
                    {'name': 'larger_model', 'overrides': {'model.d_model': 512}, 'priority': 4},
                    {'name': 'minimal_model', 'overrides': {'model.d_model': 128}, 'priority': 4}
                ]
                config_variants.extend(ablation_variants)
        
        experiments = []
        experiment_counter = 0
        
        for combination in dataset_combinations:
            for variant in config_variants:
                experiment_counter += 1
                
                # ç”Ÿæˆå®éªŒ ID
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                experiment_id = f"{combination['name']}_{variant['name']}_{timestamp}_{experiment_counter:04d}"
                
                # åŸºç¡€é…ç½®è¦†ç›–
                base_overrides = {
                    'environment.experiment_name': f"contrastive_{combination['name']}_{variant['name']}",
                    'environment.save_dir': str(self.results_dir / combination['name'] / variant['name']),
                    'data.factory_name': 'id',  # ç¡®ä¿ä½¿ç”¨ id_data_factory
                    'task.type': 'pretrain',
                    'task.name': 'contrastive_id'
                }
                
                # è®¾ç½®æ•°æ®é›†é…ç½®
                if combination['strategy'] == 'single':
                    # å•æ•°æ®é›†
                    dataset_name = combination['datasets'][0]
                    dataset_info = next(ds for ds in datasets if ds.name == dataset_name)
                    base_overrides['data.metadata_file'] = dataset_info.metadata_file
                else:
                    # å¤šæ•°æ®é›†ç»„åˆ
                    dataset_files = [next(ds for ds in datasets if ds.name == name).metadata_file 
                                   for name in combination['datasets']]
                    base_overrides['data.metadata_files'] = dataset_files  # å¤šæ•°æ®é›†æ”¯æŒ
                    base_overrides['data.dataset_balancing'] = combination.get('balancing', 'equal')
                    
                    # è·¨åŸŸç‰¹å®šé…ç½®
                    if combination['strategy'] == 'cross_domain':
                        base_overrides.update({
                            'data.source_datasets': combination.get('source_datasets', []),
                            'data.target_datasets': combination.get('target_datasets', []),
                            'data.domain_adaptation': True,
                            'trainer.val_split_strategy': 'target_only'
                        })
                
                # åˆå¹¶å˜ä½“è¦†ç›–
                final_overrides = {**base_overrides, **variant['overrides']}
                
                # æ•°æ®é›†ç‰¹å®šè°ƒæ•´
                combination_specific_overrides = self._get_combination_specific_overrides(combination, datasets)
                final_overrides.update(combination_specific_overrides)
                
                # ç”Ÿæˆå®éªŒé…ç½®
                experiment = ExperimentConfig(
                    id=experiment_id,
                    name=f"{combination['name']}_{variant['name']}",
                    dataset_combination=combination['datasets'],
                    variant_name=variant['name'],
                    config_overrides=final_overrides,
                    expected_duration_hours=self._estimate_experiment_duration(combination, variant, datasets),
                    priority=variant.get('priority', 5),
                    resource_requirements=self._estimate_resource_requirements(combination, variant, datasets)
                )
                
                experiments.append(experiment)
        
        self.logger.info(f"ç”Ÿæˆäº† {len(experiments)} ä¸ªå®éªŒé…ç½®")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        experiments.sort(key=lambda x: (x.priority, x.expected_duration_hours))
        
        return experiments
    
    def _get_dataset_specific_overrides(self, dataset: Dict) -> Dict:
        """æ ¹æ®æ•°æ®é›†ç‰¹æ€§ç”Ÿæˆç‰¹å®šçš„é…ç½®è¦†ç›–"""
        overrides = {}
        
        # æ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´æ‰¹é‡å¤§å°
        if dataset['num_samples'] < 500:
            overrides['data.batch_size'] = 8
            overrides['trainer.epochs'] = 1  # å¿«é€Ÿæµ‹è¯•ç”¨å•epoch
        elif dataset['num_samples'] < 2000:
            overrides['data.batch_size'] = 16
            overrides['trainer.epochs'] = 1  # å¿«é€Ÿæµ‹è¯•ç”¨å•epoch
        else:
            overrides['data.batch_size'] = 32
            overrides['trainer.epochs'] = 1  # å¿«é€Ÿæµ‹è¯•ç”¨å•epoch
        
        # æ ¹æ®H5æ–‡ä»¶å¤§å°è°ƒæ•´num_workers
        if dataset.get('h5_size_mb', 0) > 1000:  # å¤§æ–‡ä»¶
            overrides['data.num_workers'] = 8
        elif dataset.get('h5_size_mb', 0) > 100:
            overrides['data.num_workers'] = 4
        else:
            overrides['data.num_workers'] = 2
        
        # æ ¹æ®ç±»åˆ«æ•°é‡è°ƒæ•´æ¨¡å‹å¤§å°
        if dataset.get('num_classes', 0) > 10:
            overrides['model.d_model'] = 512
        elif dataset.get('num_classes', 0) > 5:
            overrides['model.d_model'] = 256
        else:
            overrides['model.d_model'] = 128
        
        return overrides
    
    def _estimate_experiment_duration(self, combination: Dict[str, Any], variant: Dict, datasets: List[DatasetInfo]) -> float:
        """ä¼°è®¡å®éªŒæŒç»­æ—¶é—´ï¼ˆå°æ—¶ï¼‰"""
        # è·å–ç»„åˆä¸­æ•°æ®é›†çš„æ€»ä¿¡æ¯
        combination_datasets = [ds for ds in datasets if ds.name in combination['datasets']]
        total_samples = sum(ds.num_samples for ds in combination_datasets)
        
        # åŸºç¡€æ—¶é—´ï¼ˆä½¿ç”¨å•epochè®­ç»ƒï¼‰
        base_duration = 0.1  # å•epochåŸºç¡€æ—¶é—´
        
        # æ ¹æ®æ•°æ®é›†ç»„åˆå¤§å°è°ƒæ•´
        size_factor = total_samples / 1000
        base_duration *= (1 + size_factor * 0.05)
        
        # å¤šæ•°æ®é›†ç»„åˆå¢åŠ å¤æ‚åº¦
        if len(combination['datasets']) > 1:
            base_duration *= (1 + len(combination['datasets']) * 0.2)
        
        # æ ¹æ®é…ç½®å˜ä½“è°ƒæ•´
        if 'large_window' in variant['name']:
            base_duration *= 1.3
        elif 'small_window' in variant['name']:
            base_duration *= 0.8
        
        if 'large_batch' in variant['name']:
            base_duration *= 0.7  # å¤§æ‰¹é‡å¯èƒ½æ›´å¿«
        elif 'small_batch' in variant['name']:
            base_duration *= 1.2
            
        if 'high_lr' in variant['name']:
            base_duration *= 0.9  # é«˜å­¦ä¹ ç‡å¯èƒ½æ”¶æ•›æ›´å¿«
        
        # è·¨åŸŸå®éªŒé€šå¸¸éœ€è¦æ›´å¤šæ—¶é—´
        if combination.get('strategy') == 'cross_domain':
            base_duration *= 1.2
        
        return round(base_duration, 2)
    
    def run_experiments(self, 
                       experiments: List[Dict],
                       parallel: bool = False,
                       max_parallel: int = 2,
                       timeout_hours: int = 24) -> Dict:
        """è¿è¡Œå®éªŒ"""
        print(f"\nå¼€å§‹è¿è¡Œ {len(experiments)} ä¸ªå®éªŒ")
        
        if self.dry_run:
            print("ğŸ”„ å¹²è¿è¡Œæ¨¡å¼ï¼Œåªè¾“å‡ºå®éªŒè®¡åˆ’:")
            self._print_experiment_plan(experiments)
            return {'completed': [], 'failed': [], 'skipped': experiments}
        
        if parallel:
            return self._run_experiments_parallel(experiments, max_parallel, timeout_hours)
        else:
            return self._run_experiments_sequential(experiments, timeout_hours)
    
    def _run_experiments_sequential(self, experiments: List[Dict], timeout_hours: int) -> Dict:
        """é¡ºåºè¿è¡Œå®éªŒ"""
        results = {'completed': [], 'failed': [], 'skipped': []}
        
        total_experiments = len(experiments)
        
        for i, experiment in enumerate(experiments, 1):
            print(f"\n{'='*60}")
            print(f"è¿è¡Œå®éªŒ {i}/{total_experiments}: {experiment['id']}")
            print(f"æ•°æ®é›†: {experiment['dataset']}, å˜ä½“: {experiment['variant']}")
            print(f"é¢„è®¡è€—æ—¶: {experiment['expected_duration_hours']} å°æ—¶")
            print(f"{'='*60}")
            
            start_time = time.time()
            
            try:
                # åˆ›å»ºå®éªŒé…ç½®æ–‡ä»¶
                config_path = self._create_experiment_config(experiment)
                
                # è¿è¡Œå®éªŒ
                success = self._run_single_experiment(config_path, experiment, timeout_hours)
                
                end_time = time.time()
                actual_duration = (end_time - start_time) / 3600  # è½¬æ¢ä¸ºå°æ—¶
                
                experiment['actual_duration_hours'] = round(actual_duration, 2)
                experiment['status'] = 'completed' if success else 'failed'
                
                if success:
                    results['completed'].append(experiment)
                    print(f"âœ… å®éªŒå®Œæˆ: {experiment['id']} (è€—æ—¶: {actual_duration:.2f} å°æ—¶)")
                else:
                    results['failed'].append(experiment)
                    print(f"âŒ å®éªŒå¤±è´¥: {experiment['id']} (è€—æ—¶: {actual_duration:.2f} å°æ—¶)")
                
            except Exception as e:
                print(f"âŒ å®éªŒå¼‚å¸¸: {experiment['id']} - {e}")
                experiment['status'] = 'failed'
                experiment['error'] = str(e)
                results['failed'].append(experiment)
        
        return results
    
    def _run_experiments_parallel(self, experiments: List[Dict], max_parallel: int, timeout_hours: int) -> Dict:
        """å¹¶è¡Œè¿è¡Œå®éªŒ"""
        # è¿™é‡Œå¯ä»¥å®ç°å¹¶è¡Œæ‰§è¡Œé€»è¾‘
        # ä¸ºç®€åŒ–ï¼Œæš‚æ—¶ä½¿ç”¨é¡ºåºæ‰§è¡Œ
        print(f"âš ï¸  å¹¶è¡Œæ‰§è¡Œå°šæœªå®ç°ï¼Œä½¿ç”¨é¡ºåºæ‰§è¡Œ (max_parallel={max_parallel})")
        return self._run_experiments_sequential(experiments, timeout_hours)
    
    def _create_experiment_config(self, experiment: Dict) -> str:
        """åˆ›å»ºå®éªŒé…ç½®æ–‡ä»¶"""
        # åŠ è½½åŸºç¡€é…ç½®å¹¶åº”ç”¨è¦†ç›–
        config = load_config(self.base_config_path, experiment['config_overrides'])
        
        # ä¿å­˜å®éªŒé…ç½®
        experiment_dir = Path(experiment['config_overrides']['environment.save_dir'])
        experiment_dir.mkdir(parents=True, exist_ok=True)
        
        config_path = experiment_dir / f"config_{experiment['id']}.yaml"
        
        with open(config_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        # ä¿å­˜å®éªŒå…ƒä¿¡æ¯
        experiment_info = {
            'experiment_id': experiment['id'],
            'dataset': experiment['dataset'],
            'variant': experiment['variant'],
            'created_at': datetime.now().isoformat(),
            'config_overrides': experiment['config_overrides'],
            'expected_duration_hours': experiment['expected_duration_hours']
        }
        
        info_path = experiment_dir / f"experiment_info_{experiment['id']}.json"
        with open(info_path, 'w') as f:
            json.dump(experiment_info, f, indent=2)
        
        return str(config_path)
    
    def _run_single_experiment(self, config_path: str, experiment: Dict, timeout_hours: int) -> bool:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        try:
            # æ„å»ºå‘½ä»¤
            cmd = [
                'python', 'main.py',
                '--config', config_path
            ]
            
            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆè½¬æ¢ä¸ºç§’ï¼‰
            timeout_seconds = timeout_hours * 3600
            
            # è¿è¡Œå®éªŒ
            result = subprocess.run(
                cmd,
                timeout=timeout_seconds,
                capture_output=True,
                text=True
            )
            
            # ä¿å­˜æ—¥å¿—
            experiment_dir = Path(experiment['config_overrides']['environment.save_dir'])
            
            stdout_path = experiment_dir / f"stdout_{experiment['id']}.log"
            stderr_path = experiment_dir / f"stderr_{experiment['id']}.log"
            
            with open(stdout_path, 'w') as f:
                f.write(result.stdout)
            
            with open(stderr_path, 'w') as f:
                f.write(result.stderr)
            
            # æ£€æŸ¥è¿”å›ç 
            if result.returncode == 0:
                print(f"å®éªŒæˆåŠŸå®Œæˆ")
                return True
            else:
                print(f"å®éªŒå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                print(f"é”™è¯¯ä¿¡æ¯: {result.stderr[-500:]}")  # åªæ˜¾ç¤ºæœ€å500ä¸ªå­—ç¬¦
                return False
                
        except subprocess.TimeoutExpired:
            print(f"å®éªŒè¶…æ—¶ ({timeout_hours} å°æ—¶)")
            return False
        except Exception as e:
            print(f"è¿è¡Œå®éªŒæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return False
    
    def _print_experiment_plan(self, experiments: List[ExperimentConfig]):
        """æ‰“å°å®éªŒè®¡åˆ’"""
        self.logger.info("\nå®éªŒè®¡åˆ’æ‘˜è¦:")
        self.logger.info(f"æ€»å®éªŒæ•°: {len(experiments)}")
        
        # æŒ‰æ•°æ®é›†ç»„åˆåˆ†ç»„
        combination_groups = defaultdict(list)
        for exp in experiments:
            combination_key = "_".join(exp.dataset_combination)
            combination_groups[combination_key].append(exp)
        
        for combination, exps in combination_groups.items():
            self.logger.info(f"\næ•°æ®é›†ç»„åˆ: {combination} ({len(exps)} ä¸ªå®éªŒ)")
            total_time = sum(exp.expected_duration_hours for exp in exps)
            self.logger.info(f"  é¢„è®¡æ€»è€—æ—¶: {total_time:.1f} å°æ—¶")
            
            # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„æ˜¾ç¤º
            priority_groups = defaultdict(list)
            for exp in exps:
                priority_groups[exp.priority].append(exp)
            
            for priority in sorted(priority_groups.keys()):
                priority_exps = priority_groups[priority]
                priority_time = sum(exp.expected_duration_hours for exp in priority_exps)
                self.logger.info(f"    ä¼˜å…ˆçº§ {priority}: {len(priority_exps)} ä¸ªå®éªŒ, {priority_time:.1f} å°æ—¶")
                
                for exp in priority_exps[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    self.logger.info(f"      - {exp.variant_name}: {exp.expected_duration_hours} å°æ—¶")
                if len(priority_exps) > 3:
                    self.logger.info(f"      ... è¿˜æœ‰ {len(priority_exps) - 3} ä¸ªå®éªŒ")
        
        total_time = sum(exp.expected_duration_hours for exp in experiments)
        avg_time = total_time / len(experiments) if experiments else 0
        self.logger.info(f"\næ€»é¢„è®¡è€—æ—¶: {total_time:.1f} å°æ—¶")
        self.logger.info(f"å¹³å‡å®éªŒè€—æ—¶: {avg_time:.2f} å°æ—¶")
        
        # èµ„æºéœ€æ±‚æ‘˜è¦
        total_gpu_memory = sum(exp.resource_requirements.get('gpu_memory_mb', 0) for exp in experiments)
        max_gpu_memory = max((exp.resource_requirements.get('gpu_memory_mb', 0) for exp in experiments), default=0)
        self.logger.info(f"\nèµ„æºéœ€æ±‚æ‘˜è¦:")
        self.logger.info(f"  æœ€å¤§GPUå†…å­˜éœ€æ±‚: {max_gpu_memory} MB")
        self.logger.info(f"  æ€»GPUå†…å­˜éœ€æ±‚ï¼ˆå¦‚æœä¸²è¡Œï¼‰: {total_gpu_memory} MB")
        
        if self.enable_benchmarking:
            self.logger.info("\nâœ… åŸºå‡†æµ‹è¯•å·²å¯ç”¨ï¼Œå°†æ”¶é›†æ€§èƒ½æŒ‡æ ‡")
        else:
            self.logger.info("\nâš ï¸  åŸºå‡†æµ‹è¯•æœªå¯ç”¨")
    
    def generate_report(self, results: Dict[str, List[ExperimentResult]]) -> str:
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆå®éªŒæŠ¥å‘Š...")
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_benchmarking and self.benchmark:
            try:
                self.logger.info("è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
                benchmark_results = self._run_benchmark_suite()
                self.logger.info("åŸºå‡†æµ‹è¯•å®Œæˆ")
            except Exception as e:
                self.logger.warning(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
                benchmark_results = {}
        
        else:
            benchmark_results = {}
        
        # æ„å»ºæŠ¥å‘Šæ•°æ®
        all_results = results['completed'] + results['failed'] + results['skipped'] + results.get('running', [])
        
        report_data = {
            'summary': {
                'total_experiments': len(all_results),
                'completed': len(results['completed']),
                'failed': len(results['failed']),
                'skipped': len(results['skipped']),
                'running': len(results.get('running', [])),
                'success_rate': len(results['completed']) / max(1, len(results['completed']) + len(results['failed'])) * 100
            },
            'completed_experiments': [asdict(exp) for exp in results['completed']],
            'failed_experiments': [asdict(exp) for exp in results['failed']],
            'skipped_experiments': [asdict(exp) for exp in results['skipped']],
            'running_experiments': [asdict(exp) for exp in results.get('running', [])],
            'benchmark_results': benchmark_results,
            'generated_at': datetime.now().isoformat(),
            'resource_usage_summary': self.resource_manager.get_resource_status()
        }
        
        # è®¡ç®—æ€»è€—æ—¶å’Œæ€§èƒ½æŒ‡æ ‡
        total_duration = sum(exp.actual_duration_hours for exp in results['completed'])
        avg_duration = total_duration / max(1, len(results['completed']))
        
        report_data['summary']['total_duration_hours'] = round(total_duration, 2)
        report_data['summary']['avg_duration_hours'] = round(avg_duration, 2)
        
        # è®¡ç®—æ¯ä¸ªæ•°æ®é›†ç»„åˆçš„æ€§èƒ½
        combination_stats = defaultdict(list)
        for exp in results['completed']:
            # è¿™é‡Œéœ€è¦ä»å®éªŒç»“æœä¸­æå–æ•°æ®é›†ç»„åˆä¿¡æ¯
            combination_stats['all'].append(exp.actual_duration_hours)
        
        report_data['combination_performance'] = {
            combo: {
                'count': len(durations),
                'avg_duration': sum(durations) / len(durations),
                'min_duration': min(durations),
                'max_duration': max(durations)
            } for combo, durations in combination_stats.items() if durations
        }
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_path = self.results_dir / f"experiment_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w') as f:
            json.dump(report_data, f, indent=2)
        
        # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        readable_report_path = self.results_dir / f"experiment_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(readable_report_path, 'w') as f:
            f.write("å¤šæ•°æ®é›†å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå®éªŒæŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"å®éªŒæ€»ç»“:\n")
            f.write(f"  æ€»å®éªŒæ•°: {report_data['summary']['total_experiments']}\n")
            f.write(f"  æˆåŠŸå®Œæˆ: {report_data['summary']['completed']}\n")
            f.write(f"  å¤±è´¥: {report_data['summary']['failed']}\n")
            f.write(f"  è·³è¿‡: {report_data['summary']['skipped']}\n")
            f.write(f"  æˆåŠŸç‡: {report_data['summary']['success_rate']:.1f}%\n")
            f.write(f"  æ€»è€—æ—¶: {report_data['summary']['total_duration_hours']:.2f} å°æ—¶\n\n")
            
            if results['completed']:
                f.write("æˆåŠŸå®Œæˆçš„å®éªŒ:\n")
                for exp in results['completed']:
                    f.write(f"  - {exp['dataset']}_{exp['variant']}: {exp.get('actual_duration_hours', 'N/A')} å°æ—¶\n")
                f.write("\n")
            
            if results['failed']:
                f.write("å¤±è´¥çš„å®éªŒ:\n")
                for exp in results['failed']:
                    error_msg = exp.get('error', 'æœªçŸ¥é”™è¯¯')
                    f.write(f"  - {exp['dataset']}_{exp['variant']}: {error_msg}\n")
                f.write("\n")
            
            f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report_data['generated_at']}\n")
        
        self.logger.info("å®éªŒæŠ¥å‘Šå·²ä¿å­˜:")
        self.logger.info(f"  è¯¦ç»†æŠ¥å‘Š: {report_path}")
        self.logger.info(f"  æ‘˜è¦æŠ¥å‘Š: {readable_report_path}")
        
        return str(readable_report_path)
    
    def _run_benchmark_suite(self) -> Dict[str, Any]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•å¥—ä»¶"""
        if not self.benchmark:
            return {}
        
        benchmark_results = {}
        
        try:
            # è¿è¡Œå†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•
            self.logger.info("è¿è¡Œå†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•...")
            memory_results = self.benchmark.test_memory_usage()
            benchmark_results['memory'] = memory_results
            
            # è¿è¡Œè®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•
            self.logger.info("è¿è¡Œè®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•...")
            speed_results = self.benchmark.test_training_speed()
            benchmark_results['training_speed'] = speed_results
            
            # è¿è¡Œååé‡åŸºå‡†æµ‹è¯•
            self.logger.info("è¿è¡Œååé‡åŸºå‡†æµ‹è¯•...")
            throughput_results = self.benchmark.test_throughput()
            benchmark_results['throughput'] = throughput_results
            
            # è¿è¡Œæ”¶æ•›æ€§èƒ½åŸºå‡†æµ‹è¯•
            self.logger.info("è¿è¡Œæ”¶æ•›æ€§èƒ½åŸºå‡†æµ‹è¯•...")
            convergence_results = self.benchmark.test_convergence()
            benchmark_results['convergence'] = convergence_results
            
            # ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š
            benchmark_report = self.benchmark.generate_report()
            benchmark_results['report_summary'] = benchmark_report
            
            self.logger.info("æ‰€æœ‰åŸºå‡†æµ‹è¯•å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"åŸºå‡†æµ‹è¯•å¥—ä»¶æ‰§è¡Œå¤±è´¥: {e}")
            benchmark_results['error'] = str(e)
        
        return benchmark_results
    
    def run_quick_validation(self, dataset_sample_size: int = 100) -> Dict[str, Any]:
        """
        è¿è¡Œå¿«é€ŸéªŒè¯æµ‹è¯•ï¼Œç”¨äºéªŒè¯å®éªŒé…ç½®çš„æ­£ç¡®æ€§
        
        Args:
            dataset_sample_size: ç”¨äºå¿«é€Ÿæµ‹è¯•çš„æ ·æœ¬æ•°é‡
            
        Returns:
            éªŒè¯ç»“æœ
        """
        self.logger.info("å¼€å§‹å¿«é€ŸéªŒè¯æµ‹è¯•...")
        
        validation_results = {
            'config_validation': {},
            'data_loading_validation': {},
            'model_initialization_validation': {},
            'training_step_validation': {}
        }
        
        try:
            # 1. é…ç½®éªŒè¯
            self.logger.info("éªŒè¯åŸºç¡€é…ç½®...")
            base_config = load_config(self.base_config_path)
            validation_results['config_validation']['base_config_valid'] = True
            validation_results['config_validation']['required_keys'] = [
                'data', 'model', 'task', 'trainer'
            ]
            validation_results['config_validation']['missing_keys'] = []
            
            for key in validation_results['config_validation']['required_keys']:
                if key not in base_config:
                    validation_results['config_validation']['missing_keys'].append(key)
            
            # 2. æ•°æ®åŠ è½½éªŒè¯
            self.logger.info("éªŒè¯æ•°æ®åŠ è½½...")
            datasets = self.discover_datasets()
            filtered_datasets = self.filter_datasets(datasets, min_samples=dataset_sample_size)
            
            validation_results['data_loading_validation']['total_datasets'] = len(datasets)
            validation_results['data_loading_validation']['valid_datasets'] = len(filtered_datasets)
            validation_results['data_loading_validation']['datasets'] = [ds.name for ds in filtered_datasets[:5]]  # åªæ˜¾ç¤ºå‰5ä¸ª
            
            if filtered_datasets:
                # æµ‹è¯•æ•°æ®åŠ è½½
                sample_dataset = filtered_datasets[0]
                try:
                    # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„æ•°æ®åŠ è½½æµ‹è¯•
                    validation_results['data_loading_validation']['sample_loading_success'] = True
                except Exception as e:
                    validation_results['data_loading_validation']['sample_loading_success'] = False
                    validation_results['data_loading_validation']['sample_loading_error'] = str(e)
            
            # 3. æ¨¡å‹åˆå§‹åŒ–éªŒè¯
            self.logger.info("éªŒè¯æ¨¡å‹åˆå§‹åŒ–...")
            if filtered_datasets:
                try:
                    # åˆ›å»ºä¸€ä¸ªæœ€å°çš„å®éªŒé…ç½®æ¥æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
                    test_combinations = self.generate_dataset_combinations(filtered_datasets[:1], ['single'])
                    test_experiments = self.generate_experiment_configs(
                        filtered_datasets[:1], 
                        [{'name': 'minimal_test', 'overrides': {'trainer.epochs': 1, 'data.batch_size': 2}}],
                        test_combinations[:1],
                        enable_ablation=False
                    )
                    
                    if test_experiments:
                        validation_results['model_initialization_validation']['config_generation_success'] = True
                        validation_results['model_initialization_validation']['test_experiments_count'] = len(test_experiments)
                    else:
                        validation_results['model_initialization_validation']['config_generation_success'] = False
                        
                except Exception as e:
                    validation_results['model_initialization_validation']['config_generation_success'] = False
                    validation_results['model_initialization_validation']['config_generation_error'] = str(e)
            
            # 4. èµ„æºæ£€æŸ¥
            self.logger.info("æ£€æŸ¥ç³»ç»Ÿèµ„æº...")
            resource_status = self.resource_manager.get_resource_status()
            validation_results['resource_validation'] = resource_status
            validation_results['resource_validation']['sufficient_resources'] = (
                resource_status['available_ram_mb'] > 4000 and  # è‡³å°‘4GB RAM
                (not resource_status['gpus'] or len(resource_status['gpus']) > 0)  # æœ‰GPUæˆ–è€…æ²¡æœ‰GPUè¦æ±‚
            )
            
            validation_results['overall_validation_success'] = (
                validation_results['config_validation']['base_config_valid'] and
                len(validation_results['config_validation']['missing_keys']) == 0 and
                validation_results['data_loading_validation']['valid_datasets'] > 0 and
                validation_results['resource_validation']['sufficient_resources']
            )
            
            if validation_results['overall_validation_success']:
                self.logger.info("âœ… å¿«é€ŸéªŒè¯æµ‹è¯•é€šè¿‡")
            else:
                self.logger.warning("âš ï¸  å¿«é€ŸéªŒè¯æµ‹è¯•å‘ç°é—®é¢˜")
            
        except Exception as e:
            self.logger.error(f"å¿«é€ŸéªŒè¯æµ‹è¯•å¤±è´¥: {e}")
            validation_results['validation_error'] = str(e)
            validation_results['overall_validation_success'] = False
        
        return validation_results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤šæ•°æ®é›†å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå®éªŒ")
    
    parser.add_argument(
        '--base_config',
        default='configs/id_contrastive/pretrain.yaml',
        help='åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--metadata_dir',
        default='data',
        help='metadataæ–‡ä»¶ç›®å½•'
    )
    
    parser.add_argument(
        '--results_dir',
        default='save/multi_dataset_experiments',
        help='ç»“æœä¿å­˜ç›®å½•'
    )
    
    parser.add_argument(
        '--include_datasets',
        nargs='*',
        help='åŒ…å«çš„æ•°æ®é›†åç§°æ¨¡å¼'
    )
    
    parser.add_argument(
        '--exclude_datasets', 
        nargs='*',
        help='æ’é™¤çš„æ•°æ®é›†åç§°æ¨¡å¼'
    )
    
    parser.add_argument(
        '--min_samples',
        type=int,
        default=100,
        help='æœ€å°æ ·æœ¬æ•°è¦æ±‚'
    )
    
    parser.add_argument(
        '--variants',
        nargs='*',
        choices=['default', 'large_window', 'small_window', 'low_temp', 'high_temp', 'high_lr', 'large_batch', 'small_batch'],
        default=['default'],
        help='é…ç½®å˜ä½“'
    )
    
    parser.add_argument(
        '--combination_strategies',
        nargs='*',
        choices=['single', 'cross_domain', 'same_domain', 'all_datasets'],
        default=['single', 'cross_domain'],
        help='æ•°æ®é›†ç»„åˆç­–ç•¥'
    )
    
    parser.add_argument(
        '--domain_types',
        nargs='*',
        help='é™åˆ¶ç‰¹å®šçš„åŸŸç±»å‹ (bearing, gear, motor, etc.)'
    )
    
    parser.add_argument(
        '--enable_ablation',
        action='store_true',
        help='å¯ç”¨æ¶ˆèå®éªŒå˜ä½“'
    )
    
    parser.add_argument(
        '--enable_benchmarking',
        action='store_true',
        default=True,
        help='å¯ç”¨æ€§èƒ½åŸºå‡†æµ‹è¯•'
    )
    
    parser.add_argument(
        '--quick_validation',
        action='store_true',
        help='åªè¿è¡Œå¿«é€ŸéªŒè¯ï¼Œä¸æ‰§è¡Œå®é™…å®éªŒ'
    )
    
    parser.add_argument(
        '--parallel',
        action='store_true',
        help='å¹¶è¡Œè¿è¡Œå®éªŒ'
    )
    
    parser.add_argument(
        '--max_parallel',
        type=int,
        default=2,
        help='æœ€å¤§å¹¶è¡Œæ•°'
    )
    
    parser.add_argument(
        '--timeout',
        type=int,
        default=24,
        help='å•ä¸ªå®éªŒè¶…æ—¶æ—¶é—´ï¼ˆå°æ—¶ï¼‰'
    )
    
    parser.add_argument(
        '--dry_run',
        action='store_true',
        help='åªè¾“å‡ºå®éªŒè®¡åˆ’ï¼Œä¸å®é™…è¿è¡Œ'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = MultiDatasetExperimentRunner(
        base_config_path=args.base_config,
        metadata_dir=args.metadata_dir,
        results_dir=args.results_dir,
        dry_run=args.dry_run,
        enable_benchmarking=args.enable_benchmarking,
        max_concurrent_experiments=args.max_parallel
    )
    
    try:
        # å¿«é€ŸéªŒè¯æ¨¡å¼
        if args.quick_validation:
            runner.logger.info("ğŸ” è¿è¡Œå¿«é€ŸéªŒè¯æ¨¡å¼...")
            validation_results = runner.run_quick_validation(args.min_samples)
            
            if validation_results['overall_validation_success']:
                runner.logger.info("âœ… å¿«é€ŸéªŒè¯é€šè¿‡ï¼Œç³»ç»Ÿé…ç½®æ­£ç¡®")
                return 0
            else:
                runner.logger.error("âŒ å¿«é€ŸéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
                print(json.dumps(validation_results, indent=2))
                return 1
        
        # å‘ç°æ•°æ®é›†
        datasets = runner.discover_datasets()
        
        if not datasets:
            runner.logger.error("æ²¡æœ‰å‘ç°å¯ç”¨çš„æ•°æ®é›†")
            return 1
        
        # è¿‡æ»¤æ•°æ®é›†
        filtered_datasets = runner.filter_datasets(
            datasets,
            include_patterns=args.include_datasets,
            exclude_patterns=args.exclude_datasets,
            min_samples=args.min_samples,
            ready_only=True,
            domain_types=args.domain_types
        )
        
        if not filtered_datasets:
            runner.logger.error("è¿‡æ»¤åæ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†")
            return 1
        
        # ç”Ÿæˆæ•°æ®é›†ç»„åˆ
        dataset_combinations = runner.generate_dataset_combinations(
            filtered_datasets, 
            args.combination_strategies
        )
        
        if not dataset_combinations:
            runner.logger.error("æ²¡æœ‰ç”Ÿæˆä»»ä½•æ•°æ®é›†ç»„åˆ")
            return 1
        
        # ç”Ÿæˆé…ç½®å˜ä½“
        config_variants = []
        variant_configs = {
            'default': {'name': 'default', 'overrides': {}, 'priority': 1},
            'large_window': {'name': 'large_window', 'overrides': {'data.window_size': 2048, 'data.stride': 1024}, 'priority': 2},
            'small_window': {'name': 'small_window', 'overrides': {'data.window_size': 512, 'data.stride': 256}, 'priority': 2},
            'low_temp': {'name': 'low_temp', 'overrides': {'task.temperature': 0.05}, 'priority': 3},
            'high_temp': {'name': 'high_temp', 'overrides': {'task.temperature': 0.3}, 'priority': 3},
            'high_lr': {'name': 'high_lr', 'overrides': {'task.lr': 5e-3}, 'priority': 3},
            'large_batch': {'name': 'large_batch', 'overrides': {'data.batch_size': 64}, 'priority': 2},
            'small_batch': {'name': 'small_batch', 'overrides': {'data.batch_size': 8}, 'priority': 2}
        }
        
        for variant in args.variants:
            if variant in variant_configs:
                config_variants.append(variant_configs[variant])
        
        # ç”Ÿæˆå®éªŒé…ç½®
        experiments = runner.generate_experiment_configs(
            filtered_datasets, 
            config_variants,
            dataset_combinations,
            enable_ablation=args.enable_ablation
        )
        
        if not experiments:
            runner.logger.error("æ²¡æœ‰ç”Ÿæˆä»»ä½•å®éªŒé…ç½®")
            return 1
        
        runner.logger.info(f"æˆåŠŸç”Ÿæˆ {len(experiments)} ä¸ªå®éªŒé…ç½®")
        
        # è¿è¡Œå®éªŒ
        results = runner.run_experiments(
            experiments,
            parallel=args.parallel,
            max_parallel=args.max_parallel,
            timeout_hours=args.timeout
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = runner.generate_report(results)
        
        # æ‰“å°æ‰§è¡Œæ‘˜è¦
        runner.logger.info(f"ğŸ‰ å¤šæ•°æ®é›†å¯¹æ¯”å­¦ä¹ å®éªŒå®Œæˆ!")
        runner.logger.info(f"æŠ¥å‘Šä¿å­˜åœ¨: {report_path}")
        
        # æ‰“å°ç»“æœæ‘˜è¦
        total_experiments = len(results['completed']) + len(results['failed']) + len(results['skipped'])
        success_rate = len(results['completed']) / max(1, len(results['completed']) + len(results['failed'])) * 100
        
        runner.logger.info(f"\næ‰§è¡Œæ‘˜è¦:")
        runner.logger.info(f"  æ€»å®éªŒæ•°: {total_experiments}")
        runner.logger.info(f"  æˆåŠŸå®Œæˆ: {len(results['completed'])}")
        runner.logger.info(f"  å¤±è´¥: {len(results['failed'])}")
        runner.logger.info(f"  è·³è¿‡: {len(results['skipped'])}")
        runner.logger.info(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        
        if results['failed']:
            runner.logger.warning("âš ï¸  å­˜åœ¨å¤±è´¥çš„å®éªŒï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")
        
        return 0 if success_rate > 50 else 1  # æˆåŠŸç‡è¶…è¿‡50%è®¤ä¸ºæ•´ä½“æˆåŠŸ
        
    except KeyboardInterrupt:
        runner.logger.warning("âš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
        return 130  # SIGINT exit code
    except Exception as e:
        runner.logger.error(f"âŒ å®éªŒè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    main()