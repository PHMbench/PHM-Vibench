# é˜¶æ®µ3: å®éªŒæ‰§è¡ŒæŒ‡å—

ContrastiveIDTaskå®éªŒè®¾è®¡ã€æ‰§è¡Œå’Œç›‘æ§çš„å®Œæ•´æŒ‡å—ã€‚

## ğŸ“‹ æœ¬é˜¶æ®µç›®æ ‡

- [x] è®¾è®¡ç³»ç»Ÿæ€§å®éªŒæ–¹æ¡ˆ
- [x] æ‰§è¡Œå•æ•°æ®é›†å’Œè·¨æ•°æ®é›†å®éªŒ
- [x] è¿›è¡Œå…¨é¢çš„æ¶ˆèç ”ç©¶
- [x] ç›‘æ§å’Œä¼˜åŒ–å®éªŒè¿‡ç¨‹

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å•æ•°æ®é›†å®éªŒ
```bash
python multi_dataset_runner.py \
    --datasets CWRU \
    --strategy single \
    --config ../examples/config_templates/single_dataset.yaml \
    --output_dir results/single_cwru
```

### 2. è·¨æ•°æ®é›†åŸŸæ³›åŒ–
```bash
python multi_dataset_runner.py \
    --datasets CWRU,XJTU \
    --strategy cross_domain \
    --config ../examples/config_templates/cross_domain.yaml \
    --output_dir results/cross_domain
```

### 3. æ¶ˆèç ”ç©¶
```bash
python ablation_study.py \
    --config base_config.yaml \
    --parameters temperature,window_size,batch_size \
    --output_dir results/ablation
```

## ğŸ› ï¸ æ ¸å¿ƒå·¥å…·è¯¦è§£

### multi_dataset_runner.py
**ä¸»è¦åŠŸèƒ½**: ç»Ÿä¸€çš„å¤šæ•°æ®é›†å®éªŒç®¡ç†å™¨

#### å®éªŒç­–ç•¥
```bash
# å•æ•°æ®é›†å®éªŒ
python multi_dataset_runner.py --strategy single --datasets CWRU

# è·¨æ•°æ®é›†åŸŸæ³›åŒ– (æºâ†’ç›®æ ‡)
python multi_dataset_runner.py --strategy cross_domain --datasets CWRU,XJTU

# å¤šæ•°æ®é›†è”åˆè®­ç»ƒ
python multi_dataset_runner.py --strategy multi_dataset --datasets CWRU,XJTU,PU

# åŸŸè‡ªé€‚åº”å®éªŒ
python multi_dataset_runner.py --strategy domain_adaptation --datasets CWRU,XJTU
```

#### é«˜çº§é€‰é¡¹
```bash
# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå®éªŒ
python multi_dataset_runner.py \
    --datasets CWRU,XJTU,PU,FEMTO \
    --strategy cross_domain \
    --parallel \
    --max_workers 4

# è‡ªåŠ¨è¶…å‚æ•°ç½‘æ ¼æœç´¢
python multi_dataset_runner.py \
    --datasets CWRU \
    --strategy single \
    --grid_search \
    --param_grid config/grid_search.yaml

# ç»§ç»­ä¸­æ–­çš„å®éªŒ
python multi_dataset_runner.py --resume --checkpoint_dir results/interrupted_exp/
```

### ablation_study.py
**ä¸»è¦åŠŸèƒ½**: ç³»ç»Ÿæ€§è¶…å‚æ•°æ¶ˆèç ”ç©¶

#### å•å‚æ•°æ‰«æ
```bash
# æ¸©åº¦å‚æ•°æ¶ˆè
python ablation_study.py \
    --config base_config.yaml \
    --param_sweep temperature 0.01,0.05,0.07,0.1,0.2,0.5 \
    --dataset CWRU \
    --output_dir results/temp_ablation

# çª—å£å¤§å°æ¶ˆè
python ablation_study.py \
    --config base_config.yaml \
    --param_sweep window_size 128,256,512,1024 \
    --dataset CWRU
```

#### å¤šå‚æ•°ç»„åˆ
```bash
# å¤šå‚æ•°ç½‘æ ¼æœç´¢
python ablation_study.py \
    --config base_config.yaml \
    --parameters temperature,window_size,batch_size \
    --max_combinations 50 \
    --optimization_metric accuracy

# è´å¶æ–¯ä¼˜åŒ–
python ablation_study.py \
    --config base_config.yaml \
    --parameters temperature,lr,weight_decay \
    --optimizer bayesian \
    --n_trials 100
```

## ğŸ“Š å®éªŒè®¾è®¡æ–¹æ¡ˆ

### ğŸ¯ åŸºç¡€å®éªŒçŸ©é˜µ

| å®éªŒç±»å‹ | æ•°æ®é›†ç»„åˆ | ç›®çš„ | é¢„æœŸç»“æœ |
|----------|------------|------|----------|
| Baseline | CWRUå•ç‹¬ | å»ºç«‹åŸºå‡†æ€§èƒ½ | ~75-85% |
| Cross-Domain | CWRUâ†’XJTU | æµ‹è¯•åŸŸæ³›åŒ–èƒ½åŠ› | ~60-75% |
| Multi-Source | CWRU+XJTUâ†’PU | å¤šæºåŸŸé¢„è®­ç»ƒ | ~70-80% |
| Few-Shot | CWRUâ†’XJTU(5%) | å°‘æ ·æœ¬é€‚åº” | ~50-65% |

### ğŸ§ª æ¶ˆèç ”ç©¶è®¾è®¡

#### æ ¸å¿ƒå‚æ•°æ¶ˆè
```yaml
# config/ablation_params.yaml
temperature:
  values: [0.01, 0.03, 0.05, 0.07, 0.1, 0.15, 0.2, 0.3, 0.5]
  priority: high

window_size:
  values: [64, 128, 256, 512, 1024]
  priority: high

num_window:
  values: [1, 2, 3, 4, 5]
  priority: medium

batch_size:
  values: [8, 16, 32, 64]
  priority: medium
```

#### æ¶æ„ç»„ä»¶æ¶ˆè
```bash
# æŸå¤±å‡½æ•°æ¶ˆè
python ablation_study.py --param_sweep loss_type infonce,simclr,triplet,contrastive

# é‡‡æ ·ç­–ç•¥æ¶ˆè
python ablation_study.py --param_sweep sampling_strategy random,sequential,evenly_spaced

# ç‰¹å¾ç»´åº¦æ¶ˆè
python ablation_study.py --param_sweep d_model 32,64,128,256,512
```

## ğŸ“ˆ å®éªŒç›‘æ§ä¸ç®¡ç†

### ğŸ” å®éªŒçŠ¶æ€ç›‘æ§
```bash
# æŸ¥çœ‹æ‰€æœ‰å®éªŒçŠ¶æ€
python multi_dataset_runner.py --status --output_dir results/

# å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦
tail -f results/experiment_name/training.log

# GPUä½¿ç”¨ç›‘æ§
watch -n 1 nvidia-smi

# å®éªŒèµ„æºç›‘æ§
python monitor_experiments.py --output_dir results/ --refresh 10
```

### ğŸ“Š å®éªŒè¿›åº¦å¯è§†åŒ–
```python
# å¯åŠ¨å®éªŒç›‘æ§ç•Œé¢
from multi_dataset_runner import ExperimentMonitor

monitor = ExperimentMonitor('results/')
monitor.start_web_interface(port=8080)
# è®¿é—® http://localhost:8080 æŸ¥çœ‹è¿›åº¦
```

### ğŸš¨ å®éªŒå¼‚å¸¸å¤„ç†
```bash
# è‡ªåŠ¨é‡å¯å¤±è´¥å®éªŒ
python multi_dataset_runner.py --auto_restart --check_interval 600

# å®éªŒå¥åº·æ£€æŸ¥
python experiment_health_check.py --results_dir results/ --fix_issues
```

## ğŸ¯ å®éªŒæœ€ä½³å®è·µ

### ğŸ”„ å®éªŒç‰ˆæœ¬ç®¡ç†
```bash
# æ¯ä¸ªå®éªŒä¿å­˜å®Œæ•´é…ç½®
export EXPERIMENT_NAME="cwru_baseline_v1.0"
python multi_dataset_runner.py \
    --config base_config.yaml \
    --experiment_name $EXPERIMENT_NAME \
    --save_config \
    --git_commit

# å®éªŒç»“æœç‰ˆæœ¬æ§åˆ¶
git add results/$EXPERIMENT_NAME/
git commit -m "Add experiment: $EXPERIMENT_NAME"
git tag exp-$EXPERIMENT_NAME
```

### ğŸ“ å®éªŒè®°å½•ç®¡ç†
```python
# experiment_logger.py
class ExperimentLogger:
    def __init__(self, experiment_name):
        self.name = experiment_name
        self.start_time = time.time()

    def log_hyperparams(self, config):
        """è®°å½•è¶…å‚æ•°é…ç½®"""

    def log_metrics(self, metrics, step):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡"""

    def log_artifacts(self, file_paths):
        """è®°å½•å®éªŒäº§ç‰©"""
```

### ğŸ² éšæœºç§å­ç®¡ç†
```python
# ç¡®ä¿å®éªŒå¯é‡ç°
def set_deterministic_training():
    torch.manual_seed(42)
    np.random.seed(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

## ğŸ§° é«˜çº§å®éªŒæŠ€å·§

### å¹¶è¡Œå®éªŒæ‰§è¡Œ
```bash
# ä½¿ç”¨GNU parallelæ‰§è¡Œå¤šä¸ªç‹¬ç«‹å®éªŒ
cat experiment_list.txt | parallel -j 4 python multi_dataset_runner.py --config {}

# åˆ†å¸ƒå¼è®­ç»ƒ
torchrun --nproc_per_node=4 multi_dataset_runner.py --distributed

# é›†ç¾¤æ‰¹å¤„ç†
sbatch --array=1-10 run_ablation_array.sh
```

### æ—©åœå’Œæ£€æŸ¥ç‚¹ç®¡ç†
```python
# æ™ºèƒ½æ—©åœç­–ç•¥
early_stopping_config = {
    'patience': 10,
    'min_delta': 0.001,
    'monitor': 'val_accuracy',
    'mode': 'max'
}

# æ£€æŸ¥ç‚¹ä¿å­˜ç­–ç•¥
checkpoint_config = {
    'save_top_k': 3,
    'monitor': 'val_accuracy',
    'every_n_epochs': 5,
    'save_last': True
}
```

### åŠ¨æ€è¶…å‚æ•°è°ƒæ•´
```python
# å­¦ä¹ ç‡è°ƒåº¦
scheduler_config = {
    'type': 'cosine_annealing',
    'T_max': 100,
    'eta_min': 1e-6
}

# æ¸©åº¦å‚æ•°è¡°å‡
temperature_schedule = {
    'initial': 0.1,
    'decay_rate': 0.95,
    'decay_steps': 10
}
```

## ğŸ“Š å®éªŒç»“æœåˆ†æ

### å®éªŒç»“æœæ±‡æ€»
```bash
# ç”Ÿæˆå®éªŒæ±‡æ€»æŠ¥å‘Š
python analyze_experiments.py \
    --results_dir results/ \
    --output_report experiment_summary.html

# å¯¼å‡ºç»“æœåˆ°è¡¨æ ¼
python export_results.py \
    --results_dir results/ \
    --format csv,json,latex \
    --metrics accuracy,f1_score,precision,recall
```

### ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
```python
# å¤šå®éªŒç»Ÿè®¡åˆ†æ
from scipy import stats
from multi_dataset_runner import ResultsAnalyzer

analyzer = ResultsAnalyzer()

# åŠ è½½å¤šæ¬¡è¿è¡Œç»“æœ
results_A = analyzer.load_experiment_results('results/method_A/')
results_B = analyzer.load_experiment_results('results/method_B/')

# t-test
t_stat, p_value = stats.ttest_ind(results_A, results_B)
print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: p-value = {p_value:.4f}")

# æ•ˆåº”é‡è®¡ç®—
cohen_d = analyzer.compute_effect_size(results_A, results_B)
print(f"Cohen's d: {cohen_d:.4f}")
```

## ğŸ”§ æ•…éšœæ’é™¤

### âŒ è®­ç»ƒä¸æ”¶æ•›
```bash
# è¯Šæ–­è®­ç»ƒé—®é¢˜
python diagnose_training.py --experiment_dir results/problematic_exp/

# è°ƒè¯•å»ºè®®
python ablation_study.py \
    --config debug_config.yaml \
    --param_sweep lr 1e-4,5e-4,1e-3,5e-3 \
    --debug_mode
```

### âŒ å†…å­˜æº¢å‡º (OOM)
```python
# åŠ¨æ€æ‰¹å¤§å°è°ƒæ•´
def find_optimal_batch_size(initial_size=32):
    for batch_size in [initial_size//2, initial_size//4, initial_size//8]:
        try:
            run_training(batch_size=batch_size)
            return batch_size
        except torch.cuda.OutOfMemoryError:
            continue
    raise RuntimeError("æ— æ³•æ‰¾åˆ°åˆé€‚çš„æ‰¹å¤§å°")
```

### âŒ å®éªŒä¸­æ–­æ¢å¤
```bash
# è‡ªåŠ¨æ¢å¤ä¸­æ–­çš„å®éªŒ
python multi_dataset_runner.py \
    --resume_from results/interrupted_exp/checkpoints/last.ckpt \
    --continue_training
```

## ğŸ¯ è¿›å…¥ä¸‹ä¸€é˜¶æ®µ

### æ£€æŸ¥æ¸…å•
- [ ] åŸºçº¿å®éªŒå®Œæˆä¸”æ€§èƒ½åˆç†
- [ ] è·¨æ•°æ®é›†å®éªŒæ˜¾ç¤ºåŸŸæ³›åŒ–èƒ½åŠ›
- [ ] æ¶ˆèç ”ç©¶è¦†ç›–å…³é”®è¶…å‚æ•°
- [ ] å®éªŒç»“æœå·²ä¿å­˜å¹¶ç‰ˆæœ¬æ§åˆ¶

### å®éªŒè´¨é‡è¯„ä¼°
```bash
# éªŒè¯å®éªŒå®Œæ•´æ€§
python validate_experiments.py --results_dir results/ --check_completeness

# ç”Ÿæˆå®éªŒè´¨é‡æŠ¥å‘Š
python experiment_quality_check.py --results_dir results/
```

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨
```bash
# è¿›å…¥ç»“æœåˆ†æé˜¶æ®µ
cd ../04_analysis/

# å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•
python performance_benchmark.py --experiments_dir ../03_experiments/results/
```

## ğŸ“š æ·±å…¥å­¦ä¹ 

### å®éªŒè®¾è®¡ç†è®º
- **å¯¹ç…§å®éªŒè®¾è®¡**: ç¡®ä¿å•ä¸€å˜é‡æ§åˆ¶
- **å¤šé‡æ¯”è¾ƒæ ¡æ­£**: Bonferroni, FDRæ ¡æ­£æ–¹æ³•
- **æ•ˆåº”é‡è®¡ç®—**: Cohen's d, eta squared
- **ç½®ä¿¡åŒºé—´**: Bootstrapæ–¹æ³•

### ç›¸å…³å·¥å…·å’Œæ¡†æ¶
- **Weights & Biases**: å®éªŒè·Ÿè¸ªå’Œå¯è§†åŒ–
- **MLflow**: æœºå™¨å­¦ä¹ å®éªŒç®¡ç†
- **Optuna**: è¶…å‚æ•°ä¼˜åŒ–æ¡†æ¶
- **Ray Tune**: åˆ†å¸ƒå¼è¶…å‚æ•°è°ƒä¼˜

---

**ğŸ‰ æ­å–œï¼æ‚¨å·²æŒæ¡å®éªŒæ‰§è¡Œçš„æ ¸å¿ƒæŠ€èƒ½ã€‚**

å¥½çš„å®éªŒè®¾è®¡æ˜¯ç§‘å­¦ç ”ç©¶çš„åŸºç¡€ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„å®éªŒï¼Œæ‚¨å°†è·å¾—æœ‰è¯´æœåŠ›çš„ç ”ç©¶ç»“æœã€‚

è®©æˆ‘ä»¬è¿›å…¥[ç»“æœåˆ†æé˜¶æ®µ](../04_analysis/README.md)æ·±å…¥æŒ–æ˜å®éªŒæ´å¯Ÿã€‚