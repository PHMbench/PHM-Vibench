# é˜¶æ®µ5: è®ºæ–‡æ”¯æ’‘æŒ‡å—

å­¦æœ¯è®ºæ–‡æ’°å†™æ”¯æŒã€åŸºå‡†å¯¹æ¯”å’Œå‘è¡¨å‡†å¤‡çš„å®Œæ•´æŒ‡å—ã€‚

## ğŸ“‹ æœ¬é˜¶æ®µç›®æ ‡

- [x] ä¸åŸºçº¿æ–¹æ³•è¿›è¡Œå…¨é¢å¯¹æ¯”
- [x] ç”Ÿæˆè®ºæ–‡çº§çš„è¡¨æ ¼å’Œå›¾è¡¨
- [x] è¿›è¡Œä¸¥æ ¼çš„ç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯
- [x] å‡†å¤‡å¯é‡ç°æ€§ææ–™

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºçº¿æ–¹æ³•å¯¹æ¯”
```bash
python baseline_comparison.py \
    --methods raw_signal,fft_features,cnn,lstm,transformer,contrastive_id \
    --datasets CWRU,XJTU,PU \
    --cross_validation 5 \
    --statistical_test \
    --output_dir paper_results/
```

### 2. ç”Ÿæˆè®ºæ–‡è¡¨æ ¼
```bash
python generate_paper_tables.py \
    --results_dir ../04_analysis/benchmarks/ \
    --format latex \
    --style ieee \
    --output_dir paper_results/tables/
```

### 3. ç”Ÿæˆé«˜è´¨é‡å›¾è¡¨
```bash
python generate_paper_figures.py \
    --results_dir ../04_analysis/benchmarks/ \
    --figure_types training_curves,confusion_matrix,parameter_sensitivity \
    --style ieee \
    --dpi 300 \
    --output_dir paper_results/figures/
```

## ğŸ› ï¸ æ ¸å¿ƒå·¥å…·è¯¦è§£

### baseline_comparison.py
**ä¸»è¦åŠŸèƒ½**: ä¸ä¼ ç»Ÿæ–¹æ³•çš„å…¨é¢æ€§èƒ½å¯¹æ¯”

#### åŸºçº¿æ–¹æ³•é…ç½®
```python
# baseline_methods.py
baseline_methods = {
    'raw_signal': {
        'description': 'åŸå§‹ä¿¡å·ç›´æ¥åˆ†ç±»',
        'model': 'RandomForest',
        'features': 'raw_time_series'
    },
    'fft_features': {
        'description': 'FFTé¢‘åŸŸç‰¹å¾',
        'model': 'SVM',
        'features': 'frequency_domain'
    },
    'statistical_features': {
        'description': 'ç»Ÿè®¡ç‰¹å¾æå–',
        'model': 'XGBoost',
        'features': ['mean', 'std', 'skewness', 'kurtosis', 'rms']
    },
    'cnn_1d': {
        'description': 'ä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œ',
        'model': 'CNN1D',
        'architecture': 'conv1d_x3_dense_x2'
    },
    'lstm': {
        'description': 'é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ',
        'model': 'LSTM',
        'architecture': 'bidirectional_lstm'
    },
    'transformer': {
        'description': 'Transformerç¼–ç å™¨',
        'model': 'TransformerEncoder',
        'architecture': 'multihead_attention'
    }
}
```

#### å¯¹æ¯”å®éªŒæ‰§è¡Œ
```bash
# å•æ•°æ®é›†åŸºçº¿å¯¹æ¯”
python baseline_comparison.py \
    --dataset CWRU \
    --methods all \
    --n_runs 5 \
    --save_predictions

# è·¨æ•°æ®é›†åŸŸæ³›åŒ–å¯¹æ¯”
python baseline_comparison.py \
    --source_dataset CWRU \
    --target_dataset XJTU \
    --domain_adaptation_methods \
    --include_upper_bound
```

### generate_paper_tables.py
**ä¸»è¦åŠŸèƒ½**: ç”Ÿæˆç¬¦åˆæœŸåˆŠæ ‡å‡†çš„ç»“æœè¡¨æ ¼

#### LaTeXè¡¨æ ¼ç”Ÿæˆ
```bash
# IEEEæ ¼å¼ä¸»ç»“æœè¡¨
python generate_paper_tables.py \
    --template ieee_main_results \
    --results_file ../04_analysis/benchmarks/comparison_results.json \
    --metrics accuracy,precision,recall,f1_score \
    --statistical_notation \
    --highlight_best

# æ¶ˆèç ”ç©¶è¡¨æ ¼
python generate_paper_tables.py \
    --template ablation_study \
    --ablation_results ../03_experiments/results/ablation/ \
    --parameters temperature,window_size,num_window \
    --show_improvements
```

### generate_paper_figures.py
**ä¸»è¦åŠŸèƒ½**: ç”Ÿæˆé«˜è´¨é‡çš„è®ºæ–‡å›¾è¡¨

#### å›¾è¡¨ç±»å‹é…ç½®
```python
# figure_configs.py
paper_figures = {
    'training_curves': {
        'type': 'line_plot',
        'metrics': ['loss', 'accuracy'],
        'style': 'ieee',
        'comparison': 'multi_method'
    },
    'confusion_matrix': {
        'type': 'heatmap',
        'normalization': 'true',
        'colormap': 'Blues',
        'annotations': True
    },
    'parameter_sensitivity': {
        'type': 'line_plot',
        'x_axis': 'parameter_value',
        'y_axis': 'performance_metric',
        'error_bars': '95_ci'
    },
    'domain_generalization': {
        'type': 'bar_plot',
        'grouping': 'source_target_pairs',
        'metrics': 'accuracy',
        'comparison': 'methods'
    }
}
```

## ğŸ“Š è®ºæ–‡ææ–™ç”Ÿæˆ

### ğŸ“‹ ä¸»è¦ç»“æœè¡¨æ ¼

#### è¡¨1: å•æ•°æ®é›†æ€§èƒ½å¯¹æ¯”
```python
# ç”Ÿæˆä¸»ç»“æœè¡¨æ ¼
python generate_paper_tables.py \
    --table_type main_results \
    --datasets CWRU,XJTU,PU,FEMTO \
    --methods baseline_all,contrastive_id \
    --format latex \
    --caption "ä¸åŒæ–¹æ³•åœ¨å·¥ä¸šæŒ¯åŠ¨æ•°æ®é›†ä¸Šçš„æ•…éšœè¯Šæ–­æ€§èƒ½å¯¹æ¯”"
```

**é¢„æœŸè¾“å‡º**:
```latex
\begin{table*}[ht]
\centering
\caption{ä¸åŒæ–¹æ³•åœ¨å·¥ä¸šæŒ¯åŠ¨æ•°æ®é›†ä¸Šçš„æ•…éšœè¯Šæ–­æ€§èƒ½å¯¹æ¯”}
\label{tab:main_results}
\begin{tabular}{lccccc}
\hline
\multirow{2}{*}{æ–¹æ³•} & \multicolumn{4}{c}{å‡†ç¡®ç‡ (\%)} & \multirow{2}{*}{å¹³å‡} \\
\cline{2-5}
 & CWRU & XJTU & PU & FEMTO & \\
\hline
Raw Signal & 65.2Â±2.1 & 59.8Â±3.2 & 63.4Â±2.8 & 58.9Â±3.5 & 61.8Â±2.9 \\
FFT Features & 73.1Â±1.8 & 68.7Â±2.4 & 72.3Â±2.1 & 69.5Â±2.9 & 70.9Â±2.3 \\
Statistical Features & 78.4Â±2.3 & 74.2Â±2.8 & 77.6Â±2.4 & 73.8Â±3.1 & 76.0Â±2.7 \\
CNN-1D & 82.3Â±1.9 & 78.9Â±2.2 & 80.6Â±2.0 & 77.4Â±2.8 & 79.8Â±2.2 \\
LSTM & 79.8Â±2.4 & 77.6Â±2.6 & 79.1Â±2.3 & 75.2Â±3.0 & 77.9Â±2.6 \\
Transformer & 84.7Â±1.7 & 81.3Â±2.1 & 83.5Â±1.9 & 80.1Â±2.5 & 82.4Â±2.1 \\
\hline
\textbf{ContrastiveID (Ours)} & \textbf{87.6Â±1.5} & \textbf{85.4Â±1.8} & \textbf{86.3Â±1.7} & \textbf{83.9Â±2.2} & \textbf{85.8Â±1.8} \\
\hline
\end{tabular}
\end{table*}
```

#### è¡¨2: è·¨æ•°æ®é›†åŸŸæ³›åŒ–æ€§èƒ½
```python
# ç”ŸæˆåŸŸæ³›åŒ–è¡¨æ ¼
python generate_paper_tables.py \
    --table_type domain_generalization \
    --source_target_pairs "CWRUâ†’XJTU,XJTUâ†’PU,PUâ†’FEMTO" \
    --adaptation_methods "Direct,FineTune,DomainAdapt,ContrastiveID" \
    --statistical_significance
```

### ğŸ“ˆ å…³é”®å›¾è¡¨ç”Ÿæˆ

#### å›¾1: è®­ç»ƒæ”¶æ•›æ›²çº¿
```python
# ç”Ÿæˆè®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾
python generate_paper_figures.py \
    --figure_type training_curves \
    --experiments baseline_cnn,baseline_transformer,contrastive_id \
    --metrics loss,accuracy \
    --smooth_curves \
    --confidence_intervals \
    --style ieee \
    --output training_convergence.pdf
```

#### å›¾2: å‚æ•°æ•æ„Ÿæ€§åˆ†æ
```python
# ç”Ÿæˆå‚æ•°æ•æ„Ÿæ€§å›¾
python generate_paper_figures.py \
    --figure_type parameter_sensitivity \
    --ablation_results ../03_experiments/results/ablation/ \
    --parameters temperature,window_size \
    --subplot_layout 1x2 \
    --output parameter_sensitivity.pdf
```

#### å›¾3: æ··æ·†çŸ©é˜µå¯è§†åŒ–
```python
# ç”Ÿæˆæ··æ·†çŸ©é˜µå›¾
python generate_paper_figures.py \
    --figure_type confusion_matrix \
    --predictions_file best_model_predictions.npz \
    --class_names "Normal,Inner,Outer,Ball" \
    --normalize \
    --output confusion_matrix.pdf
```

## ğŸ”¬ ç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯

### ä¸¥æ ¼çš„ç»Ÿè®¡æ£€éªŒ
```python
# statistical_validation.py
from scipy import stats
import numpy as np

def validate_statistical_significance(results_dict, alpha=0.05):
    """æ‰§è¡Œä¸¥æ ¼çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""

    methods = list(results_dict.keys())
    n_comparisons = len(methods) * (len(methods) - 1) // 2

    # Bonferroniæ ¡æ­£
    corrected_alpha = alpha / n_comparisons

    # æ‰§è¡Œæˆå¯¹tæ£€éªŒ
    pairwise_results = {}
    for i, method1 in enumerate(methods):
        for method2 in methods[i+1:]:
            t_stat, p_value = stats.ttest_ind(
                results_dict[method1],
                results_dict[method2]
            )

            # è®¡ç®—æ•ˆåº”é‡ (Cohen's d)
            pooled_std = np.sqrt(
                (np.var(results_dict[method1]) + np.var(results_dict[method2])) / 2
            )
            effect_size = (
                np.mean(results_dict[method1]) - np.mean(results_dict[method2])
            ) / pooled_std

            pairwise_results[f"{method1}_vs_{method2}"] = {
                'p_value': p_value,
                'corrected_p_value': p_value * n_comparisons,
                'significant': p_value < corrected_alpha,
                'effect_size': effect_size,
                'effect_magnitude': classify_effect_size(abs(effect_size))
            }

    return pairwise_results

def classify_effect_size(d):
    """åˆ†ç±»æ•ˆåº”é‡å¤§å°"""
    if d < 0.2:
        return "negligible"
    elif d < 0.5:
        return "small"
    elif d < 0.8:
        return "medium"
    else:
        return "large"
```

### ç½®ä¿¡åŒºé—´è®¡ç®—
```python
# confidence_intervals.py
def bootstrap_confidence_interval(data, n_bootstrap=10000, confidence=0.95):
    """Bootstrapç½®ä¿¡åŒºé—´è®¡ç®—"""

    bootstrap_means = []
    n = len(data)

    for _ in range(n_bootstrap):
        bootstrap_sample = np.random.choice(data, size=n, replace=True)
        bootstrap_means.append(np.mean(bootstrap_sample))

    alpha = 1 - confidence
    lower_percentile = (alpha / 2) * 100
    upper_percentile = (1 - alpha / 2) * 100

    ci_lower = np.percentile(bootstrap_means, lower_percentile)
    ci_upper = np.percentile(bootstrap_means, upper_percentile)

    return ci_lower, ci_upper
```

## ğŸ“„ è®ºæ–‡å†™ä½œæ”¯æŒ

### ç»“æœæè¿°ç”Ÿæˆ
```python
# result_description_generator.py
class ResultDescriptionGenerator:
    def __init__(self, results):
        self.results = results

    def generate_main_results_description(self):
        """ç”Ÿæˆä¸»è¦ç»“æœçš„æ–‡å­—æè¿°"""

        best_method = max(self.results.keys(), key=lambda k: np.mean(self.results[k]))
        best_performance = np.mean(self.results[best_method])
        best_std = np.std(self.results[best_method])

        # æ‰¾åˆ°æœ€ä½³åŸºçº¿æ–¹æ³•
        baseline_methods = [k for k in self.results.keys() if 'ContrastiveID' not in k]
        best_baseline = max(baseline_methods, key=lambda k: np.mean(self.results[k]))
        baseline_performance = np.mean(self.results[best_baseline])

        improvement = best_performance - baseline_performance
        relative_improvement = (improvement / baseline_performance) * 100

        description = f"""
        Our proposed ContrastiveID method achieves the best performance across all datasets,
        with an average accuracy of {best_performance:.1f}Â±{best_std:.1f}%. This represents
        a {improvement:.1f} percentage point improvement ({relative_improvement:.1f}% relative
        improvement) over the best baseline method ({best_baseline}: {baseline_performance:.1f}%).
        """

        return description.strip()

    def generate_statistical_significance_description(self, stats_results):
        """ç”Ÿæˆç»Ÿè®¡æ˜¾è‘—æ€§æè¿°"""

        significant_comparisons = [
            k for k, v in stats_results.items()
            if v['significant'] and 'ContrastiveID' in k
        ]

        description = f"""
        Statistical analysis using paired t-tests with Bonferroni correction (Î±=0.05)
        confirms that ContrastiveID significantly outperforms all baseline methods
        ({len(significant_comparisons)} out of {len(stats_results)} comparisons, all p<0.001).
        Effect sizes range from medium to large (Cohen's d > 0.5), indicating practical significance.
        """

        return description.strip()
```

### æ–¹æ³•æè¿°æ¨¡æ¿
```python
# method_description_templates.py
method_descriptions = {
    'contrastive_learning': """
    We employ a contrastive learning framework based on InfoNCE loss to learn discriminative
    representations from unlabeled vibration signals. The core idea is to maximize agreement
    between differently augmented views of the same signal while minimizing agreement between
    views from different signals.
    """,

    'window_sampling': """
    To generate positive pairs for contrastive learning, we extract multiple non-overlapping
    windows from each long vibration signal. This strategy exploits the temporal consistency
    of fault patterns within the same equipment instance while providing sufficient data
    augmentation for effective representation learning.
    """,

    'infonce_loss': """
    The InfoNCE loss function is formulated as:
    L = -âˆ‘áµ¢ log(exp(sim(záµ¢, záµ¢âº)/Ï„) / âˆ‘â±¼ exp(sim(záµ¢, zâ±¼)/Ï„))
    where záµ¢ and záµ¢âº are the anchor and positive representations, Ï„ is the temperature
    parameter, and sim(Â·,Â·) denotes cosine similarity.
    """
}
```

## ğŸ¯ å‘è¡¨å‡†å¤‡æ¸…å•

### ğŸ“Š å¿…éœ€ææ–™æ¸…å•
- [ ] **ä¸»è¦ç»“æœè¡¨**: æ‰€æœ‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½å¯¹æ¯”
- [ ] **æ¶ˆèç ”ç©¶è¡¨**: å…³é”®è¶…å‚æ•°å½±å“åˆ†æ
- [ ] **åŸŸæ³›åŒ–è¡¨**: è·¨æ•°æ®é›†è½¬ç§»æ€§èƒ½
- [ ] **è®­ç»ƒæ›²çº¿å›¾**: æ”¶æ•›æ€§å’Œç¨³å®šæ€§å±•ç¤º
- [ ] **æ··æ·†çŸ©é˜µ**: è¯¦ç»†çš„åˆ†ç±»æ€§èƒ½åˆ†æ
- [ ] **å‚æ•°æ•æ„Ÿæ€§å›¾**: è¶…å‚æ•°å½±å“å¯è§†åŒ–
- [ ] **ç»Ÿè®¡æ˜¾è‘—æ€§æŠ¥å‘Š**: ä¸¥æ ¼çš„ç»Ÿè®¡éªŒè¯

### ğŸ“ å†™ä½œæ£€æŸ¥æ¸…å•
- [ ] **æ–¹æ³•åˆ›æ–°ç‚¹**: æ˜ç¡®é˜è¿°æŠ€æœ¯è´¡çŒ®
- [ ] **å®éªŒè®¾è®¡**: å®Œæ•´çš„å¯¹æ¯”å®éªŒæ–¹æ¡ˆ
- [ ] **ç»“æœåˆ†æ**: æ·±å…¥çš„æ€§èƒ½åˆ†æå’Œè§£é‡Š
- [ ] **ç»Ÿè®¡éªŒè¯**: ä¸¥æ ¼çš„æ˜¾è‘—æ€§æ£€éªŒ
- [ ] **å¯é‡ç°æ€§**: è¯¦ç»†çš„å®ç°ç»†èŠ‚å’Œå‚æ•°è®¾ç½®

### ğŸ”— å¯é‡ç°æ€§ææ–™
```bash
# ç”Ÿæˆå¯é‡ç°æ€§åŒ…
python prepare_reproducibility_package.py \
    --results_dir ../04_analysis/benchmarks/ \
    --code_dir ../../ \
    --config_files ../examples/config_templates/ \
    --output_package reproducibility_package.zip

# ç”Ÿæˆç¯å¢ƒé…ç½®
python generate_environment_config.py \
    --export_requirements \
    --export_conda_env \
    --export_docker_config \
    --output_dir reproducibility_package/
```

## ğŸ† æœŸåˆŠæŠ•ç¨¿å»ºè®®

### ğŸ“š ç›®æ ‡æœŸåˆŠå‚è€ƒ

#### é¡¶çº§æœŸåˆŠ (å½±å“å› å­ > 6)
- **IEEE Transactions on Industrial Informatics** (TII)
  - é‡ç‚¹: å·¥ä¸šåº”ç”¨ä»·å€¼å’Œå®é™…éƒ¨ç½²å¯è¡Œæ€§
  - å®éªŒè¦æ±‚: å¤šä¸ªçœŸå®å·¥ä¸šæ•°æ®é›†éªŒè¯

- **IEEE Transactions on Neural Networks and Learning Systems** (TNNLS)
  - é‡ç‚¹: å­¦ä¹ ç®—æ³•åˆ›æ–°å’Œç†è®ºåˆ†æ
  - å®éªŒè¦æ±‚: è¯¦ç»†çš„æ¶ˆèç ”ç©¶å’Œç†è®ºè¯æ˜

- **Mechanical Systems and Signal Processing** (MSSP)
  - é‡ç‚¹: ä¿¡å·å¤„ç†æ–¹æ³•åˆ›æ–°
  - å®éªŒè¦æ±‚: ä¿¡å·å¤„ç†è§’åº¦çš„æ·±å…¥åˆ†æ

#### ä¼˜è´¨æœŸåˆŠ (å½±å“å› å­ 3-6)
- **IEEE Sensors Journal**
- **ISA Transactions**
- **Knowledge-Based Systems**

### ğŸ“ æŠ•ç¨¿å‡†å¤‡æ—¶é—´è§„åˆ’
```
ç¬¬1å‘¨: å®Œæˆæ‰€æœ‰å®éªŒå’Œåˆ†æ
ç¬¬2å‘¨: ç”Ÿæˆè¡¨æ ¼ã€å›¾è¡¨å’Œç»Ÿè®¡éªŒè¯
ç¬¬3å‘¨: æ’°å†™è®ºæ–‡åˆç¨¿
ç¬¬4å‘¨: è®ºæ–‡ä¿®æ”¹å’Œå®Œå–„
ç¬¬5å‘¨: æœ€ç»ˆæ£€æŸ¥å’ŒæŠ•ç¨¿
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### LaTeXé›†æˆ
```bash
# ç›´æ¥é›†æˆåˆ°LaTeXé¡¹ç›®
python latex_integration.py \
    --paper_template ieee_template.tex \
    --results_dir paper_results/ \
    --auto_insert_tables \
    --auto_insert_figures \
    --output_dir latex_paper/
```

### è‡ªåŠ¨å¼•ç”¨ç®¡ç†
```python
# citation_manager.py
def generate_method_citations():
    """ç”Ÿæˆæ–¹æ³•ç›¸å…³çš„å¼•ç”¨"""
    citations = {
        'infonce': '@inproceedings{oord2018representation, ...}',
        'transformer': '@article{vaswani2017attention, ...}',
        'domain_adaptation': '@inproceedings{ganin2015unsupervised, ...}',
        'vibration_analysis': '@article{lei2020applications, ...}'
    }
    return citations
```

## ğŸ¯ è¿›å…¥æœ€ç»ˆé˜¶æ®µ

### è®ºæ–‡è´¨é‡æ£€æŸ¥
- [ ] æŠ€æœ¯å†…å®¹å‡†ç¡®å®Œæ•´
- [ ] å®éªŒè®¾è®¡ç§‘å­¦ä¸¥è°¨
- [ ] ç»“æœåˆ†ææ·±å…¥é€å½»
- [ ] å†™ä½œè¡¨è¾¾æ¸…æ™°å‡†ç¡®
- [ ] å›¾è¡¨è§„èŒƒç¾è§‚

### æŠ•ç¨¿å‰æœ€ç»ˆæ£€æŸ¥
```bash
# è¿è¡Œæœ€ç»ˆæ£€æŸ¥è„šæœ¬
python final_paper_check.py \
    --paper_dir latex_paper/ \
    --results_validation \
    --reproducibility_check \
    --citation_verification \
    --format_compliance
```

## ğŸ“š æ‰©å±•èµ„æº

### è®ºæ–‡å†™ä½œæŒ‡å—
- **ç§‘æŠ€è®ºæ–‡å†™ä½œ**: ç»“æ„åŒ–å†™ä½œæ–¹æ³•
- **ç»Ÿè®¡æŠ¥å‘Š**: ç»Ÿè®¡ç»“æœçš„æ­£ç¡®æè¿°
- **å›¾è¡¨è®¾è®¡**: å­¦æœ¯å›¾è¡¨æœ€ä½³å®è·µ
- **æœŸåˆŠæŠ•ç¨¿**: æŠ•ç¨¿æµç¨‹å’ŒæŠ€å·§

### ç›¸å…³å·¥å…·
- **LaTeX**: Overleaf, TeXstudio
- **å›¾è¡¨**: Matplotlib, TikZ, Origin
- **å¼•ç”¨ç®¡ç†**: Mendeley, Zotero
- **å†™ä½œè¾…åŠ©**: Grammarly, æœ‰é“ç¿»è¯‘

---

**ğŸ‰ æ­å–œï¼æ‚¨å·²å…·å¤‡äº†é«˜è´¨é‡å­¦æœ¯è®ºæ–‡çš„æ‰€æœ‰æ”¯æ’‘ææ–™ã€‚**

å¥½çš„ç ”ç©¶éœ€è¦å¥½çš„è¡¨è¾¾ï¼Œç›¸ä¿¡æ‚¨çš„å·¥ä½œå°†ä¸ºå·¥ä¸šæŒ¯åŠ¨åˆ†æé¢†åŸŸå¸¦æ¥æœ‰ä»·å€¼çš„è´¡çŒ®ã€‚

æœ€åï¼Œè®©æˆ‘ä»¬è¿›è¡Œ[å®Œæ•´æ€§éªŒè¯](../tests/README.md)ç¡®ä¿ä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼