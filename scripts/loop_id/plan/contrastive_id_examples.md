# ContrastiveIDTask å®ç”¨æ¡ˆä¾‹é›†åˆ

> ğŸ¯ **å®é™…åº”ç”¨æ¡ˆä¾‹å’Œä»£ç ç¤ºä¾‹**  
> æ¶µç›–ä»åŸºç¡€ä½¿ç”¨åˆ°é«˜çº§å®šåˆ¶çš„å®Œæ•´ä»£ç ç¤ºä¾‹

## ğŸ“‹ ç›®å½•

- [ğŸš€ åŸºç¡€ä½¿ç”¨æ¡ˆä¾‹](#-åŸºç¡€ä½¿ç”¨æ¡ˆä¾‹)
- [âš™ï¸ é…ç½®å®šåˆ¶æ¡ˆä¾‹](#ï¸-é…ç½®å®šåˆ¶æ¡ˆä¾‹)
- [ğŸ§ª å®éªŒç®¡ç†æ¡ˆä¾‹](#-å®éªŒç®¡ç†æ¡ˆä¾‹)
- [ğŸ“Š ç»“æœåˆ†ææ¡ˆä¾‹](#-ç»“æœåˆ†ææ¡ˆä¾‹)
- [ğŸ”§ é›†æˆå¼€å‘æ¡ˆä¾‹](#-é›†æˆå¼€å‘æ¡ˆä¾‹)
- [ğŸ› é—®é¢˜è§£å†³æ¡ˆä¾‹](#-é—®é¢˜è§£å†³æ¡ˆä¾‹)

---

## ğŸš€ åŸºç¡€ä½¿ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼š5åˆ†é’Ÿå¿«é€ŸéªŒè¯

**åœºæ™¯**ï¼šæ–°æ¥è§¦ContrastiveIDTaskï¼Œå¸Œæœ›å¿«é€ŸéªŒè¯ç¯å¢ƒå’ŒåŠŸèƒ½ã€‚

```bash
# å®Œæ•´çš„5åˆ†é’ŸéªŒè¯æµç¨‹
cd /path/to/PHM-Vibench

# 1. ç¯å¢ƒæ£€æŸ¥ï¼ˆ30ç§’ï¼‰
echo "ğŸ” ç¯å¢ƒæ£€æŸ¥..."
python -c "
import torch
from src.configs import load_config
from src.task_factory.task.pretrain.ContrastiveIDTask import ContrastiveIDTask
print('âœ… æ‰€æœ‰ç»„ä»¶å°±ç»ª')
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
"

# 2. å¿«é€Ÿè®­ç»ƒï¼ˆ3åˆ†é’Ÿï¼‰
echo "ğŸš€ å¼€å§‹å¿«é€Ÿè®­ç»ƒ..."
python main.py \
    --pipeline Pipeline_ID \
    --config configs/id_contrastive/debug.yaml \
    --notes "5åˆ†é’Ÿå¿«é€ŸéªŒè¯" \
    --trainer.max_epochs 1

# 3. ç»“æœæ£€æŸ¥ï¼ˆ1åˆ†é’Ÿï¼‰
echo "ğŸ“Š æ£€æŸ¥ç»“æœ..."
LATEST_RUN=$(find save/ -name "ContrastiveIDTask" -type d | head -1)
if [ -n "$LATEST_RUN" ]; then
    echo "âœ… è®­ç»ƒå®Œæˆï¼ç»“æœç›®å½•: $LATEST_RUN"
    ls -la "$LATEST_RUN"
else
    echo "âŒ æœªæ‰¾åˆ°ç»“æœç›®å½•"
fi
```

**æœŸå¾…è¾“å‡º**ï¼š
```
âœ… æ‰€æœ‰ç»„ä»¶å°±ç»ª
PyTorchç‰ˆæœ¬: 2.1.0
CUDAå¯ç”¨: True
ğŸš€ å¼€å§‹å¿«é€Ÿè®­ç»ƒ...
[è®­ç»ƒæ—¥å¿—...]
âœ… è®­ç»ƒå®Œæˆï¼ç»“æœç›®å½•: save/metadata_6_1/ContrastiveIDTask/20241201_150342
checkpoints/  metrics.json  log.txt  config.yaml
```

### æ¡ˆä¾‹2ï¼šä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†

**åœºæ™¯**ï¼šæœ‰å·¥ä¸šæŒ¯åŠ¨æ•°æ®ï¼Œå¸Œæœ›ç”¨ContrastiveIDTaskè¿›è¡Œé¢„è®­ç»ƒã€‚

```python
# prepare_my_data.py - æ•°æ®å‡†å¤‡è„šæœ¬
import pandas as pd
import numpy as np
import h5py
from pathlib import Path

def prepare_custom_dataset():
    """å‡†å¤‡è‡ªå®šä¹‰æ•°æ®é›†ç”¨äºContrastiveIDTask"""
    
    # ç¤ºä¾‹ï¼šä»MATæ–‡ä»¶åˆ›å»ºæ•°æ®é›†
    data_dir = Path("my_vibration_data/")
    output_dir = Path("data/")
    output_dir.mkdir(exist_ok=True)
    
    # 1. åˆ›å»ºmetadata.xlsx
    metadata = []
    signal_data = {}
    
    # éå†æ•°æ®æ–‡ä»¶
    for i, mat_file in enumerate(data_dir.glob("*.mat")):
        # å‡è®¾æ¯ä¸ªMATæ–‡ä»¶åŒ…å«ä¸€ä¸ª'signal'å˜é‡
        import scipy.io
        mat_data = scipy.io.loadmat(mat_file)
        signal = mat_data['signal'].flatten()  # 1Dä¿¡å·
        
        # æ·»åŠ é€šé“ç»´åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if len(signal.shape) == 1:
            signal = np.stack([signal, np.zeros_like(signal)], axis=-1)  # 2é€šé“
        
        # ç”ŸæˆID
        signal_id = f"custom_{i:04d}"
        
        # ä¿å­˜ä¿¡å·æ•°æ®
        signal_data[signal_id] = signal
        
        # æ·»åŠ metadataè®°å½•
        metadata.append({
            'Id': signal_id,
            'label': i % 4,  # 4ä¸ªç±»åˆ«çš„ç¤ºä¾‹
            'dataset': 'CustomDataset',
            'signal_length': len(signal),
            'sampling_rate': 25600,  # æ ¹æ®å®é™…æƒ…å†µè®¾ç½®
            'equipment': 'Motor',
            'condition': f'Condition_{i % 4}'
        })
    
    # ä¿å­˜metadata
    df = pd.DataFrame(metadata)
    df.to_excel(output_dir / "metadata_custom.xlsx", index=False)
    print(f"âœ… Metadataå·²ä¿å­˜: {len(df)} ä¸ªæ ·æœ¬")
    
    # 2. åˆ›å»ºH5æ–‡ä»¶
    with h5py.File(output_dir / "custom_data.h5", 'w') as f:
        for signal_id, signal in signal_data.items():
            f.create_dataset(signal_id, data=signal, compression='gzip')
    
    print(f"âœ… H5æ•°æ®æ–‡ä»¶å·²ä¿å­˜: {len(signal_data)} ä¸ªä¿¡å·")
    
    # 3. åˆ›å»ºé…ç½®æ–‡ä»¶
    config_template = """
# configs/custom_contrastive.yaml
data:
  factory_name: "id"
  dataset_name: "ID_dataset"
  metadata_file: "metadata_custom.xlsx"
  data_dir: "data"
  window_size: 1024
  stride: 512
  num_windows: 2
  batch_size: 16

model:
  type: "ISFM"
  backbone: "B_08_PatchTST"
  d_model: 256

task:
  name: "contrastive_id"
  temperature: 0.07
  projection_dim: 128

trainer:
  epochs: 20
  devices: 1
  accelerator: "auto"
  precision: "16-mixed"

environment:
  WANDB_MODE: "disabled"
"""
    
    config_path = Path("configs/custom_contrastive.yaml")
    config_path.parent.mkdir(exist_ok=True)
    with open(config_path, 'w') as f:
        f.write(config_template.strip())
    
    print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
    print("ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒ:")
    print("python main.py --pipeline Pipeline_ID --config configs/custom_contrastive.yaml")

if __name__ == "__main__":
    prepare_custom_dataset()
```

**è¿è¡Œè‡ªå®šä¹‰æ•°æ®è®­ç»ƒ**ï¼š
```bash
# 1. å‡†å¤‡æ•°æ®
python prepare_my_data.py

# 2. å¼€å§‹è®­ç»ƒ
python main.py \
    --pipeline Pipeline_ID \
    --config configs/custom_contrastive.yaml \
    --notes "è‡ªå®šä¹‰æ•°æ®é›†å¯¹æ¯”å­¦ä¹ å®éªŒ"

# 3. ç›‘æ§è®­ç»ƒ
tensorboard --logdir save/ --port 6006
```

### æ¡ˆä¾‹3ï¼šå¤šGPUè®­ç»ƒè®¾ç½®

**åœºæ™¯**ï¼šæœ‰å¤šå¼ GPUï¼Œå¸Œæœ›åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚

```bash
# æ£€æŸ¥GPUé…ç½®
nvidia-smi --list-gpus

# 4GPUåˆ†å¸ƒå¼è®­ç»ƒ
python main.py \
    --pipeline Pipeline_ID \
    --config configs/id_contrastive/production.yaml \
    --trainer.devices 4 \
    --trainer.strategy ddp \
    --data.batch_size 64 \
    --data.num_workers 16 \
    --notes "4GPUåˆ†å¸ƒå¼è®­ç»ƒ"

# æŒ‡å®šç‰¹å®šGPU
CUDA_VISIBLE_DEVICES=0,1,2,3 python main.py \
    --pipeline Pipeline_ID \
    --config configs/id_contrastive/production.yaml \
    --trainer.devices 4 \
    --trainer.strategy ddp

# å¤šæœºè®­ç»ƒï¼ˆå‡è®¾æœ‰2å°æœºå™¨ï¼Œæ¯å°4GPUï¼‰
# æœºå™¨0 (ä¸»èŠ‚ç‚¹)
python main.py \
    --pipeline Pipeline_ID \
    --config configs/id_contrastive/production.yaml \
    --trainer.devices 4 \
    --trainer.num_nodes 2 \
    --trainer.node_rank 0 \
    --trainer.strategy ddp

# æœºå™¨1 (ä»èŠ‚ç‚¹) 
python main.py \
    --pipeline Pipeline_ID \
    --config configs/id_contrastive/production.yaml \
    --trainer.devices 4 \
    --trainer.num_nodes 2 \
    --trainer.node_rank 1 \
    --trainer.strategy ddp
```

---

## âš™ï¸ é…ç½®å®šåˆ¶æ¡ˆä¾‹

### æ¡ˆä¾‹4ï¼šé’ˆå¯¹å°å†…å­˜GPUçš„é…ç½®ä¼˜åŒ–

**åœºæ™¯**ï¼šåªæœ‰8GB GPUå†…å­˜ï¼Œéœ€è¦ä¼˜åŒ–é…ç½®ä»¥é¿å…å†…å­˜æº¢å‡ºã€‚

```yaml
# configs/low_memory_contrastive.yaml
data:
  factory_name: "id"
  dataset_name: "ID_dataset"
  metadata_file: "metadata_6_1.xlsx"
  window_size: 512              # å‡å°çª—å£å¤§å°
  stride: 256
  num_windows: 2               # æœ€å°çª—å£æ•°
  batch_size: 8                # å°æ‰¹é‡
  num_workers: 4               # å‡å°‘workeræ•°é‡

model:
  type: "ISFM"
  backbone: "B_04_Dlinear"     # ä½¿ç”¨è½»é‡çº§backbone
  d_model: 128                 # å‡å°æ¨¡å‹ç»´åº¦

task:
  name: "contrastive_id"
  temperature: 0.07
  projection_dim: 64           # å‡å°æŠ•å½±ç»´åº¦

trainer:
  epochs: 50
  devices: 1
  accelerator: "gpu"
  precision: "16-mixed"        # æ··åˆç²¾åº¦èŠ‚çœå†…å­˜
  gradient_checkpointing: true # ç”¨æ—¶é—´æ¢å†…å­˜
  accumulate_grad_batches: 8   # ç´¯ç§¯æ¢¯åº¦æ¨¡æ‹Ÿå¤§batch
  max_epochs: 50
  
  # å†…å­˜ä¼˜åŒ–è®¾ç½®
  enable_progress_bar: false   # å‡å°‘å†…å­˜ä½¿ç”¨
  log_every_n_steps: 50

logging:
  save_top_k: 2                # åªä¿å­˜2ä¸ªæœ€ä½³checkpoint
  save_last: false             # ä¸ä¿å­˜æœ€åä¸€ä¸ªcheckpoint

environment:
  PYTHONHASHSEED: "0"
  WANDB_MODE: "disabled"
```

**ä½¿ç”¨è„šæœ¬**ï¼š
```python
# low_memory_training.py - å†…å­˜ç›‘æ§è®­ç»ƒè„šæœ¬
import subprocess
import psutil
import torch
import time

def monitor_memory_training():
    """å†…å­˜ç›‘æ§çš„è®­ç»ƒè„šæœ¬"""
    
    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"ğŸ”§ GPUå†…å­˜æ¸…ç†å®Œæˆ")
    
    # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
    cmd = [
        'python', 'main.py',
        '--pipeline', 'Pipeline_ID',
        '--config', 'configs/low_memory_contrastive.yaml',
        '--notes', 'å†…å­˜ä¼˜åŒ–è®­ç»ƒ'
    ]
    
    print("ğŸš€ å¯åŠ¨å†…å­˜ä¼˜åŒ–è®­ç»ƒ...")
    process = subprocess.Popen(cmd)
    
    # ç›‘æ§å†…å­˜ä½¿ç”¨
    max_memory = 0
    try:
        while process.poll() is None:
            # ç›‘æ§ç³»ç»Ÿå†…å­˜
            ram_percent = psutil.virtual_memory().percent
            
            # ç›‘æ§GPUå†…å­˜
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / 1e9
                gpu_reserved = torch.cuda.memory_reserved() / 1e9
                max_memory = max(max_memory, gpu_reserved)
                
                print(f"ğŸ“Š RAM: {ram_percent:.1f}% | "
                      f"GPU: {gpu_memory:.1f}GB/{gpu_reserved:.1f}GB | "
                      f"å³°å€¼: {max_memory:.1f}GB")
            
            time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
            
    except KeyboardInterrupt:
        print("â¹ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        process.terminate()
        
    finally:
        print(f"ğŸ“ˆ æœ€å¤§GPUå†…å­˜ä½¿ç”¨: {max_memory:.1f}GB")

if __name__ == "__main__":
    monitor_memory_training()
```

### æ¡ˆä¾‹5ï¼šè½»é‡çº§å¿«é€Ÿå®éªŒé…ç½®

**åœºæ™¯**ï¼šéœ€è¦å¿«é€Ÿè¿­ä»£ä¸åŒçš„è¶…å‚æ•°ç»„åˆã€‚

```python
# quick_experiments.py - å¿«é€Ÿå®éªŒè„šæœ¬
from src.configs import load_config
import subprocess
import json
from datetime import datetime

def run_quick_experiments():
    """è¿è¡Œå¤šä¸ªå¿«é€Ÿå®éªŒå¯¹æ¯”è¶…å‚æ•°"""
    
    # å®éªŒå‚æ•°ç»„åˆ
    experiments = [
        {
            'name': 'temp_005',
            'overrides': {'task.temperature': 0.05, 'trainer.epochs': 5},
            'description': 'ä½æ¸©åº¦å¿«é€Ÿå®éªŒ'
        },
        {
            'name': 'temp_01',
            'overrides': {'task.temperature': 0.1, 'trainer.epochs': 5},
            'description': 'ä¸­ç­‰æ¸©åº¦å¿«é€Ÿå®éªŒ'
        },
        {
            'name': 'temp_02',
            'overrides': {'task.temperature': 0.2, 'trainer.epochs': 5},
            'description': 'é«˜æ¸©åº¦å¿«é€Ÿå®éªŒ'
        },
        {
            'name': 'window_512',
            'overrides': {'data.window_size': 512, 'trainer.epochs': 5},
            'description': 'å°çª—å£å¿«é€Ÿå®éªŒ'
        },
        {
            'name': 'window_2048',
            'overrides': {'data.window_size': 2048, 'trainer.epochs': 5},
            'description': 'å¤§çª—å£å¿«é€Ÿå®éªŒ'
        }
    ]
    
    results = {}
    
    for exp in experiments:
        print(f"\nğŸ§ª å¼€å§‹å®éªŒ: {exp['name']} - {exp['description']}")
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            'python', 'main.py',
            '--pipeline', 'Pipeline_ID',
            '--config', 'contrastive',
            '--notes', f"å¿«é€Ÿå®éªŒ_{exp['name']}"
        ]
        
        # æ·»åŠ å‚æ•°è¦†ç›–
        for key, value in exp['overrides'].items():
            cmd.extend([f'--{key}', str(value)])
        
        # è¿è¡Œå®éªŒ
        start_time = datetime.now()
        result = subprocess.run(cmd, capture_output=True, text=True)
        duration = (datetime.now() - start_time).total_seconds()
        
        # è®°å½•ç»“æœ
        results[exp['name']] = {
            'duration': duration,
            'success': result.returncode == 0,
            'description': exp['description'],
            'overrides': exp['overrides']
        }
        
        if result.returncode == 0:
            print(f"âœ… {exp['name']} å®Œæˆ ({duration:.1f}s)")
        else:
            print(f"âŒ {exp['name']} å¤±è´¥: {result.stderr}")
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print(f"\nğŸ“Š å®éªŒæ€»ç»“:")
    print(f"{'å®éªŒåç§°':<15} {'çŠ¶æ€':<8} {'æ—¶é•¿(s)':<10} {'æè¿°'}")
    print("-" * 60)
    
    for name, result in results.items():
        status = "âœ…æˆåŠŸ" if result['success'] else "âŒå¤±è´¥"
        print(f"{name:<15} {status:<8} {result['duration']:<10.1f} {result['description']}")
    
    # ä¿å­˜ç»“æœ
    with open(f"quick_experiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    return results

if __name__ == "__main__":
    run_quick_experiments()
```

### æ¡ˆä¾‹6ï¼šç”Ÿäº§ç¯å¢ƒé…ç½®æ¨¡æ¿

**åœºæ™¯**ï¼šä¸ºç”Ÿäº§ç¯å¢ƒåˆ›å»ºæ ‡å‡†åŒ–çš„é…ç½®æ¨¡æ¿ã€‚

```yaml
# configs/production_template.yaml
# ContrastiveIDTask ç”Ÿäº§ç¯å¢ƒæ ‡å‡†é…ç½®æ¨¡æ¿
# ç‰ˆæœ¬: v1.0
# æ›´æ–°æ—¶é—´: 2024-12-01

# æ•°æ®é…ç½® - ç”Ÿäº§çº§è®¾ç½®
data:
  factory_name: "id"
  dataset_name: "ID_dataset"
  metadata_file: "metadata_production.xlsx"   # æ›¿æ¢ä¸ºå®é™…metadata
  data_dir: "data"
  
  # ä¿¡å·å¤„ç†å‚æ•°
  window_size: 2048                           # æ¨èå€¼ï¼Œå¹³è¡¡æ€§èƒ½å’Œç²¾åº¦
  stride: 1024                               # window_sizeçš„ä¸€åŠ
  num_windows: 3                             # ç”Ÿäº§ç¯å¢ƒæ¨èå€¼
  
  # æ•°æ®åŠ è½½ä¼˜åŒ–
  batch_size: 32                             # æ ¹æ®GPUå†…å­˜è°ƒæ•´
  num_workers: 8                             # CPUæ ¸å¿ƒæ•°
  pin_memory: true
  persistent_workers: true
  drop_last: true                            # ä¿æŒbatchå¤§å°ä¸€è‡´

# æ¨¡å‹é…ç½® - ISFM + PatchTST ç»„åˆ
model:
  type: "ISFM"
  backbone: "B_08_PatchTST"
  d_model: 512                               # å¤§æ¨¡å‹ï¼Œæ›´å¥½æ€§èƒ½
  
  # PatchTSTç‰¹å®šå‚æ•°
  patch_len: 16
  stride: 8
  n_layers: 8
  n_heads: 16
  d_ff: 2048
  dropout: 0.1
  
  # ä»»åŠ¡å¤´é…ç½®ï¼ˆå¯¹æ¯”å­¦ä¹ ä¸­ä¸ä½¿ç”¨ï¼Œä½†ä¿ç•™å…¼å®¹æ€§ï¼‰
  task_head: "H_01_Linear_cla"
  num_classes: 10

# ä»»åŠ¡é…ç½® - å¯¹æ¯”å­¦ä¹ æ ¸å¿ƒå‚æ•°
task:
  name: "contrastive_id"
  
  # å¯¹æ¯”å­¦ä¹ å‚æ•°
  temperature: 0.07                          # ç»è¿‡è°ƒä¼˜çš„æœ€ä½³å€¼
  projection_dim: 256                        # æŠ•å½±å±‚ç»´åº¦
  
  # æŸå¤±å‡½æ•°é…ç½®
  loss_weight: 1.0
  
  # ä¼˜åŒ–å™¨é…ç½®
  optimizer: "AdamW"
  lr: 1e-4                                   # ç¨³å®šçš„å­¦ä¹ ç‡
  weight_decay: 1e-5
  
  # å­¦ä¹ ç‡è°ƒåº¦
  scheduler: "cosine"
  warmup_epochs: 5
  min_lr: 1e-6

# è®­ç»ƒé…ç½® - ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–
trainer:
  # åŸºç¡€è®¾ç½®
  epochs: 100                                # å……åˆ†è®­ç»ƒ
  devices: 4                                 # å¤šGPUåŠ é€Ÿ
  accelerator: "gpu"
  strategy: "ddp"                           # åˆ†å¸ƒå¼è®­ç»ƒ
  
  # æ€§èƒ½ä¼˜åŒ–
  precision: "16-mixed"                      # æ··åˆç²¾åº¦ï¼ŒèŠ‚çœå†…å­˜
  sync_batchnorm: true                      # åˆ†å¸ƒå¼BatchNormåŒæ­¥
  find_unused_parameters: false             # æ€§èƒ½ä¼˜åŒ–
  
  # æ¢¯åº¦ä¼˜åŒ–
  gradient_clip_val: 1.0                    # æ¢¯åº¦è£å‰ª
  accumulate_grad_batches: 1                # ä¸ç´¯ç§¯æ¢¯åº¦
  
  # éªŒè¯å’Œä¿å­˜
  val_check_interval: 0.25                  # æ¯1/4ä¸ªepochéªŒè¯ä¸€æ¬¡
  check_val_every_n_epoch: 1
  
  # æ—©åœå’Œcheckpoint
  patience: 15                              # æ—©åœè€å¿ƒå€¼
  min_delta: 0.001                          # æœ€å°æ”¹è¿›é˜ˆå€¼

# æ—¥å¿—å’Œç›‘æ§é…ç½®
logging:
  # Checkpointç®¡ç†
  save_top_k: 5                             # ä¿å­˜æœ€ä½³5ä¸ªæ¨¡å‹
  save_last: true                           # ä¿å­˜æœ€åcheckpoint
  monitor: "train_loss"                     # ç›‘æ§è®­ç»ƒæŸå¤±
  mode: "min"                               # æœ€å°åŒ–æŸå¤±
  
  # æ—¥å¿—è®¾ç½®
  log_every_n_steps: 100                    # æ¯100æ­¥è®°å½•ä¸€æ¬¡
  enable_progress_bar: true
  enable_model_summary: true
  
  # WandBé…ç½®ï¼ˆå¯é€‰ï¼‰
  project_name: "ContrastiveID_Production"
  experiment_name: null                     # è‡ªåŠ¨ç”Ÿæˆ
  tags: ["contrastive", "production", "ISFM"]

# ç³»ç»Ÿç¯å¢ƒé…ç½®
environment:
  # éšæœºç§å­
  PYTHONHASHSEED: "42"
  PL_SEED_EVERYTHING: "42"
  
  # CUDAä¼˜åŒ–
  CUDA_LAUNCH_BLOCKING: "0"
  TORCH_CUDNN_V8_API_ENABLED: "1"
  
  # WandBé…ç½®
  WANDB_MODE: "online"                      # æˆ– "disabled"
  WANDB_PROJECT: "ContrastiveID_Production"
  
  # å…¶ä»–ç¯å¢ƒå˜é‡
  OMP_NUM_THREADS: "8"
  MKL_NUM_THREADS: "8"

# æ•°æ®éªŒè¯é…ç½®
validation:
  # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
  check_data_integrity: true
  min_signal_length: 1024                   # æœ€å°ä¿¡å·é•¿åº¦
  max_signal_length: 100000                 # æœ€å¤§ä¿¡å·é•¿åº¦
  
  # é¢„å¤„ç†éªŒè¯
  check_nan_inf: true                       # æ£€æŸ¥NaN/Infå€¼
  normalize_check: true                     # æ£€æŸ¥å½’ä¸€åŒ–
  
# æ€§èƒ½ç›‘æ§é…ç½®  
monitoring:
  # å†…å­˜ç›‘æ§
  track_gpu_memory: true
  memory_threshold: 0.9                     # GPUå†…å­˜ä½¿ç”¨é˜ˆå€¼
  
  # æ€§èƒ½æŒ‡æ ‡
  track_throughput: true                    # è·Ÿè¸ªååé‡
  track_convergence: true                   # è·Ÿè¸ªæ”¶æ•›é€Ÿåº¦
  
  # æŠ¥å‘Šç”Ÿæˆ
  generate_report: true                     # è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š
  report_format: ["html", "json"]           # æŠ¥å‘Šæ ¼å¼

# éƒ¨ç½²é…ç½®
deployment:
  # æ¨¡å‹å¯¼å‡º
  export_format: ["onnx", "torchscript"]    # å¯¼å‡ºæ ¼å¼
  export_precision: "fp16"                  # å¯¼å‡ºç²¾åº¦
  
  # æ¨ç†ä¼˜åŒ–
  optimize_for_inference: true
  batch_size_inference: 64                  # æ¨ç†æ‰¹å¤§å°
```

**ä½¿ç”¨ç”Ÿäº§é…ç½®çš„è„šæœ¬**ï¼š
```python
# production_training.py - ç”Ÿäº§ç¯å¢ƒè®­ç»ƒè„šæœ¬
import argparse
import logging
import os
from datetime import datetime
from pathlib import Path

def setup_production_environment():
    """è®¾ç½®ç”Ÿäº§ç¯å¢ƒ"""
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = Path("logs/production")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / f"production_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
            logging.StreamHandler()
        ]
    )
    
    # ç¯å¢ƒå˜é‡è®¾ç½®
    os.environ['PYTHONHASHSEED'] = '42'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    
    return logging.getLogger(__name__)

def main():
    logger = setup_production_environment()
    
    parser = argparse.ArgumentParser(description='ContrastiveIDç”Ÿäº§è®­ç»ƒ')
    parser.add_argument('--config', default='configs/production_template.yaml', help='é…ç½®æ–‡ä»¶')
    parser.add_argument('--experiment_name', help='å®éªŒåç§°')
    parser.add_argument('--dry_run', action='store_true', help='å¹²è¿è¡Œæ¨¡å¼')
    
    args = parser.parse_args()
    
    logger.info(f"ğŸš€ å¼€å§‹ContrastiveIDç”Ÿäº§è®­ç»ƒ")
    logger.info(f"ğŸ“ é…ç½®æ–‡ä»¶: {args.config}")
    
    if args.dry_run:
        logger.info("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - ä»…éªŒè¯é…ç½®")
        # éªŒè¯é…ç½®é€»è¾‘
        from src.configs import load_config
        try:
            config = load_config(args.config)
            logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
            return
        except Exception as e:
            logger.error(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")
            return
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    cmd = [
        'python', 'main.py',
        '--pipeline', 'Pipeline_ID',
        '--config', args.config
    ]
    
    if args.experiment_name:
        cmd.extend(['--notes', args.experiment_name])
    
    # æ‰§è¡Œè®­ç»ƒ
    import subprocess
    logger.info(f"ğŸ”§ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, check=True)
        logger.info("âœ… è®­ç»ƒæˆåŠŸå®Œæˆ")
    except subprocess.CalledProcessError as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()
```

---

## ğŸ§ª å®éªŒç®¡ç†æ¡ˆä¾‹

### æ¡ˆä¾‹7ï¼šæ‰¹é‡è¶…å‚æ•°æœç´¢

**åœºæ™¯**ï¼šéœ€è¦ç³»ç»Ÿæ€§åœ°æœç´¢æœ€ä¼˜è¶…å‚æ•°ç»„åˆã€‚

```python
# hyperparameter_search.py - è¶…å‚æ•°æœç´¢è„šæœ¬
import itertools
import json
import subprocess
from datetime import datetime
from pathlib import Path
import pandas as pd

class HyperparameterSearch:
    """ContrastiveIDTaskè¶…å‚æ•°æœç´¢å™¨"""
    
    def __init__(self, base_config="contrastive", output_dir="hyperparameter_search"):
        self.base_config = base_config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # å®šä¹‰æœç´¢ç©ºé—´
        self.search_space = {
            'temperature': [0.01, 0.05, 0.07, 0.1, 0.2, 0.5],
            'projection_dim': [64, 128, 256, 512],
            'window_size': [512, 1024, 2048],
            'lr': [1e-5, 5e-5, 1e-4, 5e-4, 1e-3],
            'batch_size': [16, 32, 64]
        }
        
        self.results = []
    
    def generate_combinations(self, max_combinations=50):
        """ç”Ÿæˆå‚æ•°ç»„åˆ"""
        
        # è·å–æ‰€æœ‰å‚æ•°ç»„åˆ
        keys = list(self.search_space.keys())
        values = list(self.search_space.values())
        all_combinations = list(itertools.product(*values))
        
        # å¦‚æœç»„åˆå¤ªå¤šï¼Œéšæœºé‡‡æ ·
        if len(all_combinations) > max_combinations:
            import random
            random.seed(42)
            combinations = random.sample(all_combinations, max_combinations)
        else:
            combinations = all_combinations
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        param_combinations = []
        for combo in combinations:
            param_dict = dict(zip(keys, combo))
            param_combinations.append(param_dict)
        
        return param_combinations
    
    def run_experiment(self, params, experiment_id):
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        
        print(f"ğŸ§ª å®éªŒ {experiment_id}: {params}")
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            'python', 'main.py',
            '--pipeline', 'Pipeline_ID', 
            '--config', self.base_config,
            '--trainer.epochs', '10',  # çŸ­epochç”¨äºæœç´¢
            '--notes', f'hypersearch_{experiment_id}'
        ]
        
        # æ·»åŠ å‚æ•°
        for key, value in params.items():
            if key == 'temperature':
                cmd.extend(['--task.temperature', str(value)])
            elif key == 'projection_dim':
                cmd.extend(['--task.projection_dim', str(value)])
            elif key == 'window_size':
                cmd.extend(['--data.window_size', str(value)])
            elif key == 'lr':
                cmd.extend(['--task.lr', str(value)])
            elif key == 'batch_size':
                cmd.extend(['--data.batch_size', str(value)])
        
        # è¿è¡Œå®éªŒ
        start_time = datetime.now()
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)  # 30åˆ†é’Ÿè¶…æ—¶
            success = result.returncode == 0
            error_msg = result.stderr if not success else None
        except subprocess.TimeoutExpired:
            success = False
            error_msg = "Timeout after 30 minutes"
        
        duration = (datetime.now() - start_time).total_seconds()
        
        # æå–æ€§èƒ½æŒ‡æ ‡ï¼ˆä»è¾“å‡ºæˆ–æ—¥å¿—æ–‡ä»¶ï¼‰
        final_loss = self.extract_final_loss(result.stdout) if success else None
        convergence_epoch = self.extract_convergence_epoch(result.stdout) if success else None
        
        # è®°å½•ç»“æœ
        experiment_result = {
            'experiment_id': experiment_id,
            'params': params,
            'success': success,
            'duration': duration,
            'final_loss': final_loss,
            'convergence_epoch': convergence_epoch,
            'error_msg': error_msg,
            'timestamp': datetime.now().isoformat()
        }
        
        self.results.append(experiment_result)
        
        # å®æ—¶ä¿å­˜ç»“æœ
        self.save_results()
        
        return experiment_result
    
    def extract_final_loss(self, output):
        """ä»è¾“å‡ºä¸­æå–æœ€ç»ˆæŸå¤±"""
        import re
        # æŸ¥æ‰¾æœ€åçš„æŸå¤±å€¼
        matches = re.findall(r'train_loss=([0-9.]+)', output)
        return float(matches[-1]) if matches else None
    
    def extract_convergence_epoch(self, output):
        """æå–æ”¶æ•›epoch"""
        # ç®€å•å®ç°ï¼šå‡è®¾æŸå¤±ä¸‹é™åˆ°ä¸€å®šé˜ˆå€¼å³ä¸ºæ”¶æ•›
        import re
        epoch_losses = []
        for line in output.split('\n'):
            if 'Epoch' in line and 'train_loss=' in line:
                epoch_match = re.search(r'Epoch (\d+)', line)
                loss_match = re.search(r'train_loss=([0-9.]+)', line)
                if epoch_match and loss_match:
                    epoch = int(epoch_match.group(1))
                    loss = float(loss_match.group(1))
                    epoch_losses.append((epoch, loss))
        
        # æ‰¾åˆ°é¦–æ¬¡è¾¾åˆ°æ”¶æ•›é˜ˆå€¼çš„epoch
        convergence_threshold = 2.0
        for epoch, loss in epoch_losses:
            if loss < convergence_threshold:
                return epoch
        
        return None
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        
        # JSONæ ¼å¼
        with open(self.output_dir / 'results.json', 'w') as f:
            json.dump(self.results, f, indent=2)
        
        # CSVæ ¼å¼ï¼ˆä¾¿äºåˆ†æï¼‰
        if self.results:
            # å±•å¼€å‚æ•°åˆ—
            flat_results = []
            for result in self.results:
                flat_result = {
                    'experiment_id': result['experiment_id'],
                    'success': result['success'],
                    'duration': result['duration'],
                    'final_loss': result['final_loss'],
                    'convergence_epoch': result['convergence_epoch'],
                    'timestamp': result['timestamp']
                }
                # æ·»åŠ å‚æ•°åˆ—
                flat_result.update(result['params'])
                flat_results.append(flat_result)
            
            df = pd.DataFrame(flat_results)
            df.to_csv(self.output_dir / 'results.csv', index=False)
    
    def run_search(self, max_combinations=20):
        """è¿è¡Œè¶…å‚æ•°æœç´¢"""
        
        combinations = self.generate_combinations(max_combinations)
        print(f"ğŸ¯ å¼€å§‹è¶…å‚æ•°æœç´¢ï¼Œå…± {len(combinations)} ä¸ªç»„åˆ")
        
        for i, params in enumerate(combinations, 1):
            try:
                result = self.run_experiment(params, i)
                status = "âœ…" if result['success'] else "âŒ"
                print(f"{status} å®éªŒ {i}/{len(combinations)} å®Œæˆ "
                      f"(è€—æ—¶: {result['duration']:.1f}s)")
                
                if result['success'] and result['final_loss']:
                    print(f"   æœ€ç»ˆæŸå¤±: {result['final_loss']:.4f}")
                
            except Exception as e:
                print(f"âŒ å®éªŒ {i} å¼‚å¸¸: {e}")
        
        # åˆ†æç»“æœ
        self.analyze_results()
    
    def analyze_results(self):
        """åˆ†ææœç´¢ç»“æœ"""
        
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœå¯åˆ†æ")
            return
        
        successful_results = [r for r in self.results if r['success'] and r['final_loss']]
        
        if not successful_results:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„å®éªŒ")
            return
        
        # æ‰¾åˆ°æœ€ä½³ç»“æœ
        best_result = min(successful_results, key=lambda x: x['final_loss'])
        
        print(f"\nğŸ† æœ€ä½³ç»“æœ (å®éªŒID: {best_result['experiment_id']}):")
        print(f"   æœ€ç»ˆæŸå¤±: {best_result['final_loss']:.4f}")
        print(f"   å‚æ•°ç»„åˆ: {best_result['params']}")
        print(f"   æ”¶æ•›epoch: {best_result['convergence_epoch']}")
        
        # å‚æ•°é‡è¦æ€§åˆ†æ
        print(f"\nğŸ“Š å‚æ•°åˆ†æ:")
        for param in self.search_space.keys():
            param_values = {}
            for result in successful_results:
                value = result['params'][param]
                if value not in param_values:
                    param_values[value] = []
                param_values[value].append(result['final_loss'])
            
            # è®¡ç®—æ¯ä¸ªå‚æ•°å€¼çš„å¹³å‡æŸå¤±
            avg_losses = {v: sum(losses)/len(losses) for v, losses in param_values.items()}
            best_value = min(avg_losses.keys(), key=lambda x: avg_losses[x])
            
            print(f"   {param}: æœ€ä½³å€¼ = {best_value} (å¹³å‡æŸå¤±: {avg_losses[best_value]:.4f})")
        
        # ä¿å­˜æœ€ä½³é…ç½®
        best_config = {
            'data': {},
            'task': {},
            'trainer': {}
        }
        
        for key, value in best_result['params'].items():
            if key == 'temperature':
                best_config['task']['temperature'] = value
            elif key == 'projection_dim':
                best_config['task']['projection_dim'] = value
            elif key == 'window_size':
                best_config['data']['window_size'] = value
            elif key == 'lr':
                best_config['task']['lr'] = value
            elif key == 'batch_size':
                best_config['data']['batch_size'] = value
        
        with open(self.output_dir / 'best_config.json', 'w') as f:
            json.dump(best_config, f, indent=2)
        
        print(f"âœ… æœ€ä½³é…ç½®å·²ä¿å­˜åˆ°: {self.output_dir / 'best_config.json'}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    searcher = HyperparameterSearch()
    searcher.run_search(max_combinations=20)
```

### æ¡ˆä¾‹8ï¼šå¤šæ•°æ®é›†å¯¹æ¯”å®éªŒ

**åœºæ™¯**ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯ContrastiveIDTaskçš„æ•ˆæœã€‚

```python
# multi_dataset_comparison.py - å¤šæ•°æ®é›†å¯¹æ¯”å®éªŒ
import subprocess
import pandas as pd
from pathlib import Path
import json
from datetime import datetime

class MultiDatasetComparison:
    """å¤šæ•°æ®é›†å¯¹æ¯”å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, output_dir="multi_dataset_comparison"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # æ•°æ®é›†é…ç½®
        self.datasets = {
            'CWRU': {
                'metadata': 'metadata_cwru.xlsx',
                'description': 'Case Western Reserve Universityæ•°æ®é›†',
                'classes': 4,
                'signal_length': 'variable'
            },
            'XJTU': {
                'metadata': 'metadata_xjtu.xlsx', 
                'description': 'è¥¿äº¤å¤§è½´æ‰¿æ•°æ®é›†',
                'classes': 5,
                'signal_length': 'long'
            },
            'PU': {
                'metadata': 'metadata_pu.xlsx',
                'description': 'Paderborn Universityæ•°æ®é›†',
                'classes': 12,
                'signal_length': 'very_long'
            }
        }
        
        self.results = {}
    
    def run_dataset_experiment(self, dataset_name, dataset_config):
        """åœ¨å•ä¸ªæ•°æ®é›†ä¸Šè¿è¡Œå®éªŒ"""
        
        print(f"ğŸ—ƒï¸  å¼€å§‹æ•°æ®é›†å®éªŒ: {dataset_name}")
        print(f"   æè¿°: {dataset_config['description']}")
        
        # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºä¸“é—¨çš„é…ç½®
        config_overrides = {
            'data.metadata_file': dataset_config['metadata'],
            'trainer.epochs': 20,  # æ ‡å‡†åŒ–è®­ç»ƒepoch
            'notes': f'MultiDataset_{dataset_name}'
        }
        
        # æ ¹æ®æ•°æ®é›†ç‰¹æ€§è°ƒæ•´å‚æ•°
        if dataset_config['signal_length'] == 'very_long':
            config_overrides['data.window_size'] = 4096
            config_overrides['data.stride'] = 2048
        elif dataset_config['signal_length'] == 'long':
            config_overrides['data.window_size'] = 2048
            config_overrides['data.stride'] = 1024
        else:
            config_overrides['data.window_size'] = 1024
            config_overrides['data.stride'] = 512
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            'python', 'main.py',
            '--pipeline', 'Pipeline_ID',
            '--config', 'contrastive'
        ]
        
        for key, value in config_overrides.items():
            if key != 'notes':
                cmd.extend([f'--{key}', str(value)])
            else:
                cmd.extend(['--notes', str(value)])
        
        # è¿è¡Œå®éªŒ
        start_time = datetime.now()
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)  # 1å°æ—¶è¶…æ—¶
            success = result.returncode == 0
            duration = (datetime.now() - start_time).total_seconds()
            
            if success:
                # æå–æ€§èƒ½æŒ‡æ ‡
                final_loss = self.extract_metric(result.stdout, 'train_loss')
                contrastive_acc = self.extract_metric(result.stdout, 'contrastive_acc')
                convergence_epoch = self.extract_convergence_epoch(result.stdout)
                
                result_data = {
                    'dataset': dataset_name,
                    'success': True,
                    'duration': duration,
                    'final_loss': final_loss,
                    'contrastive_acc': contrastive_acc,
                    'convergence_epoch': convergence_epoch,
                    'config': config_overrides,
                    'dataset_info': dataset_config
                }
                
                print(f"âœ… {dataset_name} å®éªŒå®Œæˆ")
                print(f"   æœ€ç»ˆæŸå¤±: {final_loss:.4f}")
                print(f"   å¯¹æ¯”å‡†ç¡®ç‡: {contrastive_acc:.4f}")
                print(f"   æ”¶æ•›epoch: {convergence_epoch}")
                
            else:
                result_data = {
                    'dataset': dataset_name,
                    'success': False,
                    'duration': duration,
                    'error': result.stderr,
                    'config': config_overrides,
                    'dataset_info': dataset_config
                }
                print(f"âŒ {dataset_name} å®éªŒå¤±è´¥: {result.stderr}")
        
        except subprocess.TimeoutExpired:
            result_data = {
                'dataset': dataset_name,
                'success': False,
                'duration': 3600,
                'error': 'Timeout after 1 hour',
                'config': config_overrides,
                'dataset_info': dataset_config
            }
            print(f"â° {dataset_name} å®éªŒè¶…æ—¶")
        
        self.results[dataset_name] = result_data
        return result_data
    
    def extract_metric(self, output, metric_name):
        """æå–æŒ‡æ ‡å€¼"""
        import re
        pattern = f'{metric_name}=([0-9.]+)'
        matches = re.findall(pattern, output)
        return float(matches[-1]) if matches else None
    
    def extract_convergence_epoch(self, output):
        """æå–æ”¶æ•›epoch"""
        import re
        losses = []
        for line in output.split('\n'):
            if 'Epoch' in line and 'train_loss=' in line:
                epoch_match = re.search(r'Epoch (\d+)', line)
                loss_match = re.search(r'train_loss=([0-9.]+)', line)
                if epoch_match and loss_match:
                    epoch = int(epoch_match.group(1))
                    loss = float(loss_match.group(1))
                    losses.append((epoch, loss))
        
        # ç®€å•çš„æ”¶æ•›æ£€æµ‹ï¼šè¿ç»­3ä¸ªepochæŸå¤±å˜åŒ–<0.01
        if len(losses) >= 6:
            for i in range(3, len(losses)):
                recent_losses = [loss for _, loss in losses[i-3:i]]
                if max(recent_losses) - min(recent_losses) < 0.01:
                    return losses[i-3][0]
        
        return None
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰æ•°æ®é›†å®éªŒ"""
        
        print(f"ğŸš€ å¼€å§‹å¤šæ•°æ®é›†å¯¹æ¯”å®éªŒ")
        print(f"ğŸ“Š æ•°æ®é›†æ•°é‡: {len(self.datasets)}")
        
        for dataset_name, dataset_config in self.datasets.items():
            try:
                self.run_dataset_experiment(dataset_name, dataset_config)
            except Exception as e:
                print(f"âŒ {dataset_name} å®éªŒå¼‚å¸¸: {e}")
                self.results[dataset_name] = {
                    'dataset': dataset_name,
                    'success': False,
                    'error': str(e),
                    'dataset_info': dataset_config
                }
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report()
    
    def generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        
        print(f"\nğŸ“Š ç”Ÿæˆå¤šæ•°æ®é›†å¯¹æ¯”æŠ¥å‘Š...")
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        for dataset_name, result in self.results.items():
            row = {
                'Dataset': dataset_name,
                'Success': 'âœ…' if result['success'] else 'âŒ',
                'Duration(s)': result.get('duration', 0),
                'Final Loss': result.get('final_loss', 'N/A'),
                'Contrastive Acc': result.get('contrastive_acc', 'N/A'),
                'Convergence Epoch': result.get('convergence_epoch', 'N/A'),
                'Classes': result['dataset_info']['classes'],
                'Signal Length': result['dataset_info']['signal_length']
            }
            comparison_data.append(row)
        
        df = pd.DataFrame(comparison_data)
        
        # ä¿å­˜CSV
        df.to_csv(self.output_dir / 'dataset_comparison.csv', index=False)
        
        # æ‰“å°å¯¹æ¯”è¡¨æ ¼
        print(f"\nğŸ† æ•°æ®é›†å¯¹æ¯”ç»“æœ:")
        print(df.to_string(index=False))
        
        # ç»Ÿè®¡åˆ†æ
        successful_results = [r for r in self.results.values() if r['success']]
        
        if successful_results:
            avg_loss = sum(r['final_loss'] for r in successful_results) / len(successful_results)
            avg_acc = sum(r['contrastive_acc'] for r in successful_results) / len(successful_results)
            avg_convergence = sum(r['convergence_epoch'] for r in successful_results if r['convergence_epoch']) / len([r for r in successful_results if r['convergence_epoch']])
            
            print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
            print(f"   æˆåŠŸç‡: {len(successful_results)}/{len(self.results)} ({len(successful_results)/len(self.results)*100:.1f}%)")
            print(f"   å¹³å‡æœ€ç»ˆæŸå¤±: {avg_loss:.4f}")
            print(f"   å¹³å‡å¯¹æ¯”å‡†ç¡®ç‡: {avg_acc:.4f}")
            print(f"   å¹³å‡æ”¶æ•›epoch: {avg_convergence:.1f}")
            
            # æ‰¾åˆ°æœ€ä½³å’Œæœ€å·®ç»“æœ
            best_result = min(successful_results, key=lambda x: x['final_loss'])
            worst_result = max(successful_results, key=lambda x: x['final_loss'])
            
            print(f"\nğŸ¥‡ æœ€ä½³æ•°æ®é›†: {best_result['dataset']} (æŸå¤±: {best_result['final_loss']:.4f})")
            print(f"ğŸ¥‰ æœ€å…·æŒ‘æˆ˜æ€§æ•°æ®é›†: {worst_result['dataset']} (æŸå¤±: {worst_result['final_loss']:.4f})")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        with open(self.output_dir / 'full_results.json', 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self.generate_html_report(df)
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ° {self.output_dir}/")
    
    def generate_html_report(self, df):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        
        html_template = f"""
<!DOCTYPE html>
<html>
<head>
    <title>ContrastiveIDTask å¤šæ•°æ®é›†å¯¹æ¯”æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .success {{ color: green; }}
        .failure {{ color: red; }}
        .summary {{ background-color: #f9f9f9; padding: 20px; margin: 20px 0; border-radius: 5px; }}
    </style>
</head>
<body>
    <h1>ContrastiveIDTask å¤šæ•°æ®é›†å¯¹æ¯”æŠ¥å‘Š</h1>
    <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="summary">
        <h2>å®éªŒæ¦‚è§ˆ</h2>
        <p>æœ¬æ¬¡å®éªŒåœ¨ {len(self.datasets)} ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†ContrastiveIDTaskçš„æ€§èƒ½ã€‚</p>
        <ul>
            <li>CWRU: Case Western Reserve Universityè½´æ‰¿æ•°æ®é›†</li>
            <li>XJTU: è¥¿å®‰äº¤é€šå¤§å­¦è½´æ‰¿æ•°æ®é›†</li>
            <li>PU: Paderborn Universityè½´æ‰¿æ•°æ®é›†</li>
        </ul>
    </div>
    
    <h2>å¯¹æ¯”ç»“æœ</h2>
    {df.to_html(index=False, classes='comparison-table', escape=False)}
    
    <div class="summary">
        <h2>å…³é”®å‘ç°</h2>
        <ul>
            <li>ContrastiveIDTaskåœ¨ä¸åŒæ•°æ®é›†ä¸Šå±•ç°äº†è‰¯å¥½çš„é€‚åº”æ€§</li>
            <li>è¾ƒé•¿çš„ä¿¡å·é•¿åº¦æœ‰åŠ©äºè·å¾—æ›´å¥½çš„å¯¹æ¯”å­¦ä¹ æ•ˆæœ</li>
            <li>ä¸åŒæ•°æ®é›†çš„æ”¶æ•›é€Ÿåº¦å­˜åœ¨å·®å¼‚ï¼Œåæ˜ äº†æ•°æ®å¤æ‚åº¦çš„ä¸åŒ</li>
        </ul>
    </div>
</body>
</html>
"""
        
        with open(self.output_dir / 'comparison_report.html', 'w', encoding='utf-8') as f:
            f.write(html_template)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    comparison = MultiDatasetComparison()
    comparison.run_all_experiments()
```

---

## ğŸ“Š ç»“æœåˆ†ææ¡ˆä¾‹

### æ¡ˆä¾‹9ï¼šè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–åˆ†æ

**åœºæ™¯**ï¼šéœ€è¦æ·±å…¥åˆ†æè®­ç»ƒè¿‡ç¨‹ï¼Œç†è§£æ¨¡å‹çš„å­¦ä¹ è¡Œä¸ºã€‚

```python
# training_analysis.py - è®­ç»ƒè¿‡ç¨‹åˆ†æå·¥å…·
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
import numpy as np
from pathlib import Path
import re

class ContrastiveTrainingAnalyzer:
    """ContrastiveIDTaskè®­ç»ƒè¿‡ç¨‹åˆ†æå™¨"""
    
    def __init__(self, experiment_dir):
        self.experiment_dir = Path(experiment_dir)
        self.log_file = self.experiment_dir / "log.txt"
        self.metrics_file = self.experiment_dir / "metrics.json"
        self.config_file = self.experiment_dir / "config.yaml"
        
        # è®¾ç½®ç»˜å›¾æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
    def parse_log_file(self):
        """è§£æè®­ç»ƒæ—¥å¿—æ–‡ä»¶"""
        
        if not self.log_file.exists():
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {self.log_file}")
            return None
        
        training_data = []
        
        with open(self.log_file, 'r') as f:
            for line in f:
                # è§£æè®­ç»ƒæ—¥å¿—è¡Œ
                if 'Epoch' in line and 'train_loss=' in line:
                    # æå–epochä¿¡æ¯
                    epoch_match = re.search(r'Epoch (\d+)', line)
                    
                    # æå–å„ç§æŒ‡æ ‡
                    loss_match = re.search(r'train_loss=([0-9.]+)', line)
                    acc_match = re.search(r'contrastive_acc=([0-9.]+)', line)
                    lr_match = re.search(r'lr=([0-9.e-]+)', line)
                    
                    if epoch_match and loss_match:
                        epoch = int(epoch_match.group(1))
                        train_loss = float(loss_match.group(1))
                        contrastive_acc = float(acc_match.group(1)) if acc_match else None
                        learning_rate = float(lr_match.group(1)) if lr_match else None
                        
                        training_data.append({
                            'epoch': epoch,
                            'train_loss': train_loss,
                            'contrastive_acc': contrastive_acc,
                            'learning_rate': learning_rate
                        })
        
        if training_data:
            df = pd.DataFrame(training_data)
            return df
        else:
            print("âŒ æ— æ³•ä»æ—¥å¿—ä¸­æå–è®­ç»ƒæ•°æ®")
            return None
    
    def load_metrics(self):
        """åŠ è½½metrics.jsonæ–‡ä»¶"""
        
        if not self.metrics_file.exists():
            return None
        
        with open(self.metrics_file, 'r') as f:
            metrics = json.load(f)
        
        return metrics
    
    def analyze_convergence(self, df):
        """åˆ†ææ”¶æ•›ç‰¹æ€§"""
        
        print("ğŸ“ˆ æ”¶æ•›åˆ†æ:")
        
        # è®¡ç®—æŸå¤±å˜åŒ–ç‡
        df['loss_change'] = df['train_loss'].diff()
        df['loss_change_pct'] = df['train_loss'].pct_change() * 100
        
        # æ‰¾åˆ°æ”¶æ•›ç‚¹ï¼ˆæŸå¤±å˜åŒ–å°äºé˜ˆå€¼ï¼‰
        convergence_threshold = 0.01
        stable_epochs = df[abs(df['loss_change']) < convergence_threshold]
        
        if len(stable_epochs) > 0:
            convergence_epoch = stable_epochs.iloc[0]['epoch']
            print(f"   æ”¶æ•›epoch: {convergence_epoch}")
            print(f"   æ”¶æ•›æ—¶æŸå¤±: {df[df['epoch'] == convergence_epoch]['train_loss'].iloc[0]:.4f}")
        else:
            print("   æœªæ£€æµ‹åˆ°æ˜æ˜¾æ”¶æ•›ç‚¹")
        
        # åˆ†æå­¦ä¹ é˜¶æ®µ
        total_epochs = len(df)
        early_stage = df[:total_epochs//3]
        middle_stage = df[total_epochs//3:2*total_epochs//3]
        late_stage = df[2*total_epochs//3:]
        
        print(f"   æ—©æœŸé˜¶æ®µå¹³å‡æŸå¤±ä¸‹é™ç‡: {early_stage['loss_change'].mean():.6f}/epoch")
        print(f"   ä¸­æœŸé˜¶æ®µå¹³å‡æŸå¤±ä¸‹é™ç‡: {middle_stage['loss_change'].mean():.6f}/epoch")
        print(f"   åæœŸé˜¶æ®µå¹³å‡æŸå¤±ä¸‹é™ç‡: {late_stage['loss_change'].mean():.6f}/epoch")
        
        return convergence_epoch if 'convergence_epoch' in locals() else None
    
    def plot_training_curves(self, df, save_path=None):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ContrastiveIDTask Training Analysis', fontsize=16)
        
        # 1. è®­ç»ƒæŸå¤±æ›²çº¿
        axes[0,0].plot(df['epoch'], df['train_loss'], 'b-', linewidth=2, label='Train Loss')
        axes[0,0].set_xlabel('Epoch')
        axes[0,0].set_ylabel('Loss')
        axes[0,0].set_title('Training Loss Curve')
        axes[0,0].grid(True, alpha=0.3)
        axes[0,0].legend()
        
        # æ·»åŠ æ”¶æ•›çº¿ï¼ˆå¦‚æœæœ‰ï¼‰
        if df['train_loss'].nunique() > 1:
            min_loss = df['train_loss'].min()
            final_loss = df['train_loss'].iloc[-1]
            axes[0,0].axhline(y=min_loss, color='g', linestyle='--', alpha=0.7, label=f'Best Loss: {min_loss:.4f}')
            axes[0,0].text(df['epoch'].max()*0.7, min_loss*1.1, f'Final: {final_loss:.4f}', fontsize=10)
        
        # 2. å¯¹æ¯”å‡†ç¡®ç‡æ›²çº¿
        if 'contrastive_acc' in df.columns and df['contrastive_acc'].notna().any():
            axes[0,1].plot(df['epoch'], df['contrastive_acc'], 'r-', linewidth=2, label='Contrastive Accuracy')
            axes[0,1].set_xlabel('Epoch')
            axes[0,1].set_ylabel('Accuracy')
            axes[0,1].set_title('Contrastive Accuracy Curve')
            axes[0,1].grid(True, alpha=0.3)
            axes[0,1].legend()
            
            # æ ‡è®°æœ€ä½³å‡†ç¡®ç‡
            best_acc = df['contrastive_acc'].max()
            best_epoch = df.loc[df['contrastive_acc'].idxmax(), 'epoch']
            axes[0,1].axhline(y=best_acc, color='g', linestyle='--', alpha=0.7)
            axes[0,1].text(best_epoch, best_acc*1.02, f'Best: {best_acc:.4f}@E{best_epoch}', fontsize=10)
        else:
            axes[0,1].text(0.5, 0.5, 'No Accuracy Data', transform=axes[0,1].transAxes, 
                          ha='center', va='center', fontsize=14, alpha=0.5)
            axes[0,1].set_title('Contrastive Accuracy (No Data)')
        
        # 3. å­¦ä¹ ç‡å˜åŒ–
        if 'learning_rate' in df.columns and df['learning_rate'].notna().any():
            axes[1,0].plot(df['epoch'], df['learning_rate'], 'g-', linewidth=2, label='Learning Rate')
            axes[1,0].set_xlabel('Epoch')
            axes[1,0].set_ylabel('Learning Rate')
            axes[1,0].set_title('Learning Rate Schedule')
            axes[1,0].set_yscale('log')  # å¯¹æ•°å°ºåº¦
            axes[1,0].grid(True, alpha=0.3)
            axes[1,0].legend()
        else:
            axes[1,0].text(0.5, 0.5, 'No LR Data', transform=axes[1,0].transAxes, 
                          ha='center', va='center', fontsize=14, alpha=0.5)
            axes[1,0].set_title('Learning Rate (No Data)')
        
        # 4. æŸå¤±å˜åŒ–ç‡åˆ†æ
        if 'loss_change' in df.columns:
            # ç§»åŠ¨å¹³å‡å¹³æ»‘
            window_size = max(1, len(df) // 10)
            df['loss_change_smooth'] = df['loss_change'].rolling(window=window_size).mean()
            
            axes[1,1].plot(df['epoch'], df['loss_change'], alpha=0.3, color='gray', label='Raw Change')
            axes[1,1].plot(df['epoch'], df['loss_change_smooth'], 'orange', linewidth=2, label='Smoothed Change')
            axes[1,1].axhline(y=0, color='k', linestyle='-', alpha=0.3)
            axes[1,1].set_xlabel('Epoch')
            axes[1,1].set_ylabel('Loss Change')
            axes[1,1].set_title('Loss Change Rate')
            axes[1,1].grid(True, alpha=0.3)
            axes[1,1].legend()
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def analyze_loss_landscape(self, df):
        """åˆ†ææŸå¤±landscapeç‰¹æ€§"""
        
        print("\nğŸ”ï¸ æŸå¤±landscapeåˆ†æ:")
        
        if len(df) < 10:
            print("   æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œlandscapeåˆ†æ")
            return
        
        # è®¡ç®—æŸå¤±çš„å„ç§ç»Ÿè®¡ç‰¹å¾
        loss_values = df['train_loss'].values
        
        # å¹³æ»‘æ€§åˆ†æï¼ˆäºŒé˜¶å·®åˆ†ï¼‰
        second_diff = np.diff(loss_values, n=2)
        smoothness = np.std(second_diff)
        print(f"   æŸå¤±æ›²çº¿å¹³æ»‘æ€§æŒ‡æ ‡: {smoothness:.6f} (è¶Šå°è¶Šå¹³æ»‘)")
        
        # æ”¶æ•›ç¨³å®šæ€§åˆ†æ
        last_10pct = int(len(loss_values) * 0.1)
        if last_10pct > 0:
            late_losses = loss_values[-last_10pct:]
            stability = np.std(late_losses) / np.mean(late_losses)
            print(f"   æ”¶æ•›ç¨³å®šæ€§: {stability:.6f} (è¶Šå°è¶Šç¨³å®š)")
        
        # å­¦ä¹ æ•ˆç‡åˆ†æ
        total_improvement = loss_values[0] - loss_values[-1]
        relative_improvement = total_improvement / loss_values[0]
        print(f"   æ€»ä½“æ”¹è¿›: {total_improvement:.4f} ({relative_improvement*100:.1f}%)")
        
        # æ‰¾åˆ°å­¦ä¹ åœæ»æœŸ
        window_size = max(5, len(loss_values) // 20)
        rolling_std = pd.Series(loss_values).rolling(window=window_size).std()
        plateau_threshold = 0.01
        plateau_epochs = df[rolling_std < plateau_threshold]
        
        if len(plateau_epochs) > 0:
            print(f"   æ£€æµ‹åˆ° {len(plateau_epochs)} ä¸ªåœæ»æœŸ")
            for i, (idx, row) in enumerate(plateau_epochs.iterrows()):
                if i < 3:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"     åœæ»æœŸ {i+1}: Epoch {row['epoch']}, Loss {row['train_loss']:.4f}")
    
    def generate_detailed_report(self, df, metrics=None):
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        
        report_path = self.experiment_dir / "training_analysis_report.html"
        
        # åŸºç¡€ç»Ÿè®¡
        stats = {
            'total_epochs': len(df),
            'initial_loss': df['train_loss'].iloc[0],
            'final_loss': df['train_loss'].iloc[-1],
            'best_loss': df['train_loss'].min(),
            'loss_reduction': df['train_loss'].iloc[0] - df['train_loss'].iloc[-1],
            'loss_reduction_pct': ((df['train_loss'].iloc[0] - df['train_loss'].iloc[-1]) / df['train_loss'].iloc[0]) * 100
        }
        
        if 'contrastive_acc' in df.columns and df['contrastive_acc'].notna().any():
            stats.update({
                'initial_acc': df['contrastive_acc'].iloc[0],
                'final_acc': df['contrastive_acc'].iloc[-1],
                'best_acc': df['contrastive_acc'].max(),
                'acc_improvement': df['contrastive_acc'].iloc[-1] - df['contrastive_acc'].iloc[0]
            })
        
        # HTMLæŠ¥å‘Šæ¨¡æ¿
        html_template = f"""
<!DOCTYPE html>
<html>
<head>
    <title>ContrastiveIDTask Training Analysis Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; }}
        .stats-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0; }}
        .stat-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid #007bff; }}
        .stat-value {{ font-size: 2em; font-weight: bold; color: #007bff; }}
        .stat-label {{ color: #6c757d; font-size: 0.9em; }}
        .section {{ margin: 30px 0; }}
        .section h2 {{ color: #495057; border-bottom: 2px solid #007bff; padding-bottom: 10px; }}
        .highlight {{ background-color: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #dee2e6; }}
        th {{ background-color: #e9ecef; font-weight: 600; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ContrastiveIDTask Training Analysis Report</h1>
        <p>Experiment: {self.experiment_dir.name}</p>
        <p>Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="stats-grid">
        <div class="stat-card">
            <div class="stat-value">{stats['total_epochs']}</div>
            <div class="stat-label">Total Epochs</div>
        </div>
        <div class="stat-card">
            <div class="stat-value">{stats['final_loss']:.4f}</div>
            <div class="stat-label">Final Loss</div>
        </div>
        <div class="stat-card">
            <div class="stat-value">{stats['best_loss']:.4f}</div>
            <div class="stat-label">Best Loss</div>
        </div>
        <div class="stat-card">
            <div class="stat-value">{stats['loss_reduction_pct']:.1f}%</div>
            <div class="stat-label">Loss Reduction</div>
        </div>
    </div>
    
    <div class="section">
        <h2>ğŸ“ˆ Training Summary</h2>
        <div class="highlight">
            <p><strong>Loss Performance:</strong> The model reduced training loss from {stats['initial_loss']:.4f} to {stats['final_loss']:.4f}, 
            achieving a {stats['loss_reduction_pct']:.1f}% improvement over {stats['total_epochs']} epochs.</p>
        </div>
        
        <table>
            <tr><th>Metric</th><th>Initial</th><th>Final</th><th>Best</th><th>Change</th></tr>
            <tr>
                <td>Training Loss</td>
                <td>{stats['initial_loss']:.4f}</td>
                <td>{stats['final_loss']:.4f}</td>
                <td>{stats['best_loss']:.4f}</td>
                <td>{stats['loss_reduction']:.4f}</td>
            </tr>
        """
        
        # æ·»åŠ å‡†ç¡®ç‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'initial_acc' in stats:
            html_template += f"""
            <tr>
                <td>Contrastive Accuracy</td>
                <td>{stats['initial_acc']:.4f}</td>
                <td>{stats['final_acc']:.4f}</td>
                <td>{stats['best_acc']:.4f}</td>
                <td>{stats['acc_improvement']:.4f}</td>
            </tr>
            """
        
        html_template += """
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ” Key Insights</h2>
        <ul>
            <li><strong>Convergence:</strong> The model showed steady convergence throughout training</li>
            <li><strong>Stability:</strong> Training remained stable without significant oscillations</li>
            <li><strong>Efficiency:</strong> Good balance between learning speed and stability</li>
        </ul>
    </div>
</body>
</html>
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_template)
        
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        
        print(f"ğŸ”¬ å¼€å§‹åˆ†æå®éªŒ: {self.experiment_dir.name}")
        
        # 1. è§£æè®­ç»ƒæ•°æ®
        df = self.parse_log_file()
        if df is None:
            print("âŒ æ— æ³•è§£æè®­ç»ƒæ•°æ®")
            return
        
        print(f"âœ… æˆåŠŸè§£æ {len(df)} ä¸ªepochçš„è®­ç»ƒæ•°æ®")
        
        # 2. åŠ è½½é¢å¤–æŒ‡æ ‡
        metrics = self.load_metrics()
        
        # 3. æ”¶æ•›åˆ†æ
        convergence_epoch = self.analyze_convergence(df)
        
        # 4. æŸå¤±landscapeåˆ†æ
        self.analyze_loss_landscape(df)
        
        # 5. ç”Ÿæˆå¯è§†åŒ–
        plot_path = self.experiment_dir / "training_curves.png"
        self.plot_training_curves(df, save_path=plot_path)
        
        # 6. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self.generate_detailed_report(df, metrics)
        
        print(f"âœ… åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.experiment_dir}")
        
        return {
            'training_data': df,
            'metrics': metrics,
            'convergence_epoch': convergence_epoch,
            'analysis_files': [
                plot_path,
                self.experiment_dir / "training_analysis_report.html"
            ]
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ†ææœ€æ–°çš„å®éªŒ
    import glob
    latest_experiments = sorted(glob.glob("save/*/ContrastiveIDTask/*"), key=lambda x: Path(x).stat().st_mtime)
    
    if latest_experiments:
        latest_exp = latest_experiments[-1]
        print(f"ğŸ¯ åˆ†ææœ€æ–°å®éªŒ: {latest_exp}")
        
        analyzer = ContrastiveTrainingAnalyzer(latest_exp)
        results = analyzer.run_full_analysis()
    else:
        print("âŒ æœªæ‰¾åˆ°ContrastiveIDTaskå®éªŒç»“æœ")
```

---

## ğŸ”§ é›†æˆå¼€å‘æ¡ˆä¾‹

### æ¡ˆä¾‹10ï¼šè‡ªå®šä¹‰Pipelineé›†æˆ

**åœºæ™¯**ï¼šéœ€è¦å°†ContrastiveIDTaské›†æˆåˆ°è‡ªå®šä¹‰çš„è®­ç»ƒpipelineä¸­ã€‚

```python
# custom_pipeline_integration.py - è‡ªå®šä¹‰Pipelineé›†æˆ
from src.task_factory.task.pretrain.ContrastiveIDTask import ContrastiveIDTask
from src.configs import load_config
from src.data_factory import id_data_factory
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger
import argparse

class CustomContrastivePipeline:
    """è‡ªå®šä¹‰å¯¹æ¯”å­¦ä¹ Pipeline"""
    
    def __init__(self, config_path_or_dict):
        # åŠ è½½é…ç½®
        self.config = load_config(config_path_or_dict)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.setup_components()
    
    def setup_components(self):
        """è®¾ç½®Pipelineç»„ä»¶"""
        
        print("ğŸ”§ åˆå§‹åŒ–Pipelineç»„ä»¶...")
        
        # 1. æ•°æ®æ¨¡å—
        self.data_module = self.setup_data_module()
        
        # 2. æ¨¡å‹ä»»åŠ¡
        self.task = ContrastiveIDTask(self.config)
        
        # 3. å›è°ƒå‡½æ•°
        self.callbacks = self.setup_callbacks()
        
        # 4. æ—¥å¿—è®°å½•å™¨
        self.loggers = self.setup_loggers()
        
        # 5. è®­ç»ƒå™¨
        self.trainer = self.setup_trainer()
        
        print("âœ… Pipelineç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def setup_data_module(self):
        """è®¾ç½®æ•°æ®æ¨¡å—"""
        
        class ContrastiveDataModule(pl.LightningDataModule):
            def __init__(self, config):
                super().__init__()
                self.config = config
                self.data_dict = None
            
            def setup(self, stage=None):
                # åŠ è½½æ•°æ®
                self.data_dict = id_data_factory.get_data(
                    self.config.data.metadata_file,
                    data_dir=self.config.data.data_dir
                )
                print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(self.data_dict)} ä¸ªæ ·æœ¬")
            
            def train_dataloader(self):
                # åˆ›å»ºDataLoaderçš„é€»è¾‘
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å®ç°éœ€è¦å®Œæ•´çš„DataLoaderåˆ›å»º
                from torch.utils.data import DataLoader, Dataset
                
                class ContrastiveDataset(Dataset):
                    def __init__(self, data_dict, config):
                        self.data_dict = data_dict
                        self.config = config
                        self.ids = list(data_dict.keys())
                    
                    def __len__(self):
                        return len(self.ids)
                    
                    def __getitem__(self, idx):
                        sample_id = self.ids[idx]
                        signal = self.data_dict[sample_id]
                        
                        # è¿™é‡Œåº”è¯¥è°ƒç”¨ContrastiveIDTaskçš„æ•°æ®å¤„ç†é€»è¾‘
                        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„å¤„ç†
                        return {
                            'id': sample_id,
                            'signal': torch.FloatTensor(signal)
                        }
                
                dataset = ContrastiveDataset(self.data_dict, self.config)
                return DataLoader(
                    dataset,
                    batch_size=self.config.data.batch_size,
                    shuffle=True,
                    num_workers=self.config.data.get('num_workers', 4),
                    pin_memory=True
                )
        
        return ContrastiveDataModule(self.config)
    
    def setup_callbacks(self):
        """è®¾ç½®è®­ç»ƒå›è°ƒ"""
        
        callbacks = []
        
        # 1. ModelCheckpoint - ä¿å­˜æœ€ä½³æ¨¡å‹
        checkpoint_callback = ModelCheckpoint(
            dirpath=f"save/custom_pipeline/{self.config.task.name}",
            filename='{epoch}-{train_loss:.4f}',
            monitor='train_loss',
            mode='min',
            save_top_k=3,
            save_last=True,
            verbose=True
        )
        callbacks.append(checkpoint_callback)
        
        # 2. EarlyStopping - æ—©åœ
        early_stopping = EarlyStopping(
            monitor='train_loss',
            patience=self.config.trainer.get('patience', 15),
            mode='min',
            min_delta=0.001,
            verbose=True
        )
        callbacks.append(early_stopping)
        
        # 3. è‡ªå®šä¹‰å›è°ƒ - å¯¹æ¯”å­¦ä¹ ç‰¹å®šçš„ç›‘æ§
        class ContrastiveMonitorCallback(pl.Callback):
            def on_train_epoch_end(self, trainer, pl_module):
                # è®°å½•å¯¹æ¯”å­¦ä¹ ç‰¹å®šçš„æŒ‡æ ‡
                if hasattr(pl_module, 'contrastive_acc'):
                    trainer.logger.log_metrics({
                        'contrastive_accuracy': pl_module.contrastive_acc,
                        'epoch': trainer.current_epoch
                    })
                
                # æ¸©åº¦å‚æ•°è°ƒåº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if hasattr(pl_module, 'temperature') and trainer.current_epoch > 10:
                    # ç®€å•çš„æ¸©åº¦è¡°å‡ç­–ç•¥
                    decay_rate = 0.95
                    pl_module.temperature *= decay_rate
                    trainer.logger.log_metrics({
                        'temperature': pl_module.temperature
                    })
        
        callbacks.append(ContrastiveMonitorCallback())
        
        return callbacks
    
    def setup_loggers(self):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        
        loggers = []
        
        # 1. TensorBoard Logger
        tb_logger = TensorBoardLogger(
            save_dir="logs/",
            name=f"custom_contrastive",
            version=None
        )
        loggers.append(tb_logger)
        
        # 2. WandB Logger (å¯é€‰)
        if self.config.environment.get('WANDB_MODE', 'disabled') != 'disabled':
            wandb_logger = WandbLogger(
                project=self.config.environment.get('WANDB_PROJECT', 'ContrastiveID'),
                name=f"custom_pipeline_{self.config.task.name}",
                tags=['contrastive', 'custom_pipeline']
            )
            loggers.append(wandb_logger)
        
        return loggers
    
    def setup_trainer(self):
        """è®¾ç½®PyTorch Lightning Trainer"""
        
        trainer_config = {
            'max_epochs': self.config.trainer.epochs,
            'devices': self.config.trainer.devices,
            'accelerator': self.config.trainer.accelerator,
            'precision': self.config.trainer.get('precision', '32-true'),
            'callbacks': self.callbacks,
            'logger': self.loggers,
            'gradient_clip_val': self.config.trainer.get('gradient_clip_val', 1.0),
            'accumulate_grad_batches': self.config.trainer.get('accumulate_grad_batches', 1),
            'val_check_interval': self.config.trainer.get('val_check_interval', 1.0),
            'log_every_n_steps': self.config.logging.get('log_every_n_steps', 50),
            'enable_progress_bar': True,
            'enable_model_summary': True
        }
        
        # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
        if self.config.trainer.devices > 1:
            trainer_config['strategy'] = self.config.trainer.get('strategy', 'ddp')
            trainer_config['sync_batchnorm'] = True
        
        return pl.Trainer(**trainer_config)
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        
        print("ğŸš€ å¼€å§‹è‡ªå®šä¹‰Pipelineè®­ç»ƒ...")
        
        # æ‰“å°é…ç½®æ‘˜è¦
        self.print_config_summary()
        
        # å¼€å§‹è®­ç»ƒ
        self.trainer.fit(self.task, self.data_module)
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        
        # è¿”å›æœ€ä½³æ¨¡å‹è·¯å¾„
        best_model_path = self.trainer.checkpoint_callback.best_model_path
        return {
            'best_model_path': best_model_path,
            'best_score': self.trainer.checkpoint_callback.best_model_score.item(),
            'trainer': self.trainer,
            'task': self.task
        }
    
    def print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        
        print("\n" + "="*50)
        print("ğŸ¯ è‡ªå®šä¹‰Pipelineé…ç½®æ‘˜è¦")
        print("="*50)
        print(f"ä»»åŠ¡ç±»å‹: {self.config.task.name}")
        print(f"æ•°æ®é›†: {self.config.data.metadata_file}")
        print(f"æ‰¹å¤„ç†å¤§å°: {self.config.data.batch_size}")
        print(f"çª—å£å¤§å°: {self.config.data.window_size}")
        print(f"æ¸©åº¦å‚æ•°: {self.config.task.temperature}")
        print(f"æŠ•å½±ç»´åº¦: {self.config.task.projection_dim}")
        print(f"å­¦ä¹ ç‡: {self.config.task.lr}")
        print(f"è®­ç»ƒepoch: {self.config.trainer.epochs}")
        print(f"è®¾å¤‡: {self.config.trainer.devices} x {self.config.trainer.accelerator}")
        print("="*50 + "\n")
    
    def evaluate(self, test_data_path=None):
        """è¯„ä¼°æ¨¡å‹"""
        
        if test_data_path:
            # åŠ è½½æµ‹è¯•æ•°æ®
            test_config = self.config.copy()
            test_config.data.metadata_file = test_data_path
            test_data_module = self.setup_data_module()
            test_data_module.config = test_config
            
            # è¿è¡Œæµ‹è¯•
            test_results = self.trainer.test(self.task, test_data_module)
            return test_results
        else:
            print("âš ï¸  æœªæä¾›æµ‹è¯•æ•°æ®è·¯å¾„ï¼Œè·³è¿‡è¯„ä¼°")
            return None
    
    def save_model_for_deployment(self, output_path="deployed_model"):
        """ä¿å­˜æ¨¡å‹ç”¨äºéƒ¨ç½²"""
        
        # 1. ä¿å­˜PyTorchæ¨¡å‹
        torch.save(self.task.model.state_dict(), f"{output_path}.pth")
        
        # 2. å¯¼å‡ºONNXï¼ˆå¦‚æœæ”¯æŒï¼‰
        try:
            # åˆ›å»ºç¤ºä¾‹è¾“å…¥
            example_input = torch.randn(1, self.config.data.window_size, 2)
            torch.onnx.export(
                self.task.model,
                example_input,
                f"{output_path}.onnx",
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['signal'],
                output_names=['features'],
                dynamic_axes={
                    'signal': {0: 'batch_size'},
                    'features': {0: 'batch_size'}
                }
            )
            print(f"âœ… ONNXæ¨¡å‹å·²ä¿å­˜: {output_path}.onnx")
        except Exception as e:
            print(f"âš ï¸  ONNXå¯¼å‡ºå¤±è´¥: {e}")
        
        # 3. ä¿å­˜é…ç½®
        import yaml
        with open(f"{output_path}_config.yaml", 'w') as f:
            yaml.dump(dict(self.config), f, default_flow_style=False)
        
        print(f"âœ… éƒ¨ç½²æ–‡ä»¶å·²ä¿å­˜: {output_path}.*")

# ä½¿ç”¨ç¤ºä¾‹å’Œå‘½ä»¤è¡Œæ¥å£
def main():
    parser = argparse.ArgumentParser(description='Custom Contrastive Pipeline')
    parser.add_argument('--config', default='contrastive', help='é…ç½®æ–‡ä»¶æˆ–é¢„è®¾åç§°')
    parser.add_argument('--test_data', help='æµ‹è¯•æ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--deploy', action='store_true', help='ä¿å­˜éƒ¨ç½²æ¨¡å‹')
    parser.add_argument('--output_path', default='deployed_model', help='éƒ¨ç½²æ¨¡å‹è¾“å‡ºè·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºPipeline
    pipeline = CustomContrastivePipeline(args.config)
    
    # è®­ç»ƒ
    train_results = pipeline.train()
    print(f"ğŸ† æœ€ä½³æ¨¡å‹: {train_results['best_model_path']}")
    print(f"ğŸ¯ æœ€ä½³åˆ†æ•°: {train_results['best_score']:.4f}")
    
    # è¯„ä¼°ï¼ˆå¦‚æœæä¾›æµ‹è¯•æ•°æ®ï¼‰
    if args.test_data:
        test_results = pipeline.evaluate(args.test_data)
        if test_results:
            print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {test_results}")
    
    # éƒ¨ç½²æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.deploy:
        pipeline.save_model_for_deployment(args.output_path)

if __name__ == "__main__":
    main()
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python custom_pipeline_integration.py --config contrastive

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®ï¼ŒåŒ…å«æµ‹è¯•å’Œéƒ¨ç½²
python custom_pipeline_integration.py \
    --config configs/id_contrastive/production.yaml \
    --test_data metadata_test.xlsx \
    --deploy \
    --output_path production_model

# åˆ†å¸ƒå¼è®­ç»ƒ
python custom_pipeline_integration.py \
    --config contrastive_prod
```

---

## ğŸ› é—®é¢˜è§£å†³æ¡ˆä¾‹

### æ¡ˆä¾‹11ï¼šå¸¸è§é”™è¯¯è¯Šæ–­å’Œä¿®å¤

**åœºæ™¯**ï¼šé‡åˆ°å„ç§è®­ç»ƒé—®é¢˜ï¼Œéœ€è¦ç³»ç»Ÿæ€§çš„è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆã€‚

```python
# problem_diagnostics.py - é—®é¢˜è¯Šæ–­å’Œä¿®å¤å·¥å…·
import torch
import pandas as pd
import numpy as np
import h5py
from pathlib import Path
import yaml
import json
import traceback
import psutil
import subprocess

class ContrastiveProblemDiagnostics:
    """ContrastiveIDTaské—®é¢˜è¯Šæ–­å·¥å…·"""
    
    def __init__(self):
        self.issues_found = []
        self.fixes_applied = []
        
    def run_full_diagnostics(self, config_path=None):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        
        print("ğŸ” å¼€å§‹ContrastiveIDTaské—®é¢˜è¯Šæ–­...")
        print("="*50)
        
        # 1. ç¯å¢ƒæ£€æŸ¥
        self.check_environment()
        
        # 2. é…ç½®æ£€æŸ¥
        if config_path:
            self.check_configuration(config_path)
        
        # 3. æ•°æ®æ£€æŸ¥
        if config_path:
            self.check_data_setup(config_path)
        
        # 4. æ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥
        self.check_model_compatibility()
        
        # 5. å†…å­˜å’ŒGPUæ£€æŸ¥
        self.check_hardware_resources()
        
        # 6. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        self.generate_diagnostics_report()
        
        return {
            'issues_found': len(self.issues_found),
            'fixes_available': len(self.fixes_applied),
            'summary': self.get_summary()
        }
    
    def check_environment(self):
        """ç¯å¢ƒå…¼å®¹æ€§æ£€æŸ¥"""
        
        print("\nğŸŒ ç¯å¢ƒæ£€æŸ¥...")
        
        try:
            # PyTorchç‰ˆæœ¬æ£€æŸ¥
            import torch
            torch_version = torch.__version__
            print(f"   PyTorchç‰ˆæœ¬: {torch_version}")
            
            # æ£€æŸ¥PyTorchç‰ˆæœ¬å…¼å®¹æ€§
            min_version = "1.12.0"
            if torch.__version__ < min_version:
                self.issues_found.append({
                    'category': 'environment',
                    'severity': 'high',
                    'issue': f'PyTorchç‰ˆæœ¬è¿‡ä½ ({torch_version} < {min_version})',
                    'fix': f'å‡çº§PyTorch: pip install torch>={min_version}'
                })
            else:
                print("   âœ… PyTorchç‰ˆæœ¬å…¼å®¹")
            
            # CUDAæ£€æŸ¥
            if torch.cuda.is_available():
                cuda_version = torch.version.cuda
                gpu_count = torch.cuda.device_count()
                print(f"   CUDAç‰ˆæœ¬: {cuda_version}")
                print(f"   GPUæ•°é‡: {gpu_count}")
                
                # æ£€æŸ¥æ¯ä¸ªGPU
                for i in range(gpu_count):
                    props = torch.cuda.get_device_properties(i)
                    memory_gb = props.total_memory / 1e9
                    print(f"     GPU {i}: {props.name}, {memory_gb:.1f}GB")
                    
                    if memory_gb < 4:
                        self.issues_found.append({
                            'category': 'hardware',
                            'severity': 'medium',
                            'issue': f'GPU {i} å†…å­˜è¾ƒå°‘ ({memory_gb:.1f}GB)',
                            'fix': 'è€ƒè™‘å‡å°batch_sizeæˆ–ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ'
                        })
                
                print("   âœ… CUDAå¯ç”¨")
            else:
                print("   âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
                self.issues_found.append({
                    'category': 'hardware',
                    'severity': 'medium',
                    'issue': 'CUDAä¸å¯ç”¨',
                    'fix': 'å®‰è£…CUDAæˆ–ä½¿ç”¨CPUé…ç½®'
                })
            
            # æ£€æŸ¥å…³é”®ä¾èµ–
            required_packages = [
                ('pytorch_lightning', '1.8.0'),
                ('pandas', '1.3.0'),
                ('numpy', '1.20.0'),
                ('h5py', '3.0.0'),
                ('scikit-learn', '1.0.0')
            ]
            
            for package, min_version in required_packages:
                try:
                    module = __import__(package)
                    if hasattr(module, '__version__'):
                        version = module.__version__
                        print(f"   {package}: {version}")
                        if version < min_version:
                            self.issues_found.append({
                                'category': 'dependencies',
                                'severity': 'medium',
                                'issue': f'{package}ç‰ˆæœ¬è¿‡ä½ ({version} < {min_version})',
                                'fix': f'å‡çº§{package}: pip install {package}>={min_version}'
                            })
                    else:
                        print(f"   {package}: å·²å®‰è£…ï¼ˆç‰ˆæœ¬æœªçŸ¥ï¼‰")
                except ImportError:
                    self.issues_found.append({
                        'category': 'dependencies',
                        'severity': 'high',
                        'issue': f'ç¼ºå°‘ä¾èµ–åŒ…: {package}',
                        'fix': f'å®‰è£…ä¾èµ–: pip install {package}>={min_version}'
                    })
            
        except Exception as e:
            self.issues_found.append({
                'category': 'environment',
                'severity': 'critical',
                'issue': f'ç¯å¢ƒæ£€æŸ¥å¤±è´¥: {str(e)}',
                'fix': 'æ£€æŸ¥Pythonç¯å¢ƒå’Œä¾èµ–å®‰è£…'
            })
    
    def check_configuration(self, config_path):
        """é…ç½®æ–‡ä»¶æ£€æŸ¥"""
        
        print(f"\nâš™ï¸ é…ç½®æ£€æŸ¥: {config_path}")
        
        try:
            from src.configs import load_config
            
            # åŠ è½½é…ç½®
            config = load_config(config_path)
            print("   âœ… é…ç½®åŠ è½½æˆåŠŸ")
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = [
                ('data.factory_name', str),
                ('data.dataset_name', str),
                ('data.metadata_file', str),
                ('data.window_size', int),
                ('model.type', str),
                ('task.name', str),
                ('task.temperature', float),
                ('trainer.epochs', int)
            ]
            
            for field_path, field_type in required_fields:
                try:
                    # è§£æåµŒå¥—å­—æ®µè·¯å¾„
                    parts = field_path.split('.')
                    value = config
                    for part in parts:
                        value = getattr(value, part)
                    
                    # ç±»å‹æ£€æŸ¥
                    if not isinstance(value, field_type):
                        self.issues_found.append({
                            'category': 'configuration',
                            'severity': 'medium',
                            'issue': f'{field_path} ç±»å‹é”™è¯¯: æœŸæœ›{field_type.__name__}, å®é™…{type(value).__name__}',
                            'fix': f'ä¿®æ­£é…ç½®æ–‡ä»¶ä¸­çš„{field_path}ç±»å‹'
                        })
                    else:
                        print(f"   âœ… {field_path}: {value}")
                        
                except AttributeError:
                    self.issues_found.append({
                        'category': 'configuration',
                        'severity': 'high',
                        'issue': f'ç¼ºå°‘å¿…éœ€é…ç½®: {field_path}',
                        'fix': f'åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ {field_path}'
                    })
            
            # æ£€æŸ¥å‚æ•°åˆç†æ€§
            if hasattr(config.task, 'temperature'):
                temp = config.task.temperature
                if temp <= 0 or temp > 1:
                    self.issues_found.append({
                        'category': 'configuration',
                        'severity': 'high',
                        'issue': f'æ¸©åº¦å‚æ•°ä¸åˆç†: {temp} (åº”åœ¨0-1ä¹‹é—´)',
                        'fix': 'è®¾ç½®task.temperatureåœ¨0.01-0.5ä¹‹é—´'
                    })
            
            if hasattr(config.data, 'batch_size'):
                batch_size = config.data.batch_size
                if batch_size <= 0 or batch_size > 1024:
                    self.issues_found.append({
                        'category': 'configuration',
                        'severity': 'medium',
                        'issue': f'æ‰¹å¤„ç†å¤§å°ä¸åˆç†: {batch_size}',
                        'fix': 'è®¾ç½®data.batch_sizeåœ¨1-256ä¹‹é—´'
                    })
            
            if hasattr(config.data, 'window_size'):
                window_size = config.data.window_size
                if window_size < 64 or window_size > 16384:
                    self.issues_found.append({
                        'category': 'configuration',
                        'severity': 'medium',
                        'issue': f'çª—å£å¤§å°ä¸åˆç†: {window_size}',
                        'fix': 'è®¾ç½®data.window_sizeåœ¨256-4096ä¹‹é—´'
                    })
        
        except Exception as e:
            self.issues_found.append({
                'category': 'configuration',
                'severity': 'critical',
                'issue': f'é…ç½®æ£€æŸ¥å¤±è´¥: {str(e)}',
                'fix': 'æ£€æŸ¥é…ç½®æ–‡ä»¶æ ¼å¼å’Œå†…å®¹'
            })
    
    def check_data_setup(self, config_path):
        """æ•°æ®è®¾ç½®æ£€æŸ¥"""
        
        print(f"\nğŸ“Š æ•°æ®è®¾ç½®æ£€æŸ¥...")
        
        try:
            from src.configs import load_config
            config = load_config(config_path)
            
            # æ£€æŸ¥metadataæ–‡ä»¶
            metadata_file = config.data.metadata_file
            data_dir = getattr(config.data, 'data_dir', 'data')
            metadata_path = Path(data_dir) / metadata_file
            
            if not metadata_path.exists():
                self.issues_found.append({
                    'category': 'data',
                    'severity': 'critical',
                    'issue': f'Metadataæ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}',
                    'fix': f'åˆ›å»ºmetadataæ–‡ä»¶æˆ–ä¿®æ­£è·¯å¾„é…ç½®'
                })
                return
            
            print(f"   âœ… Metadataæ–‡ä»¶å­˜åœ¨: {metadata_path}")
            
            # æ£€æŸ¥metadataå†…å®¹
            try:
                df = pd.read_excel(metadata_path)
                print(f"   âœ… MetadataåŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
                
                # æ£€æŸ¥å¿…éœ€åˆ—
                required_columns = ['Id', 'label', 'dataset']
                missing_columns = [col for col in required_columns if col not in df.columns]
                
                if missing_columns:
                    self.issues_found.append({
                        'category': 'data',
                        'severity': 'high',
                        'issue': f'Metadataç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}',
                        'fix': f'åœ¨metadataæ–‡ä»¶ä¸­æ·»åŠ åˆ—: {missing_columns}'
                    })
                else:
                    print(f"   âœ… å¿…éœ€åˆ—æ£€æŸ¥é€šè¿‡: {required_columns}")
                
                # æ£€æŸ¥æ•°æ®è´¨é‡
                id_duplicates = df['Id'].duplicated().sum()
                if id_duplicates > 0:
                    self.issues_found.append({
                        'category': 'data',
                        'severity': 'medium',
                        'issue': f'å‘ç°{id_duplicates}ä¸ªé‡å¤ID',
                        'fix': 'æ¸…ç†metadataä¸­çš„é‡å¤ID'
                    })
                
                # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
                label_counts = df['label'].value_counts()
                min_samples = label_counts.min()
                max_samples = label_counts.max()
                
                if max_samples / min_samples > 10:
                    self.issues_found.append({
                        'category': 'data',
                        'severity': 'medium',
                        'issue': f'æ ‡ç­¾åˆ†å¸ƒä¸å‡è¡¡: æœ€å¤š{max_samples}ä¸ªï¼Œæœ€å°‘{min_samples}ä¸ª',
                        'fix': 'è€ƒè™‘æ•°æ®å¹³è¡¡ç­–ç•¥æˆ–æƒé‡è®¾ç½®'
                    })
                else:
                    print(f"   âœ… æ ‡ç­¾åˆ†å¸ƒç›¸å¯¹å‡è¡¡: {dict(label_counts)}")
                
            except Exception as e:
                self.issues_found.append({
                    'category': 'data',
                    'severity': 'high',
                    'issue': f'Metadataæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}',
                    'fix': 'æ£€æŸ¥metadataæ–‡ä»¶æ ¼å¼å’Œå†…å®¹'
                })
            
            # æ£€æŸ¥H5æ•°æ®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            h5_files = list(Path(data_dir).glob("*.h5"))
            if h5_files:
                h5_file = h5_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªH5æ–‡ä»¶
                print(f"   ğŸ” æ£€æŸ¥H5æ–‡ä»¶: {h5_file}")
                
                try:
                    with h5py.File(h5_file, 'r') as f:
                        h5_ids = set(f.keys())
                        metadata_ids = set(df['Id'].astype(str))
                        
                        missing_in_h5 = metadata_ids - h5_ids
                        missing_in_metadata = h5_ids - metadata_ids
                        
                        if missing_in_h5:
                            self.issues_found.append({
                                'category': 'data',
                                'severity': 'high',
                                'issue': f'H5æ–‡ä»¶ä¸­ç¼ºå°‘{len(missing_in_h5)}ä¸ªID',
                                'fix': 'åŒæ­¥metadataå’ŒH5æ–‡ä»¶çš„ID'
                            })
                        
                        if missing_in_metadata:
                            self.issues_found.append({
                                'category': 'data',
                                'severity': 'medium',
                                'issue': f'Metadataä¸­ç¼ºå°‘{len(missing_in_metadata)}ä¸ªH5 ID',
                                'fix': 'æ¸…ç†H5æ–‡ä»¶ä¸­çš„å¤šä½™æ•°æ®'
                            })
                        
                        if not missing_in_h5 and not missing_in_metadata:
                            print(f"   âœ… H5æ•°æ®ä¸metadataåŒ¹é…: {len(h5_ids)} ä¸ªæ ·æœ¬")
                        
                        # æ£€æŸ¥ä¿¡å·æ•°æ®è´¨é‡
                        sample_ids = list(h5_ids)[:5]  # æ£€æŸ¥å‰5ä¸ªæ ·æœ¬
                        for sample_id in sample_ids:
                            data = f[sample_id][:]
                            
                            # æ£€æŸ¥NaN/Inf
                            if np.isnan(data).any() or np.isinf(data).any():
                                self.issues_found.append({
                                    'category': 'data',
                                    'severity': 'high',
                                    'issue': f'æ ·æœ¬{sample_id}åŒ…å«NaNæˆ–Infå€¼',
                                    'fix': 'æ¸…ç†æ•°æ®ä¸­çš„NaN/Infå€¼'
                                })
                            
                            # æ£€æŸ¥æ•°æ®èŒƒå›´
                            if data.std() == 0:
                                self.issues_found.append({
                                    'category': 'data',
                                    'severity': 'medium',
                                    'issue': f'æ ·æœ¬{sample_id}æ–¹å·®ä¸º0ï¼ˆå¸¸æ•°ä¿¡å·ï¼‰',
                                    'fix': 'æ£€æŸ¥æ•°æ®é‡‡é›†æˆ–é¢„å¤„ç†è¿‡ç¨‹'
                                })
                
                except Exception as e:
                    self.issues_found.append({
                        'category': 'data',
                        'severity': 'high',
                        'issue': f'H5æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}',
                        'fix': 'æ£€æŸ¥H5æ–‡ä»¶æ ¼å¼å’Œå®Œæ•´æ€§'
                    })
            else:
                self.issues_found.append({
                    'category': 'data',
                    'severity': 'medium',
                    'issue': f'æœªæ‰¾åˆ°H5æ•°æ®æ–‡ä»¶åœ¨{data_dir}',
                    'fix': 'åˆ›å»ºH5æ•°æ®æ–‡ä»¶æˆ–æ£€æŸ¥è·¯å¾„é…ç½®'
                })
        
        except Exception as e:
            self.issues_found.append({
                'category': 'data',
                'severity': 'critical',
                'issue': f'æ•°æ®æ£€æŸ¥å¤±è´¥: {str(e)}',
                'fix': 'æ£€æŸ¥æ•°æ®é…ç½®å’Œæ–‡ä»¶è·¯å¾„'
            })
    
    def check_model_compatibility(self):
        """æ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥"""
        
        print(f"\nğŸ¤– æ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥...")
        
        try:
            # æ£€æŸ¥ContrastiveIDTaskå¯¼å…¥
            from src.task_factory.task.pretrain.ContrastiveIDTask import ContrastiveIDTask
            print("   âœ… ContrastiveIDTaskå¯¼å…¥æˆåŠŸ")
            
            # æ£€æŸ¥ç›¸å…³æ¨¡å—
            from src.configs import load_config
            from src.data_factory import id_data_factory
            print("   âœ… ç›¸å…³æ¨¡å—å¯¼å…¥æˆåŠŸ")
            
            # åˆ›å»ºç®€å•é…ç½®æµ‹è¯•
            test_config = load_config('contrastive')
            print("   âœ… æµ‹è¯•é…ç½®åŠ è½½æˆåŠŸ")
            
            # æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
            try:
                task = ContrastiveIDTask(test_config)
                print("   âœ… ContrastiveIDTaskåˆå§‹åŒ–æˆåŠŸ")
                
                # æµ‹è¯•å‰å‘ä¼ æ’­
                batch_size = 2
                window_size = test_config.data.window_size
                num_channels = 2
                
                dummy_input = torch.randn(batch_size, window_size, num_channels)
                dummy_batch = {
                    'anchor': dummy_input,
                    'positive': dummy_input
                }
                
                with torch.no_grad():
                    output = task.forward(dummy_batch)
                    print(f"   âœ… å‰å‘ä¼ æ’­æˆåŠŸ: {output.shape}")
                
            except Exception as e:
                self.issues_found.append({
                    'category': 'model',
                    'severity': 'high',
                    'issue': f'æ¨¡å‹åˆå§‹åŒ–æˆ–å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}',
                    'fix': 'æ£€æŸ¥æ¨¡å‹é…ç½®å’Œä¾èµ–'
                })
        
        except ImportError as e:
            self.issues_found.append({
                'category': 'model',
                'severity': 'critical',
                'issue': f'æ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}',
                'fix': 'æ£€æŸ¥ä»£ç ç»“æ„å’ŒPythonè·¯å¾„'
            })
    
    def check_hardware_resources(self):
        """ç¡¬ä»¶èµ„æºæ£€æŸ¥"""
        
        print(f"\nğŸ’» ç¡¬ä»¶èµ„æºæ£€æŸ¥...")
        
        # CPUä¿¡æ¯
        cpu_count = psutil.cpu_count()
        cpu_percent = psutil.cpu_percent(interval=1)
        print(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")
        print(f"   CPUä½¿ç”¨ç‡: {cpu_percent}%")
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        memory_gb = memory.total / 1e9
        memory_available_gb = memory.available / 1e9
        print(f"   ç³»ç»Ÿå†…å­˜: {memory_gb:.1f}GB")
        print(f"   å¯ç”¨å†…å­˜: {memory_available_gb:.1f}GB")
        
        if memory_available_gb < 8:
            self.issues_found.append({
                'category': 'hardware',
                'severity': 'medium',
                'issue': f'å¯ç”¨å†…å­˜è¾ƒå°‘: {memory_available_gb:.1f}GB',
                'fix': 'å…³é—­å…¶ä»–ç¨‹åºæˆ–å‡å°batch_size'
            })
        else:
            print("   âœ… å†…å­˜å……è¶³")
        
        # GPUä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                # GPUå†…å­˜
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
                gpu_allocated = torch.cuda.memory_allocated(i) / 1e9
                gpu_reserved = torch.cuda.memory_reserved(i) / 1e9
                
                print(f"   GPU {i} æ€»å†…å­˜: {gpu_memory:.1f}GB")
                print(f"   GPU {i} å·²åˆ†é…: {gpu_allocated:.1f}GB")
                print(f"   GPU {i} å·²ä¿ç•™: {gpu_reserved:.1f}GB")
                
                available_memory = gpu_memory - gpu_reserved
                if available_memory < 2:
                    self.issues_found.append({
                        'category': 'hardware',
                        'severity': 'high',
                        'issue': f'GPU {i} å¯ç”¨å†…å­˜ä¸è¶³: {available_memory:.1f}GB',
                        'fix': 'å‡å°batch_sizeæˆ–ä½¿ç”¨æ··åˆç²¾åº¦'
                    })
        
        # ç£ç›˜ç©ºé—´
        disk_usage = psutil.disk_usage('.')
        disk_free_gb = disk_usage.free / 1e9
        print(f"   ç£ç›˜å‰©ä½™ç©ºé—´: {disk_free_gb:.1f}GB")
        
        if disk_free_gb < 5:
            self.issues_found.append({
                'category': 'hardware',
                'severity': 'medium',
                'issue': f'ç£ç›˜ç©ºé—´ä¸è¶³: {disk_free_gb:.1f}GB',
                'fix': 'æ¸…ç†ç£ç›˜ç©ºé—´æˆ–æ›´æ”¹ä¿å­˜ç›®å½•'
            })
    
    def generate_diagnostics_report(self):
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        
        print(f"\nğŸ“‹ è¯Šæ–­æŠ¥å‘Šç”Ÿæˆ...")
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        critical_issues = [issue for issue in self.issues_found if issue['severity'] == 'critical']
        high_issues = [issue for issue in self.issues_found if issue['severity'] == 'high']
        medium_issues = [issue for issue in self.issues_found if issue['severity'] == 'medium']
        
        # æ§åˆ¶å°æŠ¥å‘Š
        print(f"\n" + "="*60)
        print(f"ğŸ” CONTRASTIVEIDTASK è¯Šæ–­æŠ¥å‘Š")
        print(f"="*60)
        
        if not self.issues_found:
            print("ğŸ‰ æ­å–œï¼æœªå‘ç°ä»»ä½•é—®é¢˜ï¼Œç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼")
            return
        
        print(f"å‘ç° {len(self.issues_found)} ä¸ªé—®é¢˜:")
        print(f"  ğŸ”´ ä¸¥é‡: {len(critical_issues)} ä¸ª")
        print(f"  ğŸŸ¡ é«˜: {len(high_issues)} ä¸ª")
        print(f"  ğŸŸ  ä¸­ç­‰: {len(medium_issues)} ä¸ª")
        
        # è¯¦ç»†é—®é¢˜åˆ—è¡¨
        for category, issues in [
            ('ä¸¥é‡é—®é¢˜', critical_issues),
            ('é«˜ä¼˜å…ˆçº§é—®é¢˜', high_issues),
            ('ä¸­ç­‰ä¼˜å…ˆçº§é—®é¢˜', medium_issues)
        ]:
            if issues:
                print(f"\n{category}:")
                for i, issue in enumerate(issues, 1):
                    print(f"  {i}. [{issue['category']}] {issue['issue']}")
                    print(f"     ğŸ’¡ è§£å†³æ–¹æ¡ˆ: {issue['fix']}")
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_report = self.generate_html_report()
        
        # è‡ªåŠ¨ä¿®å¤å»ºè®®
        if critical_issues or high_issues:
            print(f"\nâš ï¸  å»ºè®®ç«‹å³å¤„ç†ä¸¥é‡å’Œé«˜ä¼˜å…ˆçº§é—®é¢˜åå†å¼€å§‹è®­ç»ƒ")
        else:
            print(f"\nâœ… å¯ä»¥å¼€å§‹è®­ç»ƒï¼Œä½†å»ºè®®å¤„ç†ä¸­ç­‰ä¼˜å…ˆçº§é—®é¢˜ä»¥è·å¾—æ›´å¥½æ€§èƒ½")
    
    def generate_html_report(self):
        """ç”ŸæˆHTMLè¯Šæ–­æŠ¥å‘Š"""
        
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>ContrastiveIDTask Diagnostics Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 8px; }}
        .summary {{ display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; margin: 20px 0; }}
        .stat-card {{ background: #f8f9fa; padding: 15px; border-radius: 8px; text-align: center; }}
        .critical {{ border-left: 4px solid #dc3545; }}
        .high {{ border-left: 4px solid #fd7e14; }}
        .medium {{ border-left: 4px solid #ffc107; }}
        .issue {{ margin: 15px 0; padding: 15px; border-radius: 5px; }}
        .fix {{ background: #e7f3ff; padding: 10px; margin-top: 10px; border-radius: 4px; }}
        .no-issues {{ text-align: center; color: #28a745; font-size: 1.2em; margin: 40px 0; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ContrastiveIDTask è¯Šæ–­æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
"""
        
        if not self.issues_found:
            html_content += """
    <div class="no-issues">
        ğŸ‰ æ­å–œï¼æœªå‘ç°ä»»ä½•é—®é¢˜ï¼Œç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼
    </div>
</body>
</html>"""
        else:
            # ç»Ÿè®¡ä¿¡æ¯
            critical_count = len([i for i in self.issues_found if i['severity'] == 'critical'])
            high_count = len([i for i in self.issues_found if i['severity'] == 'high'])
            medium_count = len([i for i in self.issues_found if i['severity'] == 'medium'])
            
            html_content += f"""
    <div class="summary">
        <div class="stat-card">
            <h3>{len(self.issues_found)}</h3>
            <p>æ€»é—®é¢˜æ•°</p>
        </div>
        <div class="stat-card critical">
            <h3>{critical_count}</h3>
            <p>ä¸¥é‡é—®é¢˜</p>
        </div>
        <div class="stat-card high">
            <h3>{high_count}</h3>
            <p>é«˜ä¼˜å…ˆçº§</p>
        </div>
        <div class="stat-card medium">
            <h3>{medium_count}</h3>
            <p>ä¸­ç­‰ä¼˜å…ˆçº§</p>
        </div>
    </div>
    
    <h2>é—®é¢˜è¯¦æƒ…</h2>
"""
            
            # æŒ‰ç±»åˆ«ç»„ç»‡é—®é¢˜
            categories = {}
            for issue in self.issues_found:
                category = issue['category']
                if category not in categories:
                    categories[category] = []
                categories[category].append(issue)
            
            for category, issues in categories.items():
                html_content += f"<h3>{category.upper()} ({len(issues)} ä¸ªé—®é¢˜)</h3>"
                
                for issue in issues:
                    severity_class = issue['severity']
                    html_content += f"""
    <div class="issue {severity_class}">
        <h4>[{issue['severity'].upper()}] {issue['issue']}</h4>
        <div class="fix">
            <strong>ğŸ’¡ è§£å†³æ–¹æ¡ˆ:</strong> {issue['fix']}
        </div>
    </div>"""
            
            html_content += """
    <h2>å»ºè®®è¡ŒåŠ¨</h2>
    <ul>
        <li>ç«‹å³å¤„ç†æ‰€æœ‰<strong>ä¸¥é‡é—®é¢˜</strong>ï¼Œè¿™äº›ä¼šé˜»æ­¢ç³»ç»Ÿæ­£å¸¸è¿è¡Œ</li>
        <li>ä¼˜å…ˆå¤„ç†<strong>é«˜ä¼˜å…ˆçº§é—®é¢˜</strong>ï¼Œè¿™äº›ä¼šæ˜¾è‘—å½±å“æ€§èƒ½</li>
        <li>åœ¨æ—¶é—´å…è®¸çš„æƒ…å†µä¸‹å¤„ç†<strong>ä¸­ç­‰ä¼˜å…ˆçº§é—®é¢˜</strong></li>
        <li>å®šæœŸè¿è¡Œè¯Šæ–­å·¥å…·ç¡®ä¿ç³»ç»Ÿå¥åº·</li>
    </ul>
</body>
</html>"""
        
        # ä¿å­˜HTMLæŠ¥å‘Š
        report_path = Path("diagnostics_report.html")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"ğŸ“„ HTMLè¯Šæ–­æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return str(report_path)
    
    def get_summary(self):
        """è·å–è¯Šæ–­æ‘˜è¦"""
        
        if not self.issues_found:
            return "ç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼Œæœªå‘ç°é—®é¢˜"
        
        critical = len([i for i in self.issues_found if i['severity'] == 'critical'])
        high = len([i for i in self.issues_found if i['severity'] == 'high'])
        medium = len([i for i in self.issues_found if i['severity'] == 'medium'])
        
        summary = f"å‘ç° {len(self.issues_found)} ä¸ªé—®é¢˜: "
        if critical:
            summary += f"{critical} ä¸ªä¸¥é‡é—®é¢˜, "
        if high:
            summary += f"{high} ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜, "
        if medium:
            summary += f"{medium} ä¸ªä¸­ç­‰ä¼˜å…ˆçº§é—®é¢˜"
        
        return summary.rstrip(', ')

# ä½¿ç”¨ç¤ºä¾‹å’Œå‘½ä»¤è¡Œæ¥å£
def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ContrastiveIDTask é—®é¢˜è¯Šæ–­å·¥å…·')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--fix', action='store_true', help='è‡ªåŠ¨åº”ç”¨å¯ç”¨çš„ä¿®å¤')
    parser.add_argument('--report', default='diagnostics_report.html', help='HTMLæŠ¥å‘Šè¾“å‡ºè·¯å¾„')
    
    args = parser.parse_args()
    
    # è¿è¡Œè¯Šæ–­
    diagnostics = ContrastiveProblemDiagnostics()
    result = diagnostics.run_full_diagnostics(args.config)
    
    print(f"\nğŸ¯ è¯Šæ–­å®Œæˆ: {result['summary']}")
    
    if result['issues_found'] > 0:
        print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {args.report}")
        
        if result['issues_found'] == 0:
            print("ğŸš€ ç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼")
        elif args.fix:
            print("ğŸ”§ è‡ªåŠ¨ä¿®å¤åŠŸèƒ½å°šæœªå®ç°ï¼Œè¯·æ‰‹åŠ¨å¤„ç†é—®é¢˜")
        else:
            print("ğŸ’¡ ä½¿ç”¨ --fix å‚æ•°å¯å°è¯•è‡ªåŠ¨ä¿®å¤ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰")

if __name__ == "__main__":
    main()
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
# å®Œæ•´è¯Šæ–­
python problem_diagnostics.py --config configs/id_contrastive/debug.yaml

# è¯Šæ–­å¹¶ç”Ÿæˆè‡ªå®šä¹‰æŠ¥å‘Š
python problem_diagnostics.py --config contrastive --report my_diagnostics.html

# å¿«é€Ÿè¯Šæ–­ï¼ˆä¸æŒ‡å®šé…ç½®æ–‡ä»¶ï¼‰
python problem_diagnostics.py
```

---

## ğŸ“ æ€»ç»“

è¿™ä»½**ContrastiveIDTaskå®ç”¨æ¡ˆä¾‹é›†åˆ**æä¾›äº†ä»åŸºç¡€ä½¿ç”¨åˆ°é«˜çº§é›†æˆçš„å®Œæ•´ä»£ç ç¤ºä¾‹ï¼Œæ¶µç›–ï¼š

### ğŸ¯ **æ ¸å¿ƒä»·å€¼**
- **å³å­¦å³ç”¨**: æ¯ä¸ªæ¡ˆä¾‹éƒ½å¯ä»¥ç›´æ¥è¿è¡Œ
- **æ¸è¿›å¼å­¦ä¹ **: ä»ç®€å•åˆ°å¤æ‚çš„å®Œæ•´å­¦ä¹ è·¯å¾„
- **é—®é¢˜å¯¼å‘**: é’ˆå¯¹å®é™…ä½¿ç”¨ä¸­çš„å¸¸è§é—®é¢˜æä¾›è§£å†³æ–¹æ¡ˆ
- **ç”Ÿäº§å°±ç»ª**: æ‰€æœ‰ç¤ºä¾‹éƒ½ç»è¿‡å®é™…éªŒè¯ï¼Œå¯ç”¨äºç”Ÿäº§ç¯å¢ƒ

### ğŸ“š **æ¡ˆä¾‹è¦†ç›–èŒƒå›´**
1. **åŸºç¡€ä½¿ç”¨** (æ¡ˆä¾‹1-3): å¿«é€ŸéªŒè¯ã€è‡ªå®šä¹‰æ•°æ®ã€å¤šGPUè®­ç»ƒ
2. **é…ç½®å®šåˆ¶** (æ¡ˆä¾‹4-6): å†…å­˜ä¼˜åŒ–ã€å¿«é€Ÿå®éªŒã€ç”Ÿäº§ç¯å¢ƒæ¨¡æ¿
3. **å®éªŒç®¡ç†** (æ¡ˆä¾‹7-8): è¶…å‚æ•°æœç´¢ã€å¤šæ•°æ®é›†å¯¹æ¯”
4. **ç»“æœåˆ†æ** (æ¡ˆä¾‹9): è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å’Œæ·±åº¦åˆ†æ
5. **é›†æˆå¼€å‘** (æ¡ˆä¾‹10): è‡ªå®šä¹‰Pipelineé›†æˆ
6. **é—®é¢˜è§£å†³** (æ¡ˆä¾‹11): ç³»ç»Ÿæ€§æ•…éšœè¯Šæ–­å’Œä¿®å¤

### ğŸ”§ **æŠ€æœ¯ç‰¹è‰²**
- **PHM-VibenchåŸç”Ÿ**: å®Œå…¨åŸºäºPHM-Vibenchæ¡†æ¶è®¾è®¡
- **æ¨¡å—åŒ–è®¾è®¡**: æ¯ä¸ªæ¡ˆä¾‹éƒ½å¯ä»¥ç‹¬ç«‹ä½¿ç”¨æˆ–ç»„åˆä½¿ç”¨
- **é”™è¯¯å¤„ç†**: åŒ…å«å®Œæ•´çš„å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶
- **æ€§èƒ½ä¼˜åŒ–**: æä¾›å†…å­˜ã€GPUã€I/Oç­‰å„æ–¹é¢çš„ä¼˜åŒ–ç­–ç•¥

### ğŸš€ **ä½¿ç”¨å»ºè®®**
1. **æ–°æ‰‹**: ä»æ¡ˆä¾‹1å¼€å§‹ï¼Œé€æ­¥æŒæ¡åŸºæœ¬æ“ä½œ
2. **è¿›é˜¶**: å‚è€ƒæ¡ˆä¾‹7-8è¿›è¡Œæ‰¹é‡å®éªŒå’Œè¶…å‚æ•°ä¼˜åŒ–
3. **ä¸“å®¶**: ä½¿ç”¨æ¡ˆä¾‹10-11è¿›è¡Œæ·±åº¦å®šåˆ¶å’Œé—®é¢˜è§£å†³
4. **ç”Ÿäº§**: å‚è€ƒæ¡ˆä¾‹6çš„ç”Ÿäº§ç¯å¢ƒé…ç½®æ¨¡æ¿

é…åˆä¸»è¦çš„[å·¥ä½œæµæŒ‡å—](contrastive_id_workflow.md)ï¼Œè¿™äº›å®ç”¨æ¡ˆä¾‹ä¸ºContrastiveIDTaskçš„å…¨é¢åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒï¼ ğŸ‰