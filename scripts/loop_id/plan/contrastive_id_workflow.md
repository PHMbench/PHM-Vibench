# ContrastiveIDTask åˆ†æ­¥å®æ–½å·¥ä½œæµæŒ‡å—

> ğŸ¯ **åŸºäºPHM-Vibenchæ¡†æ¶çš„å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå®Œæ•´å®æ–½æŒ‡å—**  
> éµå¾ªPHM-Vibenchè®¾è®¡ç†å¿µï¼Œæä¾›ä»æ•°æ®å‡†å¤‡åˆ°ç”Ÿäº§éƒ¨ç½²çš„å…¨æµç¨‹æ“ä½œæŒ‡å—

## ğŸ“‹ ç›®å½•

- [ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5åˆ†é’Ÿä¸Šæ‰‹ï¼‰](#-å¿«é€Ÿå¼€å§‹5åˆ†é’Ÿä¸Šæ‰‹)
- [ğŸ“Š ä¸‰å¤§æ ¸å¿ƒæ¦‚å¿µ](#-ä¸‰å¤§æ ¸å¿ƒæ¦‚å¿µ)
- [âš™ï¸ åˆ†æ­¥å®æ–½å·¥ä½œæµ](#ï¸-åˆ†æ­¥å®æ–½å·¥ä½œæµ)
- [ğŸ”§ é«˜çº§ç”¨æ³•](#-é«˜çº§ç”¨æ³•)
- [ğŸ› é—®é¢˜è¯Šæ–­æµç¨‹](#-é—®é¢˜è¯Šæ–­æµç¨‹)
- [ğŸ“Š æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•](#-æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•)
- [ğŸ¯ å¿«é€Ÿå‘½ä»¤å‚è€ƒ](#-å¿«é€Ÿå‘½ä»¤å‚è€ƒ)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5åˆ†é’Ÿä¸Šæ‰‹ï¼‰

### é€‚ç”¨å¯¹è±¡
- **PHMåŸºç¡€æ¨¡å‹å¼€å‘è€…**: æƒ³è¦ä½¿ç”¨å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ
- **æŒ¯åŠ¨ä¿¡å·ç ”ç©¶è€…**: éœ€è¦æ— ç›‘ç£ç‰¹å¾å­¦ä¹ 
- **å·¥ç¨‹å¸ˆ**: å¸Œæœ›æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒéªŒè¯

```bash
# 1. ç¡®è®¤PHM-Vibenchç¯å¢ƒ
python -c "import torch; print(f'âœ… PyTorch {torch.__version__}')"
python -c "from src.configs import load_config; print('âœ… é…ç½®ç³»ç»Ÿå°±ç»ª')"
python -c "from src.task_factory.task.pretrain.ContrastiveIDTask import ContrastiveIDTask; print('âœ… ContrastiveIDTaskå°±ç»ª')"
```

### ç¬¬äºŒæ­¥ï¼šè¿è¡Œæœ€å°ç¤ºä¾‹

```bash
# 2. ä½¿ç”¨é¢„è®¾é…ç½®å¿«é€ŸéªŒè¯ï¼ˆ1 epochï¼ŒCPUæ¨¡å¼ï¼‰
python main.py --pipeline Pipeline_ID --config configs/id_contrastive/debug.yaml
```

æœŸå¾…è¾“å‡ºï¼š
```
âœ… ContrastiveIDTaskåˆå§‹åŒ–æˆåŠŸ
ğŸ”„ å¼€å§‹è®­ç»ƒ (1 epoch)...
ğŸ“ˆ Epoch 1: loss=2.45, contrastive_acc=0.25
âœ… è®­ç»ƒå®Œæˆ! ç»“æœä¿å­˜åœ¨: save/metadata_6_1/ContrastiveIDTask/*/
```

### ç¬¬ä¸‰æ­¥ï¼šéªŒè¯ç»“æœ

```bash
# 3. æŸ¥çœ‹è®­ç»ƒç»“æœ
ls -la save/*/ContrastiveIDTask/*/
# åº”è¯¥çœ‹åˆ°: checkpoints/, metrics.json, log.txt, figures/
```

ğŸ‰ **æ­å–œï¼ä½ å·²æˆåŠŸè¿è¡ŒContrastiveIDTask**

---

## ğŸ“Š ä¸‰å¤§æ ¸å¿ƒæ¦‚å¿µ

### 1. Pipeline_ID å·¥ä½œæµä½“ç³»

ContrastiveIDTaskå®Œå…¨é›†æˆåœ¨PHM-Vibenchçš„Pipeline_IDä¸­ï¼š

```python
# Pipeline_ID è°ƒç”¨é“¾
main.py --pipeline Pipeline_ID 
    â†“
src/Pipeline_ID.py (å§”æ‰˜ç»™é»˜è®¤pipeline)
    â†“  
src/Pipeline_01_default.py
    â†“
Factoryæ¨¡å¼ç»„ä»¶è‡ªåŠ¨åŠ è½½:
â”œâ”€â”€ data_factory: id_data_factory + ID_dataset
â”œâ”€â”€ model_factory: ISFM + PatchTST backbone  
â”œâ”€â”€ task_factory: ContrastiveIDTask
â””â”€â”€ trainer_factory: PyTorch Lightning
```

### 2. é…ç½®é¢„è®¾ç³»ç»Ÿï¼ˆPHM-Vibench v5.0ï¼‰

PHM-Vibenchæä¾›4ä¸ªContrastiveIDTaské¢„è®¾é…ç½®ï¼š

```python
# é…ç½®é¢„è®¾æ˜ å°„
PRESET_TEMPLATES = {
    'contrastive': 'configs/id_contrastive/debug.yaml',          # ğŸ› è°ƒè¯•æ¨¡å¼
    'contrastive_prod': 'configs/id_contrastive/production.yaml', # ğŸš€ ç”Ÿäº§æ¨¡å¼
    'contrastive_ablation': 'configs/id_contrastive/ablation.yaml', # ğŸ§ª æ¶ˆèç ”ç©¶
    'contrastive_cross': 'configs/id_contrastive/cross_dataset.yaml' # ğŸŒ è·¨åŸŸæ³›åŒ–
}

# ç»Ÿä¸€åŠ è½½æ–¹å¼
from src.configs import load_config
config = load_config('contrastive')  # è‡ªåŠ¨åŠ è½½debug.yaml
```

**é…ç½®åœºæ™¯çŸ©é˜µ**ï¼š

| é¢„è®¾åç§° | ç”¨é€” | èµ„æºéœ€æ±‚ | æ‰§è¡Œæ—¶é—´ | æœ€ä½³åœºæ™¯ |
|---------|------|----------|----------|----------|
| `contrastive` | ğŸ› å¿«é€ŸéªŒè¯ | CPU, <4GB | 2-5åˆ†é’Ÿ | å¼€å‘è°ƒè¯• |
| `contrastive_prod` | ğŸš€ å®Œæ•´è®­ç»ƒ | GPU, 16GB+ | 2-24å°æ—¶ | æ­£å¼å®éªŒ |
| `contrastive_ablation` | ğŸ§ª å‚æ•°ç ”ç©¶ | GPU, 8GB+ | 1-12å°æ—¶ | è®ºæ–‡å®éªŒ |
| `contrastive_cross` | ğŸŒ è·¨åŸŸæµ‹è¯• | Multi-GPU | 4-48å°æ—¶ | æ³›åŒ–éªŒè¯ |

### 3. Factoryæ¨¡å¼é›†æˆæ¶æ„

ContrastiveIDTaskæ— ç¼é›†æˆPHM-Vibenchçš„å››å¤§å·¥å‚ï¼š

```yaml
# å®Œæ•´çš„Factoryé…ç½®ç¤ºä¾‹
data:
  factory_name: "id"              # â†’ id_data_factory  
  dataset_name: "ID_dataset"      # â†’ IDæ•°æ®å¤„ç†å™¨
  window_size: 1024               # â†’ é•¿ä¿¡å·çª—å£é‡‡æ ·
  
model:
  type: "ISFM"                    # â†’ model_factory/ISFM
  backbone: "B_08_PatchTST"       # â†’ Transformer backbone
  task_head: "H_01_Linear_cla"    # â†’ åˆ†ç±»å¤´ï¼ˆå¯¹æ¯”å­¦ä¹ ä¸­ä¸ä½¿ç”¨ï¼‰
  
task:
  name: "contrastive_id"          # â†’ task_factoryæ³¨å†Œå
  temperature: 0.07               # â†’ InfoNCEæ¸©åº¦å‚æ•°
  projection_dim: 128             # â†’ å¯¹æ¯”å­¦ä¹ æŠ•å½±ç»´åº¦
  
trainer:
  accelerator: "auto"             # â†’ trainer_factory/PyTorch Lightning
  devices: 1                      # â†’ GPUè®¾å¤‡é…ç½®
  precision: "16-mixed"           # â†’ æ··åˆç²¾åº¦è®­ç»ƒ
```

---

## âš™ï¸ åˆ†æ­¥å®æ–½å·¥ä½œæµ

### é˜¶æ®µä¸€ï¼šæ•°æ®å‡†å¤‡ï¼ˆSteps 1-3ï¼‰

#### Step 1: å‡†å¤‡Metadataæ–‡ä»¶

ContrastiveIDTaskä½¿ç”¨PHM-Vibenchçš„ä¸‰æ–‡ä»¶æ•°æ®æ¶æ„ï¼š

```bash
# æ•°æ®æ–‡ä»¶ç»“æ„
data/
â”œâ”€â”€ metadata_contrastive.xlsx    # ğŸ“‹ æ•°æ®ç´¢å¼•ï¼ˆå¿…éœ€ï¼‰
â”œâ”€â”€ contrastive_data.h5         # ğŸ“Š ä¿¡å·æ•°æ®ï¼ˆå¿…éœ€ï¼‰  
â””â”€â”€ corpus_contrastive.xlsx     # ğŸ“ æ–‡æœ¬æè¿°ï¼ˆå¯é€‰ï¼‰
```

**metadataæ–‡ä»¶æ ¼å¼**ï¼š
```excel
Id        | label | dataset | signal_length | sampling_rate | file_path
id_cwru_001 | 0   | CWRU   | 10240        | 12000        | data/cwru/001.mat
id_xjtu_001 | 1   | XJTU   | 20480        | 25600        | data/xjtu/001.mat
id_pu_001   | 2   | PU     | 8192         | 64000        | data/pu/001.mat
```

#### Step 2: ç”ŸæˆH5æ•°æ®æ–‡ä»¶

ä½¿ç”¨PHM-Vibenchçš„æ•°æ®å·¥å‚å·¥å…·ï¼š

```python
# æ–¹å¼1: ä½¿ç”¨data_factoryå·¥å…·ç”Ÿæˆ
from src.data_factory.id_data_factory import generate_h5_from_metadata

# ä»metadataç”ŸæˆH5æ–‡ä»¶
generate_h5_from_metadata(
    metadata_path="data/metadata_contrastive.xlsx",
    output_h5_path="data/contrastive_data.h5",
    signal_column="signal",  # H5ä¸­çš„ä¿¡å·æ•°æ®åˆ—å
    progress_bar=True
)
```

```bash
# æ–¹å¼2: ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·
python scripts/prepare_data.py \
    --metadata data/metadata_contrastive.xlsx \
    --output data/contrastive_data.h5 \
    --format h5
```

#### Step 3: éªŒè¯æ•°æ®å®Œæ•´æ€§

```python
# æ•°æ®éªŒè¯è„šæœ¬
from src.data_factory import id_data_factory

# åŠ è½½æ•°æ®éªŒè¯
try:
    data_dict = id_data_factory.get_data("metadata_contrastive.xlsx")
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(data_dict)} ä¸ªæ ·æœ¬")
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    sample_id = list(data_dict.keys())[0]
    sample_data = data_dict[sample_id]
    print(f"âœ… æ ·æœ¬å½¢çŠ¶: {sample_data.shape}")
    print(f"âœ… æ•°æ®ç±»å‹: {sample_data.dtype}")
    
except Exception as e:
    print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
```

### é˜¶æ®µäºŒï¼šé…ç½®é€‰æ‹©ä¸å®šåˆ¶ï¼ˆSteps 4-6ï¼‰

#### Step 4: é€‰æ‹©åˆé€‚çš„é…ç½®åœºæ™¯

æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©é…ç½®ï¼š

```bash
# ğŸ› å¿«é€ŸéªŒè¯ - 5åˆ†é’Ÿå†…å®Œæˆ
python main.py --pipeline Pipeline_ID --config contrastive

# ğŸš€ ç”Ÿäº§è®­ç»ƒ - å®Œæ•´å®éªŒ
python main.py --pipeline Pipeline_ID --config contrastive_prod  

# ğŸ§ª æ¶ˆèç ”ç©¶ - å‚æ•°å¯¹æ¯”
python main.py --pipeline Pipeline_ID --config contrastive_ablation

# ğŸŒ è·¨åŸŸæ³›åŒ– - å¤šæ•°æ®é›†
python main.py --pipeline Pipeline_ID --config contrastive_cross
```

#### Step 5: é…ç½®å‚æ•°å®šåˆ¶

ä½¿ç”¨PHM-Vibench v5.0é…ç½®ç³»ç»Ÿè¿›è¡Œå‚æ•°è¦†ç›–ï¼š

```python
# æ–¹å¼1: é…ç½®æ–‡ä»¶ + å‘½ä»¤è¡Œè¦†ç›–
python main.py \
    --pipeline Pipeline_ID \
    --config contrastive \
    --data.batch_size 32 \
    --task.temperature 0.1 \
    --trainer.epochs 10
```

```python
# æ–¹å¼2: Python APIé…ç½®è¦†ç›–
from src.configs import load_config

# åŠ è½½åŸºç¡€é…ç½®å¹¶è¦†ç›–å‚æ•°
config = load_config('contrastive', {
    'data.batch_size': 32,
    'data.window_size': 2048,
    'task.temperature': 0.1,
    'task.projection_dim': 256,
    'trainer.epochs': 50
})

# é“¾å¼é…ç½®æ–¹å¼
config = load_config('contrastive').copy().update({
    'model.d_model': 512,
    'trainer.devices': 4,
    'trainer.strategy': 'ddp'
})
```

#### Step 6: é…ç½®éªŒè¯æ£€æŸ¥

```python
# é…ç½®éªŒè¯è„šæœ¬
from src.configs import load_config, validate_config_completeness

config = load_config('contrastive')

# æ£€æŸ¥å¿…éœ€å­—æ®µ
required_fields = [
    'data.factory_name', 'data.dataset_name', 'data.window_size',
    'model.type', 'task.name', 'task.temperature',
    'trainer.epochs', 'trainer.devices'
]

for field in required_fields:
    if not hasattr(config, field.replace('.', '.')):
        print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
    else:
        print(f"âœ… {field}: {getattr(config, field.replace('.', '.'))}")
```

### é˜¶æ®µä¸‰ï¼šè®­ç»ƒæ‰§è¡Œï¼ˆSteps 7-12ï¼‰

#### Step 7: åŸºç¡€è®­ç»ƒå¯åŠ¨

```bash
# æ ‡å‡†è®­ç»ƒå‘½ä»¤
python main.py \
    --pipeline Pipeline_ID \
    --config configs/id_contrastive/debug.yaml \
    --notes "ContrastiveIDé¦–æ¬¡è®­ç»ƒå®éªŒ"
```

è®­ç»ƒè¿‡ç¨‹ç›‘æ§è¦ç‚¹ï¼š
- **InfoNCEæŸå¤±**ï¼šåº”è¯¥ä»çº¦3.0é€æ¸é™åˆ°1.5-2.0
- **å¯¹æ¯”å‡†ç¡®ç‡**ï¼šä»éšæœºæ°´å¹³ï¼ˆ~0.25ï¼‰æå‡åˆ°0.6+
- **å†…å­˜ä½¿ç”¨**ï¼šç¡®ä¿GPUå†…å­˜ä½¿ç”¨<80%

#### Step 8: å®æ—¶ç›‘æ§è®¾ç½®

```python
# å¯åŠ¨TensorBoardç›‘æ§
import subprocess
subprocess.Popen(["tensorboard", "--logdir", "save/"])
print("ğŸ” TensorBoardå·²å¯åŠ¨: http://localhost:6006")

# å®æ—¶æ—¥å¿—ç›‘æ§
tail -f save/*/ContrastiveIDTask/*/log.txt
```

```python
# è‡ªå®šä¹‰ç›‘æ§è„šæœ¬
import time
import json
from pathlib import Path

def monitor_training(save_dir="save/", interval=30):
    """å®æ—¶ç›‘æ§è®­ç»ƒè¿›ç¨‹"""
    while True:
        # æŸ¥æ‰¾æœ€æ–°å®éªŒ
        latest_exp = sorted(Path(save_dir).glob("*/ContrastiveIDTask/*"))[-1]
        metrics_file = latest_exp / "metrics.json"
        
        if metrics_file.exists():
            with open(metrics_file) as f:
                metrics = json.load(f)
                print(f"ğŸ“Š Epoch {metrics.get('epoch', 0)}: "
                      f"Loss={metrics.get('train_loss', 0):.4f}, "
                      f"Acc={metrics.get('contrastive_acc', 0):.4f}")
        
        time.sleep(interval)
```

#### Step 9: è®­ç»ƒä¸­æ–­ä¸æ¢å¤

```bash
# è‡ªåŠ¨æ¢å¤æœ€æ–°checkpoint
python main.py \
    --pipeline Pipeline_ID \
    --config contrastive \
    --resume_from_checkpoint save/latest_run/checkpoints/last.ckpt

# ä»ç‰¹å®šcheckpointæ¢å¤
python main.py \
    --pipeline Pipeline_ID \
    --config contrastive \
    --resume_from_checkpoint save/metadata_6_1/ContrastiveIDTask/20241201_143052/checkpoints/epoch_10.ckpt
```

#### Step 10: åˆ†å¸ƒå¼è®­ç»ƒæ‰©å±•

```bash
# å•æœºå¤šGPUè®­ç»ƒ
python main.py \
    --pipeline Pipeline_ID \
    --config contrastive_prod \
    --trainer.devices 4 \
    --trainer.strategy ddp

# å¤šæœºåˆ†å¸ƒå¼è®­ç»ƒ
python main.py \
    --pipeline Pipeline_ID \
    --config contrastive_prod \
    --trainer.devices 8 \
    --trainer.num_nodes 2 \
    --trainer.strategy ddp
```

#### Step 11: æ··åˆç²¾åº¦ä¼˜åŒ–

```bash
# å¯ç”¨FP16æ··åˆç²¾åº¦ï¼ˆèŠ‚çœ50%å†…å­˜ï¼‰
python main.py \
    --pipeline Pipeline_ID \
    --config contrastive \
    --trainer.precision 16-mixed

# å¯ç”¨BF16ç²¾åº¦ï¼ˆæ›´ç¨³å®šï¼‰
python main.py \
    --pipeline Pipeline_ID \
    --config contrastive \
    --trainer.precision bf16-mixed
```

#### Step 12: ç»“æœåˆ†æä¸ä¿å­˜

```python
# åŠ è½½è®­ç»ƒç»“æœ
import torch
from pathlib import Path

# æŸ¥æ‰¾æœ€æ–°å®éªŒç»“æœ
latest_run = sorted(Path("save").glob("*/ContrastiveIDTask/*"))[-1]
print(f"ğŸ“ æœ€æ–°å®éªŒ: {latest_run}")

# åŠ è½½æœ€ä½³æ¨¡å‹
best_model = torch.load(latest_run / "checkpoints" / "best.ckpt")
print(f"ğŸ† æœ€ä½³æ¨¡å‹ - Epoch: {best_model['epoch']}, "
      f"Loss: {best_model['state_dict']['train_loss']:.4f}")

# è¯»å–å®Œæ•´metrics
import json
with open(latest_run / "metrics.json") as f:
    metrics = json.load(f)
    print(f"ğŸ“Š æœ€ç»ˆæ€§èƒ½:")
    print(f"   è®­ç»ƒæŸå¤±: {metrics['train_loss']:.4f}")
    print(f"   å¯¹æ¯”å‡†ç¡®ç‡: {metrics['contrastive_acc']:.4f}")
    print(f"   è®­ç»ƒæ—¶é•¿: {metrics['training_time']:.2f}s")
```

### é˜¶æ®µå››ï¼šæ‰¹é‡å®éªŒç®¡ç†ï¼ˆSteps 13-16ï¼‰

#### Step 13: å¤šæ•°æ®é›†å®éªŒ

```bash
# ä½¿ç”¨å®éªŒç®¡ç†è„šæœ¬
python scripts/multi_dataset_experiments.py \
    --datasets CWRU,XJTU,PU,MFPT \
    --config contrastive \
    --parallel 2 \
    --output_dir experiments/multi_dataset/

# è·¨æ•°æ®é›†æ³›åŒ–å®éªŒ
python scripts/multi_dataset_experiments.py \
    --source_datasets CWRU,XJTU \
    --target_datasets PU,MFPT \
    --config contrastive_cross \
    --mode cross_domain
```

#### Step 14: å‚æ•°æ¶ˆèç ”ç©¶

```bash
# æ¸©åº¦å‚æ•°æ¶ˆè
python scripts/ablation_studies.py \
    --param temperature \
    --values 0.01,0.05,0.07,0.1,0.2,0.5 \
    --config contrastive_ablation \
    --output_dir ablation/temperature/

# çª—å£å¤§å°æ¶ˆè  
python scripts/ablation_studies.py \
    --param window_size \
    --values 256,512,1024,2048,4096 \
    --config contrastive_ablation \
    --output_dir ablation/window_size/

# å¤šç»´åº¦ç»„åˆæ¶ˆè
python scripts/ablation_studies.py \
    --param_grid '{"temperature": [0.05, 0.07, 0.1], "window_size": [512, 1024, 2048]}' \
    --config contrastive_ablation \
    --output_dir ablation/grid_search/
```

#### Step 15: æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# å®Œæ•´æ€§èƒ½åŸºå‡†
python scripts/run_performance_benchmark.py \
    --config contrastive \
    --tests training,data_processing,model,scalability,hardware \
    --output_format html

# å¿«é€ŸåŸºå‡†ï¼ˆé€‚åˆCI/CDï¼‰
python scripts/run_performance_benchmark.py \
    --config contrastive \
    --quick \
    --output_format json

# ç‰¹å®šç±»å‹æµ‹è¯•
python scripts/run_performance_benchmark.py \
    --config contrastive_prod \
    --test scalability \
    --batch_sizes 16,32,64,128,256
```

#### Step 16: è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ

```python
# ç”Ÿæˆå®éªŒæŠ¥å‘Š
from benchmarks.contrastive_performance_benchmark import ContrastivePerformanceBenchmark

# åˆ›å»ºbenchmarkå®ä¾‹
benchmark = ContrastivePerformanceBenchmark()

# è¿è¡ŒåŸºå‡†æµ‹è¯•
results = benchmark.run_full_benchmark(
    config_path="configs/id_contrastive/production.yaml",
    output_dir="benchmark_results/"
)

# ç”ŸæˆHTMLæŠ¥å‘Š
benchmark.generate_report(
    results=results,
    output_path="reports/contrastive_benchmark.html",
    format="html"
)

print("ğŸ“Š åŸºå‡†æŠ¥å‘Šå·²ç”Ÿæˆ: reports/contrastive_benchmark.html")
```

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰é…ç½®å·¥ä½œæµï¼ˆSteps 17-20ï¼‰

#### Step 17: åˆ›å»ºé¡¹ç›®ä¸“ç”¨é…ç½®

```yaml
# configs/project_contrastive.yaml
# åŸºäºç”Ÿäº§é…ç½®çš„é¡¹ç›®å®šåˆ¶
data:
  factory_name: "id"
  dataset_name: "ID_dataset"
  metadata_file: "metadata_project.xlsx"  # é¡¹ç›®ä¸“ç”¨æ•°æ®
  window_size: 1024
  stride: 512
  num_windows: 4                          # å¢åŠ çª—å£æ•°é‡
  
model:
  type: "ISFM"
  backbone: "B_04_Dlinear"               # ä½¿ç”¨è½»é‡çº§backbone
  d_model: 256
  
task:
  name: "contrastive_id"
  temperature: 0.08                       # é¡¹ç›®ä¼˜åŒ–çš„æ¸©åº¦
  projection_dim: 256                     # æ›´å¤§çš„æŠ•å½±ç»´åº¦
  loss_weight: 1.0
  
trainer:
  epochs: 100                             # æ›´é•¿è®­ç»ƒ
  devices: 2
  strategy: "ddp"
  gradient_clip_val: 1.0                  # æ¢¯åº¦è£å‰ª
  
# é¡¹ç›®ä¸“ç”¨æ—¥å¿—é…ç½®
logging:
  save_top_k: 5                          # ä¿å­˜æ›´å¤šcheckpoint
  monitor: "contrastive_acc"             # ç›‘æ§å¯¹æ¯”å‡†ç¡®ç‡
  mode: "max"
```

#### Step 18: å‚æ•°ç½‘æ ¼æœç´¢è‡ªåŠ¨åŒ–

```python
# é«˜çº§ç½‘æ ¼æœç´¢è„šæœ¬
from src.configs import load_config
import itertools
import subprocess

def grid_search_contrastive():
    """å¯¹æ¯”å­¦ä¹ å‚æ•°ç½‘æ ¼æœç´¢"""
    
    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'temperature': [0.05, 0.07, 0.1, 0.15],
        'window_size': [512, 1024, 2048],
        'projection_dim': [128, 256, 512],
        'batch_size': [16, 32, 64]
    }
    
    # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
    keys = param_grid.keys()
    values = param_grid.values()
    combinations = list(itertools.product(*values))
    
    results = []
    for i, combo in enumerate(combinations):
        # åˆ›å»ºå‚æ•°å­—å…¸
        params = dict(zip(keys, combo))
        
        # æ„å»ºé…ç½®è¦†ç›–
        overrides = {
            f'task.temperature': params['temperature'],
            f'data.window_size': params['window_size'],
            f'task.projection_dim': params['projection_dim'],
            f'data.batch_size': params['batch_size']
        }
        
        # è¿è¡Œå®éªŒ
        cmd = [
            'python', 'main.py',
            '--pipeline', 'Pipeline_ID',
            '--config', 'contrastive',
            '--notes', f'grid_search_exp_{i}'
        ]
        
        # æ·»åŠ å‚æ•°è¦†ç›–
        for key, value in overrides.items():
            cmd.extend([f'--{key}', str(value)])
        
        print(f"ğŸ”„ è¿è¡Œå®éªŒ {i+1}/{len(combinations)}: {params}")
        subprocess.run(cmd)
        
        results.append(params)
    
    return results

# æ‰§è¡Œç½‘æ ¼æœç´¢
if __name__ == "__main__":
    results = grid_search_contrastive()
    print(f"âœ… å®Œæˆ {len(results)} ä¸ªå®éªŒç»„åˆ")
```

#### Step 19: Pipelineé“¾å¼ç»„åˆ

```bash
# é¢„è®­ç»ƒ â†’ å°‘æ ·æœ¬å­¦ä¹ å·¥ä½œæµ
python main.py \
    --pipeline Pipeline_02_pretrain_fewshot \
    --config_path configs/id_contrastive/production.yaml \
    --fs_config_path configs/demo/GFS/GFS_demo.yaml \
    --notes "å¯¹æ¯”é¢„è®­ç»ƒ+å°‘æ ·æœ¬å¾®è°ƒ"

# å¤šä»»åŠ¡é¢„è®­ç»ƒ â†’ å¾®è°ƒå·¥ä½œæµ  
python main.py \
    --pipeline Pipeline_03_multitask_pretrain_finetune \
    --config_path configs/id_contrastive/production.yaml \
    --finetune_tasks classification,regression \
    --notes "å¤šä»»åŠ¡å¯¹æ¯”é¢„è®­ç»ƒ"
```

#### Step 20: æ¨¡å‹å¯¼å‡ºä¸éƒ¨ç½²

```python
# æ¨¡å‹å¯¼å‡ºè„šæœ¬
import torch
import torch.onnx
from src.task_factory.task.pretrain.ContrastiveIDTask import ContrastiveIDTask
from src.configs import load_config

def export_contrastive_model(checkpoint_path, export_format="onnx"):
    """å¯¼å‡ºè®­ç»ƒå¥½çš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹"""
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    config = load_config('contrastive')
    
    # é‡å»ºæ¨¡å‹
    task = ContrastiveIDTask(config)
    task.load_state_dict(checkpoint['state_dict'])
    task.eval()
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    batch_size = 1
    window_size = config.data.window_size
    num_channels = 2
    example_input = torch.randn(batch_size, window_size, num_channels)
    
    if export_format == "onnx":
        # å¯¼å‡ºONNXæ ¼å¼
        torch.onnx.export(
            task.model,
            example_input,
            "contrastive_model.onnx",
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['signal'],
            output_names=['features'],
            dynamic_axes={
                'signal': {0: 'batch_size'},
                'features': {0: 'batch_size'}
            }
        )
        print("âœ… ONNXæ¨¡å‹å·²å¯¼å‡º: contrastive_model.onnx")
        
    elif export_format == "torchscript":
        # å¯¼å‡ºTorchScriptæ ¼å¼
        traced_model = torch.jit.trace(task.model, example_input)
        traced_model.save("contrastive_model.pt")
        print("âœ… TorchScriptæ¨¡å‹å·²å¯¼å‡º: contrastive_model.pt")
        
    elif export_format == "state_dict":
        # å¯¼å‡ºçº¯æƒé‡
        torch.save(task.model.state_dict(), "contrastive_weights.pth")
        print("âœ… æ¨¡å‹æƒé‡å·²å¯¼å‡º: contrastive_weights.pth")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    checkpoint_path = "save/best_experiment/checkpoints/best.ckpt"
    export_contrastive_model(checkpoint_path, "onnx")
```

---

## ğŸ› é—®é¢˜è¯Šæ–­æµç¨‹

### å†…å­˜é—®é¢˜è§£å†³è·¯å¾„

```
ğŸ’¾ GPUå†…å­˜ä¸è¶³?
â”œâ”€ ğŸ”„ å‡å°batch_size (å»ºè®®: 32â†’16â†’8â†’4)
â”‚   â””â”€ ä¿®æ”¹: data.batch_size
â”œâ”€ ğŸ”„ å‡å°window_size (å»ºè®®: 2048â†’1024â†’512)  
â”‚   â””â”€ ä¿®æ”¹: data.window_size
â”œâ”€ ğŸ”„ å¯ç”¨gradient_checkpointing
â”‚   â””â”€ æ·»åŠ : trainer.gradient_checkpointing=True
â”œâ”€ ğŸ”„ ä½¿ç”¨æ··åˆç²¾åº¦
â”‚   â””â”€ ä¿®æ”¹: trainer.precision="16-mixed"
â”œâ”€ ğŸ”„ å‡å°‘num_windows
â”‚   â””â”€ ä¿®æ”¹: data.num_windows (é»˜è®¤2)
â””â”€ ğŸ”„ å…³é—­ä¸å¿…è¦çš„logging
    â””â”€ è®¾ç½®: logging.save_top_k=1
```

**è¯Šæ–­å‘½ä»¤**ï¼š
```bash
# æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨
nvidia-smi

# ç›‘æ§å†…å­˜ä½¿ç”¨è¶‹åŠ¿
watch -n 1 nvidia-smi

# æ£€æŸ¥å…·ä½“å†…å­˜å ç”¨
python -c "
import torch
print(f'GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB')
print(f'å·²åˆ†é…: {torch.cuda.memory_allocated()/1e9:.1f}GB')
print(f'å·²ç¼“å­˜: {torch.cuda.memory_reserved()/1e9:.1f}GB')
"
```

### æ”¶æ•›é—®é¢˜è§£å†³è·¯å¾„

```
ğŸ“‰ æŸå¤±ä¸ä¸‹é™?
â”œâ”€ ğŸŒ¡ï¸ æ£€æŸ¥æ¸©åº¦å‚æ•° (å»ºè®®: 0.05-0.1)
â”‚   â””â”€ task.temperatureå¤ªé«˜â†’éš¾æ”¶æ•›, å¤ªä½â†’æ¢¯åº¦æ¶ˆå¤±
â”œâ”€ ğŸ“ˆ è°ƒæ•´å­¦ä¹ ç‡
â”‚   â”œâ”€ å¤ªé«˜: 1e-3 â†’ 1e-4 â†’ 1e-5
â”‚   â””â”€ å¤ªä½: 1e-5 â†’ 1e-4 â†’ 1e-3
â”œâ”€ ğŸ¯ å¢åŠ projection_dim (å»ºè®®: 128â†’256â†’512)
â”‚   â””â”€ æ›´å¤§çš„æŠ•å½±ç©ºé—´æœ‰åŠ©äºç‰¹å¾åˆ†ç¦»
â”œâ”€ ğŸ”„ æ£€æŸ¥æ•°æ®è´¨é‡
â”‚   â”œâ”€ éªŒè¯metadataå®Œæ•´æ€§
â”‚   â”œâ”€ æ£€æŸ¥H5æ•°æ®æ ¼å¼
â”‚   â””â”€ ç¡®è®¤çª—å£é‡‡æ ·ç­–ç•¥
â””â”€ â° å»¶é•¿è®­ç»ƒæ—¶é—´
    â””â”€ å¯¹æ¯”å­¦ä¹ é€šå¸¸éœ€è¦æ›´å¤šepochæ‰èƒ½æ”¶æ•›
```

**è¯Šæ–­è„šæœ¬**ï¼š
```python
# æ”¶æ•›è¯Šæ–­å·¥å…·
def diagnose_convergence(log_file):
    """åˆ†æè®­ç»ƒæ—¥å¿—è¯Šæ–­æ”¶æ•›é—®é¢˜"""
    
    import re
    losses = []
    accuracies = []
    
    with open(log_file) as f:
        for line in f:
            # æå–æŸå¤±å€¼
            loss_match = re.search(r'train_loss=([0-9.]+)', line)
            if loss_match:
                losses.append(float(loss_match.group(1)))
            
            # æå–å‡†ç¡®ç‡
            acc_match = re.search(r'contrastive_acc=([0-9.]+)', line)
            if acc_match:
                accuracies.append(float(acc_match.group(1)))
    
    # è¯Šæ–­åˆ†æ
    if len(losses) > 10:
        recent_loss_trend = losses[-5:] 
        early_loss_trend = losses[:5]
        
        print("ğŸ” æ”¶æ•›è¯Šæ–­æŠ¥å‘Š:")
        print(f"   åˆæœŸæŸå¤±: {early_loss_trend[0]:.4f}")
        print(f"   æœ€æ–°æŸå¤±: {recent_loss_trend[-1]:.4f}")
        print(f"   æŸå¤±ä¸‹é™: {early_loss_trend[0] - recent_loss_trend[-1]:.4f}")
        
        if recent_loss_trend[-1] > 2.5:
            print("âš ï¸  æŸå¤±åé«˜ï¼Œå»ºè®®:")
            print("   - é™ä½æ¸©åº¦å‚æ•°è‡³0.05-0.07")
            print("   - å¢åŠ projection_dimè‡³256+")
            print("   - æ£€æŸ¥å­¦ä¹ ç‡è®¾ç½®")
        
        if max(accuracies) < 0.4:
            print("âš ï¸  å‡†ç¡®ç‡åä½ï¼Œå»ºè®®:")
            print("   - å¢åŠ çª—å£æ•°é‡(num_windows)")
            print("   - è°ƒæ•´çª—å£å¤§å°å’Œstride")
            print("   - éªŒè¯æ•°æ®é¢„å¤„ç†")

# ä½¿ç”¨ç¤ºä¾‹
diagnose_convergence("save/latest_run/log.txt")
```

### æ•°æ®åŠ è½½é—®é¢˜è§£å†³è·¯å¾„

```
ğŸ“Š æ•°æ®åŠ è½½é”™è¯¯?
â”œâ”€ ğŸ“‹ æ£€æŸ¥metadataæ ¼å¼
â”‚   â”œâ”€ å¿…éœ€åˆ—: Id, label, dataset
â”‚   â”œâ”€ æ•°æ®ç±»å‹: Id(str), label(int), dataset(str)
â”‚   â””â”€ è·¯å¾„æ£€æŸ¥: file_pathåˆ—æ˜¯å¦å­˜åœ¨
â”œâ”€ ğŸ“¦ éªŒè¯H5æ–‡ä»¶
â”‚   â”œâ”€ æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§: h5py.is_hdf5()
â”‚   â”œâ”€ éªŒè¯æ•°æ®ç»“æ„: æ¯ä¸ªIdå¯¹åº”çš„æ•°æ®å½¢çŠ¶
â”‚   â””â”€ å†…å­˜æ˜ å°„: ç¡®ä¿H5æ–‡ä»¶æœªæŸå
â”œâ”€ ğŸ”§ é…ç½®æ£€æŸ¥
â”‚   â”œâ”€ factory_name: å¿…é¡»æ˜¯"id"
â”‚   â”œâ”€ dataset_name: å¿…é¡»æ˜¯"ID_dataset"  
â”‚   â””â”€ data_dir: æŒ‡å‘æ­£ç¡®çš„æ•°æ®ç›®å½•
â””â”€ ğŸ”„ æƒé™æ£€æŸ¥
    â”œâ”€ æ–‡ä»¶è¯»å–æƒé™
    â””â”€ ç›®å½•è®¿é—®æƒé™
```

**æ•°æ®éªŒè¯è„šæœ¬**ï¼š
```python
# æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
def validate_data_setup(metadata_path, h5_path=None):
    """éªŒè¯æ•°æ®è®¾ç½®çš„å®Œæ•´æ€§"""
    
    import pandas as pd
    import h5py
    from pathlib import Path
    
    print("ğŸ” æ•°æ®éªŒè¯å¼€å§‹...")
    
    # 1. æ£€æŸ¥metadataæ–‡ä»¶
    if not Path(metadata_path).exists():
        print(f"âŒ Metadataæ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}")
        return False
        
    try:
        df = pd.read_excel(metadata_path)
        print(f"âœ… MetadataåŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_cols = ['Id', 'label', 'dataset']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"âŒ ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
            return False
        print(f"âœ… å¿…éœ€åˆ—æ£€æŸ¥é€šè¿‡: {required_cols}")
        
    except Exception as e:
        print(f"âŒ Metadataè¯»å–å¤±è´¥: {e}")
        return False
    
    # 2. æ£€æŸ¥H5æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
    if h5_path and Path(h5_path).exists():
        try:
            with h5py.File(h5_path, 'r') as f:
                h5_ids = set(f.keys())
                metadata_ids = set(df['Id'].astype(str))
                
                missing_in_h5 = metadata_ids - h5_ids
                missing_in_metadata = h5_ids - metadata_ids
                
                if missing_in_h5:
                    print(f"âš ï¸  H5æ–‡ä»¶ä¸­ç¼ºå°‘ {len(missing_in_h5)} ä¸ªID")
                if missing_in_metadata:
                    print(f"âš ï¸  Metadataä¸­ç¼ºå°‘ {len(missing_in_metadata)} ä¸ªID")
                
                print(f"âœ… H5æ•°æ®æ£€æŸ¥å®Œæˆ: {len(h5_ids)} ä¸ªæ ·æœ¬")
                
        except Exception as e:
            print(f"âŒ H5æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
            return False
    
    print("âœ… æ•°æ®éªŒè¯é€šè¿‡!")
    return True

# ä½¿ç”¨ç¤ºä¾‹  
validate_data_setup("data/metadata_contrastive.xlsx", "data/contrastive_data.h5")
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

### ğŸš€ è®­ç»ƒæ€§èƒ½ä¼˜åŒ–

- [ ] **æ‰¹å¤„ç†å¤§å°ä¼˜åŒ–**
  ```python
  # æ‰¾åˆ°æœ€ä¼˜batch_size
  def find_optimal_batch_size():
      for batch_size in [16, 32, 64, 128, 256]:
          try:
              # æµ‹è¯•GPUå†…å­˜ä½¿ç”¨
              config = load_config('contrastive', {'data.batch_size': batch_size})
              print(f"æ‰¹å¤„ç† {batch_size}: å†…å­˜ä½¿ç”¨ {get_gpu_memory():.1f}GB")
          except RuntimeError as e:
              print(f"æ‰¹å¤„ç† {batch_size}: å†…å­˜æº¢å‡º")
              break
  ```

- [ ] **æ•°æ®åŠ è½½å¹¶è¡ŒåŒ–**
  ```yaml
  # é…ç½®æ–‡ä»¶ä¼˜åŒ–
  data:
    num_workers: 8        # CPUæ ¸å¿ƒæ•°
    pin_memory: true      # å›ºå®šå†…å­˜
    persistent_workers: true  # æŒä¹…åŒ–worker
  ```

- [ ] **æ··åˆç²¾åº¦è®­ç»ƒ**
  ```yaml
  trainer:
    precision: "16-mixed"  # èŠ‚çœ50%å†…å­˜ï¼ŒåŠ é€Ÿ2x
    # æˆ–è€… precision: "bf16-mixed"  # æ›´ç¨³å®š
  ```

- [ ] **ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰**
  ```python
  # åœ¨taskåˆå§‹åŒ–åæ·»åŠ 
  self.model = torch.compile(self.model, mode="reduce-overhead")
  ```

- [ ] **é«˜æ•ˆçš„æ•°æ®é‡‡æ ·**
  ```yaml
  data:
    window_sampling_strategy: "evenly_spaced"  # æ¯”randomæ›´é«˜æ•ˆ
    stride: 512  # åˆç†çš„strideè®¾ç½®
  ```

### ğŸ’¾ å†…å­˜ä¼˜åŒ–ç­–ç•¥

- [ ] **æ¢¯åº¦æ£€æŸ¥ç‚¹**
  ```yaml
  trainer:
    gradient_checkpointing: true  # ç”¨æ—¶é—´æ¢å†…å­˜
  ```

- [ ] **å°æ‰¹é‡ç´¯ç§¯**
  ```yaml
  trainer:
    accumulate_grad_batches: 4  # æ¨¡æ‹Ÿå¤§batch_size
  ```

- [ ] **å®šæœŸæ¸…ç†ç¼“å­˜**
  ```python
  # åœ¨è®­ç»ƒå¾ªç¯ä¸­å®šæœŸè°ƒç”¨
  if step % 100 == 0:
      torch.cuda.empty_cache()
  ```

### ğŸ“ˆ I/Oæ€§èƒ½ä¼˜åŒ–

- [ ] **SSDå­˜å‚¨**ï¼šå°†æ•°æ®é›†æ”¾åœ¨SSDä¸Š
- [ ] **å†…å­˜æ˜ å°„**ï¼šä½¿ç”¨H5æ–‡ä»¶çš„å†…å­˜æ˜ å°„
- [ ] **é¢„åŠ è½½æ•°æ®**ï¼šå¯¹äºå°æ•°æ®é›†ï¼Œé¢„åŠ è½½åˆ°å†…å­˜
- [ ] **å¼‚æ­¥I/O**ï¼šä½¿ç”¨å¼‚æ­¥æ•°æ®åŠ è½½

### ğŸ”„ åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–

- [ ] **é€‰æ‹©åˆé€‚çš„ç­–ç•¥**
  ```yaml
  trainer:
    strategy: "ddp"          # å•æœºå¤šGPU
    # strategy: "deepspeed"   # å¤§æ¨¡å‹ä¼˜åŒ–
    # strategy: "fsdp"        # å…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ
  ```

- [ ] **ç½‘ç»œä¼˜åŒ–**
  ```yaml
  trainer:
    sync_batchnorm: true     # åŒæ­¥BatchNorm
    find_unused_parameters: false  # æå‡æ€§èƒ½
  ```

---

## ğŸ¯ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

### ğŸ“‹ å¿…å¤‡å‘½ä»¤

```bash
# ğŸš€ æ ¸å¿ƒè®­ç»ƒå‘½ä»¤
python main.py --pipeline Pipeline_ID --config contrastive              # å¿«é€ŸéªŒè¯
python main.py --pipeline Pipeline_ID --config contrastive_prod         # ç”Ÿäº§è®­ç»ƒ
python main.py --pipeline Pipeline_ID --config contrastive_ablation     # æ¶ˆèç ”ç©¶
python main.py --pipeline Pipeline_ID --config contrastive_cross        # è·¨åŸŸæ³›åŒ–

# ğŸ§ª å®éªŒç®¡ç†å‘½ä»¤
python scripts/multi_dataset_experiments.py --quick                     # æ‰¹é‡å®éªŒ
python scripts/ablation_studies.py --config contrastive_ablation       # å‚æ•°æ¶ˆè
python scripts/run_performance_benchmark.py --test all                  # æ€§èƒ½æµ‹è¯•

# ğŸ“Š ç›‘æ§å‘½ä»¤
tensorboard --logdir save/                                             # å¯è§†åŒ–ç›‘æ§
tail -f save/*/ContrastiveIDTask/*/log.txt                             # å®æ—¶æ—¥å¿—
nvidia-smi -l 1                                                        # GPUç›‘æ§

# ğŸ”§ å·¥å…·å‘½ä»¤
python -c "from src.configs import load_config; print('é…ç½®ç³»ç»Ÿå°±ç»ª')"     # ç¯å¢ƒéªŒè¯
python scripts/prepare_data.py --metadata data.xlsx --output data.h5   # æ•°æ®å‡†å¤‡
```

### âš™ï¸ é«˜çº§é…ç½®å‘½ä»¤

```bash
# ğŸ“ å‚æ•°è¦†ç›–
python main.py --pipeline Pipeline_ID --config contrastive \
  --data.batch_size 32 \
  --task.temperature 0.1 \
  --trainer.epochs 50 \
  --notes "å‚æ•°è°ƒä¼˜å®éªŒ"

# ğŸ¯ ç‰¹å®šGPUè®­ç»ƒ
CUDA_VISIBLE_DEVICES=0,1 python main.py \
  --pipeline Pipeline_ID \
  --config contrastive_prod \
  --trainer.devices 2

# ğŸ’¾ å†…å­˜ä¼˜åŒ–è®­ç»ƒ
python main.py --pipeline Pipeline_ID --config contrastive \
  --trainer.precision "16-mixed" \
  --trainer.gradient_checkpointing true \
  --data.batch_size 16

# ğŸ”„ æ¢å¤è®­ç»ƒ
python main.py --pipeline Pipeline_ID --config contrastive \
  --resume_from_checkpoint save/*/checkpoints/last.ckpt
```

### ğŸ“Š çŠ¶æ€æŸ¥è¯¢å‘½ä»¤

```bash
# æŸ¥çœ‹æœ€æ–°å®éªŒ
ls -t save/*/ContrastiveIDTask/* | head -5

# æ£€æŸ¥å®éªŒçŠ¶æ€
find save/ -name "metrics.json" -exec tail -1 {} \; -print

# æ¸…ç†æ—§å®éªŒï¼ˆä¿ç•™æœ€æ–°10ä¸ªï¼‰
find save/ -name "ContrastiveIDTask" -type d | \
  head -n -10 | xargs rm -rf

# GPUä½¿ç”¨æƒ…å†µ
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# ç£ç›˜ç©ºé—´æ£€æŸ¥
du -sh save/ && df -h
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£é“¾æ¥

- [ğŸ“– ContrastiveIDTaskæŠ€æœ¯æ–‡æ¡£](../docs/contrastive_pretrain_guide.md) - å®Œæ•´çš„APIå‚è€ƒå’ŒæŠ€æœ¯ç»†èŠ‚
- [âš™ï¸ PHM-Vibenché…ç½®ç³»ç»Ÿ](../src/configs/CLAUDE.md) - é…ç½®ç³»ç»Ÿä½¿ç”¨æŒ‡å—
- [ğŸ­ ä»»åŠ¡å·¥å‚æ–‡æ¡£](../src/task_factory/CLAUDE.md) - ä»»åŠ¡å¼€å‘å’Œæ‰©å±•æŒ‡å—
- [ğŸ“Š æ€§èƒ½åŸºå‡†æŠ¥å‘Š](../benchmarks/README_performance_benchmark.md) - æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–æŒ‡å—

---

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼ŒæŒ‰ä»¥ä¸‹é¡ºåºæ’æŸ¥ï¼š

1. **æ£€æŸ¥æœ¬æŒ‡å—**çš„é—®é¢˜è¯Šæ–­æµç¨‹
2. **æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶**: `save/*/ContrastiveIDTask/*/log.txt`
3. **è¿è¡ŒéªŒè¯è„šæœ¬**: ç¡®è®¤ç¯å¢ƒå’Œæ•°æ®è®¾ç½®
4. **æŸ¥é˜…æŠ€æœ¯æ–‡æ¡£**: æ·±å…¥äº†è§£å®ç°ç»†èŠ‚
5. **ç¤¾åŒºæ”¯æŒ**: åœ¨GitHub Issuesä¸­å¯»æ±‚å¸®åŠ©

ğŸ‰ **ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼ContrastiveIDTaskå°†ä¸ºä½ çš„å·¥ä¸šä¿¡å·åˆ†æç ”ç©¶æä¾›å¼ºå¤§çš„å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒèƒ½åŠ›ã€‚**