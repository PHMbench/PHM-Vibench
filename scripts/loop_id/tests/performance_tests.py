#!/usr/bin/env python3
"""
ContrastiveIDTaskæ€§èƒ½æµ‹è¯•å¥—ä»¶
ä¸“ä¸ºç ”ç©¶æµç¨‹è®¾è®¡çš„æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parents[3]))

import torch
import numpy as np
import time
import gc
from argparse import Namespace
import warnings
warnings.filterwarnings("ignore")

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("âš ï¸ psutilæœªå®‰è£…ï¼Œå†…å­˜ç›‘æ§åŠŸèƒ½å—é™")

from src.task_factory.task.pretrain.ContrastiveIDTask import ContrastiveIDTask


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""

    def __init__(self):
        self.results = {}
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ æµ‹è¯•è®¾å¤‡: {self.device}")

    def create_test_task(self, window_size=256, d_model=64):
        """åˆ›å»ºæµ‹è¯•ä»»åŠ¡"""
        args_data = Namespace(
            window_size=window_size,
            stride=window_size // 2,
            num_window=2,
            window_sampling_strategy='random',
            normalization=True,
            dtype='float32'
        )

        args_task = Namespace(
            lr=1e-3,
            temperature=0.07,
            weight_decay=1e-4,
            loss="CE",
            metrics=["acc"]
        )

        args_model = Namespace(
            d_model=d_model,
            name="M_01_ISFM",
            backbone="B_08_PatchTST"
        )

        args_trainer = Namespace(
            epochs=1,
            accelerator=str(self.device),
            gpus=1 if torch.cuda.is_available() else 0
        )

        args_environment = Namespace(
            save_dir="save/"
        )

        # åˆ›å»ºç½‘ç»œ
        network = torch.nn.Sequential(
            torch.nn.Linear(window_size * 2, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, d_model)
        ).to(self.device)

        task = ContrastiveIDTask(
            network=network,
            args_data=args_data,
            args_model=args_model,
            args_task=args_task,
            args_trainer=args_trainer,
            args_environment=args_environment,
            metadata={}
        )

        return task, network

    def create_performance_dataset(self, num_samples, signal_length, num_channels=2):
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•æ•°æ®é›†"""
        dataset = []

        for i in range(num_samples):
            # ç”Ÿæˆé«˜è´¨é‡æµ‹è¯•ä¿¡å·
            t = np.linspace(0, 1, signal_length)
            signal = np.zeros((signal_length, num_channels))

            # å¤šé¢‘ç‡æˆåˆ†
            frequencies = [50, 120, 200, 350]
            amplitudes = [0.8, 0.4, 0.2, 0.1]

            for ch in range(num_channels):
                combined_signal = np.zeros(signal_length)
                for freq, amp in zip(frequencies, amplitudes):
                    phase = np.random.uniform(0, 2*np.pi)
                    combined_signal += amp * np.sin(2 * np.pi * freq * t + phase)

                # æ·»åŠ è°ƒåˆ¶å’Œå™ªå£°
                modulation = 1 + 0.1 * np.sin(2 * np.pi * 10 * t)  # 10Hzè°ƒåˆ¶
                noise = 0.05 * np.random.randn(signal_length)
                signal[:, ch] = combined_signal * modulation + noise

            metadata = {'Label': i % 4, 'ID': f'perf_{i:04d}'}
            dataset.append((f'perf_sample_{i:04d}', signal.astype(np.float32), metadata))

        return dataset

    def test_window_creation_performance(self):
        """æµ‹è¯•çª—å£åˆ›å»ºæ€§èƒ½"""
        print("\n=== æµ‹è¯•çª—å£åˆ›å»ºæ€§èƒ½ ===")

        task, _ = self.create_test_task()

        # ä¸åŒä¿¡å·é•¿åº¦æµ‹è¯•
        signal_lengths = [1000, 5000, 10000, 20000]
        window_creation_results = {}

        for length in signal_lengths:
            print(f"ğŸ“ æµ‹è¯•ä¿¡å·é•¿åº¦: {length}")

            # åˆ›å»ºæµ‹è¯•ä¿¡å·
            signal = np.random.randn(length, 2).astype(np.float32)

            # æµ‹è¯•ä¸åŒçª—å£æ•°é‡
            num_windows = [2, 4, 8, 16]
            length_results = {}

            for num_win in num_windows:
                times = []

                # å¤šæ¬¡æµ‹è¯•å–å¹³å‡
                for _ in range(10):
                    start_time = time.time()
                    windows = task.create_windows(signal, num_window=num_win, strategy='random')
                    end_time = time.time()

                    times.append(end_time - start_time)

                avg_time = np.mean(times)
                length_results[num_win] = {
                    'avg_time': avg_time,
                    'windows_per_sec': num_win / avg_time if avg_time > 0 else 0
                }

                print(f"  {num_win}çª—å£: {avg_time*1000:.2f}ms ({num_win/avg_time:.1f} çª—å£/ç§’)")

            window_creation_results[length] = length_results

        self.results['window_creation'] = window_creation_results
        print("âœ… çª—å£åˆ›å»ºæ€§èƒ½æµ‹è¯•å®Œæˆ")

    def test_batch_processing_performance(self):
        """æµ‹è¯•æ‰¹å¤„ç†æ€§èƒ½"""
        print("\n=== æµ‹è¯•æ‰¹å¤„ç†æ€§èƒ½ ===")

        batch_sizes = [4, 8, 16, 32] if not torch.cuda.is_available() else [4, 8, 16, 32, 64]
        batch_results = {}

        for batch_size in batch_sizes:
            print(f"ğŸ“¦ æµ‹è¯•æ‰¹å¤§å°: {batch_size}")

            task, _ = self.create_test_task()

            # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
            dataset = self.create_performance_dataset(
                num_samples=batch_size * 4,
                signal_length=1024
            )

            # æµ‹è¯•æ‰¹å¤„ç†æ—¶é—´
            batch_times = []

            for i in range(0, len(dataset), batch_size):
                batch_data = dataset[i:i+batch_size]

                start_time = time.time()
                batch = task.prepare_batch(batch_data)
                end_time = time.time()

                if len(batch['ids']) > 0:
                    batch_times.append(end_time - start_time)

            if batch_times:
                avg_batch_time = np.mean(batch_times)
                samples_per_sec = batch_size / avg_batch_time

                batch_results[batch_size] = {
                    'avg_time': avg_batch_time,
                    'samples_per_sec': samples_per_sec
                }

                print(f"  å¹³å‡æ—¶é—´: {avg_batch_time*1000:.2f}ms")
                print(f"  ååé‡: {samples_per_sec:.1f} æ ·æœ¬/ç§’")

        self.results['batch_processing'] = batch_results
        print("âœ… æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•å®Œæˆ")

    def test_infonce_computation_performance(self):
        """æµ‹è¯•InfoNCEè®¡ç®—æ€§èƒ½"""
        print("\n=== æµ‹è¯•InfoNCEè®¡ç®—æ€§èƒ½ ===")

        task, _ = self.create_test_task()

        # ä¸åŒç‰¹å¾ç»´åº¦å’Œæ‰¹å¤§å°
        test_configs = [
            (4, 64), (8, 64), (16, 64), (32, 64),
            (4, 128), (8, 128), (16, 128),
            (4, 256), (8, 256)
        ]

        infonce_results = {}

        for batch_size, feature_dim in test_configs:
            config_key = f"batch_{batch_size}_dim_{feature_dim}"
            print(f"ğŸ§® æµ‹è¯•é…ç½®: Batch={batch_size}, Dim={feature_dim}")

            # åˆ›å»ºç‰¹å¾å¼ é‡
            z_anchor = torch.randn(batch_size, feature_dim).to(self.device)
            z_positive = torch.randn(batch_size, feature_dim).to(self.device)

            # é¢„çƒ­ï¼ˆGPUæƒ…å†µä¸‹ï¼‰
            if self.device.type == 'cuda':
                for _ in range(5):
                    _ = task.infonce_loss(z_anchor, z_positive)
                torch.cuda.synchronize()

            # æ€§èƒ½æµ‹è¯•
            times = []
            num_iterations = 100

            for _ in range(num_iterations):
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()

                start_time = time.time()
                loss = task.infonce_loss(z_anchor, z_positive)

                if self.device.type == 'cuda':
                    torch.cuda.synchronize()

                end_time = time.time()
                times.append(end_time - start_time)

            avg_time = np.mean(times)
            ops_per_sec = 1 / avg_time

            infonce_results[config_key] = {
                'avg_time_ms': avg_time * 1000,
                'ops_per_sec': ops_per_sec,
                'batch_size': batch_size,
                'feature_dim': feature_dim
            }

            print(f"  å¹³å‡æ—¶é—´: {avg_time*1000:.3f}ms")
            print(f"  è®¡ç®—é€Ÿåº¦: {ops_per_sec:.1f} æ¬¡/ç§’")

        self.results['infonce_computation'] = infonce_results
        print("âœ… InfoNCEè®¡ç®—æ€§èƒ½æµ‹è¯•å®Œæˆ")

    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        print("\n=== æµ‹è¯•å†…å­˜ä½¿ç”¨ ===")

        if not PSUTIL_AVAILABLE and not torch.cuda.is_available():
            print("âš ï¸ æ— æ³•ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return

        memory_results = {}

        # ä¸åŒæ•°æ®è§„æ¨¡æµ‹è¯•
        test_scales = [
            (16, 1024, "å°è§„æ¨¡"),
            (64, 2048, "ä¸­è§„æ¨¡"),
            (128, 4096, "å¤§è§„æ¨¡")
        ]

        for num_samples, signal_length, scale_name in test_scales:
            print(f"ğŸ§  æµ‹è¯•{scale_name}: {num_samples}æ ·æœ¬ Ã— {signal_length}é•¿åº¦")

            # è®°å½•åˆå§‹å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                initial_gpu_mem = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            else:
                initial_gpu_mem = 0

            if PSUTIL_AVAILABLE:
                initial_cpu_mem = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            else:
                initial_cpu_mem = 0

            # åˆ›å»ºä»»åŠ¡å’Œæ•°æ®
            task, network = self.create_test_task(window_size=256, d_model=128)
            dataset = self.create_performance_dataset(num_samples, signal_length)

            # å¤„ç†æ•°æ®
            all_batches = []
            for i in range(0, len(dataset), 8):
                batch_data = dataset[i:i+8]
                batch = task.prepare_batch(batch_data)
                if len(batch['ids']) > 0:
                    all_batches.append(batch)

            # è®°å½•å³°å€¼å†…å­˜
            if torch.cuda.is_available():
                peak_gpu_mem = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                gpu_usage = peak_gpu_mem - initial_gpu_mem
            else:
                gpu_usage = 0

            if PSUTIL_AVAILABLE:
                peak_cpu_mem = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                cpu_usage = peak_cpu_mem - initial_cpu_mem
            else:
                cpu_usage = 0

            memory_results[scale_name] = {
                'num_samples': num_samples,
                'signal_length': signal_length,
                'cpu_memory_mb': cpu_usage,
                'gpu_memory_mb': gpu_usage
            }

            print(f"  CPUå†…å­˜ä½¿ç”¨: {cpu_usage:.1f} MB")
            print(f"  GPUå†…å­˜ä½¿ç”¨: {gpu_usage:.1f} MB")

            # æ¸…ç†å†…å­˜
            del task, network, dataset, all_batches
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        self.results['memory_usage'] = memory_results
        print("âœ… å†…å­˜ä½¿ç”¨æµ‹è¯•å®Œæˆ")

    def test_scalability(self):
        """æµ‹è¯•å¯æ‰©å±•æ€§"""
        print("\n=== æµ‹è¯•å¯æ‰©å±•æ€§ ===")

        # æµ‹è¯•ä¸åŒæ•°æ®é›†å¤§å°çš„å¤„ç†æ—¶é—´
        dataset_sizes = [50, 100, 200, 500] if not torch.cuda.is_available() else [50, 100, 200, 500, 1000]
        scalability_results = {}

        task, network = self.create_test_task()

        for size in dataset_sizes:
            print(f"ğŸ“Š æµ‹è¯•æ•°æ®é›†å¤§å°: {size}")

            # åˆ›å»ºæ•°æ®é›†
            dataset = self.create_performance_dataset(size, 1024)

            # æµ‹è¯•å®Œæ•´å¤„ç†æ—¶é—´
            start_time = time.time()

            processed_samples = 0
            for i in range(0, len(dataset), 16):  # å›ºå®šæ‰¹å¤§å°
                batch_data = dataset[i:i+16]
                batch = task.prepare_batch(batch_data)

                if len(batch['ids']) > 0:
                    # æ¨¡æ‹Ÿå‰å‘ä¼ æ’­
                    batch_size, seq_len, channels = batch['anchor'].shape
                    anchor_flat = batch['anchor'].reshape(batch_size, -1).to(self.device)
                    positive_flat = batch['positive'].reshape(batch_size, -1).to(self.device)

                    z_anchor = network(anchor_flat)
                    z_positive = network(positive_flat)

                    loss = task.infonce_loss(z_anchor, z_positive)
                    accuracy = task.compute_accuracy(z_anchor, z_positive)

                    processed_samples += len(batch['ids'])

            end_time = time.time()
            total_time = end_time - start_time

            scalability_results[size] = {
                'total_time': total_time,
                'processed_samples': processed_samples,
                'samples_per_sec': processed_samples / total_time,
                'time_per_sample': total_time / processed_samples if processed_samples > 0 else 0
            }

            print(f"  æ€»æ—¶é—´: {total_time:.2f}s")
            print(f"  å¤„ç†æ ·æœ¬: {processed_samples}")
            print(f"  ååé‡: {processed_samples/total_time:.1f} æ ·æœ¬/ç§’")

        self.results['scalability'] = scalability_results
        print("âœ… å¯æ‰©å±•æ€§æµ‹è¯•å®Œæˆ")

    def test_temperature_sensitivity(self):
        """æµ‹è¯•æ¸©åº¦å‚æ•°æ•æ„Ÿæ€§"""
        print("\n=== æµ‹è¯•æ¸©åº¦å‚æ•°æ€§èƒ½å½±å“ ===")

        temperatures = [0.01, 0.05, 0.07, 0.1, 0.2, 0.5]
        temp_results = {}

        for temp in temperatures:
            print(f"ğŸŒ¡ï¸ æµ‹è¯•æ¸©åº¦: {temp}")

            task, _ = self.create_test_task()
            task.args_task.temperature = temp

            # åˆ›å»ºæµ‹è¯•æ•°æ®
            batch_size = 32
            feature_dim = 64
            z_anchor = torch.randn(batch_size, feature_dim).to(self.device)
            z_positive = torch.randn(batch_size, feature_dim).to(self.device)

            # æ€§èƒ½æµ‹è¯•
            times = []
            for _ in range(50):
                start_time = time.time()
                loss = task.infonce_loss(z_anchor, z_positive)
                end_time = time.time()
                times.append(end_time - start_time)

            avg_time = np.mean(times)

            temp_results[temp] = {
                'avg_time_ms': avg_time * 1000,
                'loss_value': loss.item()
            }

            print(f"  å¹³å‡æ—¶é—´: {avg_time*1000:.3f}ms")
            print(f"  æŸå¤±å€¼: {loss.item():.4f}")

        self.results['temperature_sensitivity'] = temp_results
        print("âœ… æ¸©åº¦å‚æ•°æ€§èƒ½æµ‹è¯•å®Œæˆ")

    def run_all_performance_tests(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ContrastiveIDTaskæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)

        test_methods = [
            ("çª—å£åˆ›å»ºæ€§èƒ½", self.test_window_creation_performance),
            ("æ‰¹å¤„ç†æ€§èƒ½", self.test_batch_processing_performance),
            ("InfoNCEè®¡ç®—æ€§èƒ½", self.test_infonce_computation_performance),
            ("å†…å­˜ä½¿ç”¨", self.test_memory_usage),
            ("å¯æ‰©å±•æ€§", self.test_scalability),
            ("æ¸©åº¦å‚æ•°æ•æ„Ÿæ€§", self.test_temperature_sensitivity)
        ]

        for test_name, test_method in test_methods:
            try:
                print(f"\nğŸ” å¼€å§‹ {test_name} æµ‹è¯•")
                test_method()
            except Exception as e:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥: {e}")
                continue

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        self.generate_performance_report()

    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\nğŸ“Š æ€§èƒ½æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print("=" * 60)

        # æ‰¹å¤„ç†æ€§èƒ½æ€»ç»“
        if 'batch_processing' in self.results:
            print("\nğŸ”¸ æ‰¹å¤„ç†æ€§èƒ½:")
            for batch_size, metrics in self.results['batch_processing'].items():
                print(f"  æ‰¹å¤§å° {batch_size}: {metrics['samples_per_sec']:.1f} æ ·æœ¬/ç§’")

        # InfoNCEè®¡ç®—æ€§èƒ½æ€»ç»“
        if 'infonce_computation' in self.results:
            print("\nğŸ”¸ InfoNCEè®¡ç®—æ€§èƒ½:")
            for config, metrics in self.results['infonce_computation'].items():
                print(f"  {config}: {metrics['avg_time_ms']:.3f}ms")

        # å†…å­˜ä½¿ç”¨æ€»ç»“
        if 'memory_usage' in self.results:
            print("\nğŸ”¸ å†…å­˜ä½¿ç”¨:")
            for scale, metrics in self.results['memory_usage'].items():
                print(f"  {scale}: CPU {metrics['cpu_memory_mb']:.1f}MB, GPU {metrics['gpu_memory_mb']:.1f}MB")

        # å¯æ‰©å±•æ€§æ€»ç»“
        if 'scalability' in self.results:
            print("\nğŸ”¸ å¯æ‰©å±•æ€§:")
            for size, metrics in self.results['scalability'].items():
                print(f"  {size}æ ·æœ¬: {metrics['samples_per_sec']:.1f} æ ·æœ¬/ç§’")

        print("\nâœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        print("ğŸ“ˆ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°self.results")


def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    benchmark = PerformanceBenchmark()

    try:
        benchmark.run_all_performance_tests()
        return True
    except Exception as e:
        print(f"ğŸ’¥ æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_performance_tests()
    exit(0 if success else 1)