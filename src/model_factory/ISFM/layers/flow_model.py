"""
RectifiedFlowæ ¸å¿ƒæ¨¡å‹ - æœ€ç®€å®ç°
ä»…åŒ…å«Euleræ±‚è§£å™¨å’ŒåŸºç¡€åŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple
try:
    from .utils.flow_utils import TimeEmbedding, DimensionAdapter, simple_flow_loss, validate_tensor_shape
except ImportError:
    from utils.flow_utils import TimeEmbedding, DimensionAdapter, simple_flow_loss, validate_tensor_shape


class VelocityNetwork(nn.Module):
    """é€Ÿåº¦é¢„æµ‹ç½‘ç»œ - ç®€å•MLPå®ç°"""
    
    def __init__(self, latent_dim: int, hidden_dim: int = 256, 
                 time_dim: int = 64, condition_dim: int = 0):
        super().__init__()
        
        self.latent_dim = latent_dim
        self.hidden_dim = hidden_dim
        self.time_dim = time_dim
        self.condition_dim = condition_dim
        
        # è®¡ç®—è¾“å…¥ç»´åº¦
        input_dim = latent_dim + time_dim
        if condition_dim > 0:
            input_dim += condition_dim
        
        # ç®€å•çš„3å±‚MLP
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, latent_dim)
        )
        
        # Xavieråˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
    
    def forward(self, x_t: torch.Tensor, t_emb: torch.Tensor, 
                condition: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        é¢„æµ‹é€Ÿåº¦åœº
        
        Args:
            x_t: æ’å€¼ç‚¹ (batch_size, latent_dim)
            t_emb: æ—¶é—´åµŒå…¥ (batch_size, time_dim)
            condition: æ¡ä»¶ (batch_size, condition_dim)
        
        Returns:
            v: é€Ÿåº¦åœº (batch_size, latent_dim)
        """
        # æ‹¼æ¥è¾“å…¥
        inputs = [x_t, t_emb]
        if condition is not None:
            inputs.append(condition)
        
        x_input = torch.cat(inputs, dim=-1)
        return self.net(x_input)


class RectifiedFlow(nn.Module):
    """
    çŸ«æ­£æµæ¨¡å‹ - æœ€ç®€å®ç°
    ä»…åŒ…å«Euleræ±‚è§£å™¨å’ŒåŸºç¡€åŠŸèƒ½
    """
    
    def __init__(self, latent_dim: int, hidden_dim: int = 256,
                 time_dim: int = 64, condition_dim: int = 0,
                 sigma_min: float = 0.001, sigma_max: float = 1.0):
        super().__init__()
        
        self.latent_dim = latent_dim
        self.hidden_dim = hidden_dim
        self.time_dim = time_dim
        self.condition_dim = condition_dim
        self.sigma_min = sigma_min
        self.sigma_max = sigma_max
        
        # ç»„ä»¶
        self.time_embedding = TimeEmbedding(time_dim)
        self.velocity_net = VelocityNetwork(
            latent_dim=latent_dim,
            hidden_dim=hidden_dim,
            time_dim=time_dim,
            condition_dim=condition_dim
        )
    
    def forward(self, x: torch.Tensor, condition: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        è®­ç»ƒæ—¶çš„å‰å‘ä¼ æ’­
        
        Args:
            x: ç›®æ ‡æ•°æ® (batch_size, latent_dim)
            condition: æ¡ä»¶ (batch_size, condition_dim)
        
        Returns:
            dict: åŒ…å«v_pred, v_true, x_t, tç­‰
        """
        validate_tensor_shape(x, 2, "input x")
        
        batch_size, latent_dim = x.shape
        device = x.device
        
        # 1. é‡‡æ ·æ—¶é—´æ­¥ t ~ U(0,1)
        t = torch.rand(batch_size, device=device)
        
        # 2. é‡‡æ ·å™ªå£°
        noise = torch.randn_like(x)
        
        # 3. çº¿æ€§æ’å€¼: x_t = (1-t)*noise + t*x
        t_expanded = t.view(-1, 1)
        x_t = (1 - t_expanded) * noise + t_expanded * x
        
        # 4. çœŸå®é€Ÿåº¦: v_true = x - noise
        v_true = x - noise
        
        # 5. æ—¶é—´åµŒå…¥
        t_emb = self.time_embedding(t)
        
        # 6. é¢„æµ‹é€Ÿåº¦
        v_pred = self.velocity_net(x_t, t_emb, condition)
        
        return {
            'v_pred': v_pred,
            'v_true': v_true,
            'x_t': x_t,
            'noise': noise,
            't': t,
            't_emb': t_emb
        }
    
    def sample(self, batch_size: int, condition: Optional[torch.Tensor] = None,
               num_steps: int = 50, device: str = 'cpu') -> torch.Tensor:
        """
        é‡‡æ ·ç”Ÿæˆæ–°æ•°æ® - ä»…Euleræ±‚è§£å™¨
        
        Args:
            batch_size: æ‰¹é‡å¤§å°
            condition: æ¡ä»¶ (batch_size, condition_dim)
            num_steps: é‡‡æ ·æ­¥æ•°
            device: è®¡ç®—è®¾å¤‡
        
        Returns:
            samples: ç”Ÿæˆæ ·æœ¬ (batch_size, latent_dim)
        """
        self.eval()
        
        # ä»æ ‡å‡†é«˜æ–¯å™ªå£°å¼€å§‹
        x = torch.randn(batch_size, self.latent_dim, device=device)
        
        # æ—¶é—´æ­¥é•¿
        dt = 1.0 / num_steps
        
        with torch.no_grad():
            for i in range(num_steps):
                t = torch.full((batch_size,), i * dt, device=device)
                t_emb = self.time_embedding(t)
                
                # é¢„æµ‹é€Ÿåº¦
                v = self.velocity_net(x, t_emb, condition)
                
                # Eulerç§¯åˆ†: x_{i+1} = x_i + dt * v_i
                x = x + dt * v
        
        return x
    
    def compute_loss(self, model_outputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æŸå¤±
        
        Args:
            model_outputs: forward()çš„è¾“å‡º
        
        Returns:
            losses: æŸå¤±å­—å…¸
        """
        v_pred = model_outputs['v_pred']
        v_true = model_outputs['v_true']
        
        # åŸºç¡€æµåŒ¹é…æŸå¤±
        flow_loss = simple_flow_loss(v_pred, v_true)
        
        # ç®€å•çš„é€Ÿåº¦æ­£åˆ™åŒ–
        velocity_reg = torch.mean(v_pred.pow(2)) * 0.001
        
        total_loss = flow_loss + velocity_reg
        
        return {
            'flow_loss': flow_loss,
            'velocity_reg': velocity_reg,
            'total_loss': total_loss
        }
    
    def encode_to_noise(self, x: torch.Tensor, condition: Optional[torch.Tensor] = None,
                       num_steps: int = 50) -> torch.Tensor:
        """
        å°†æ•°æ®ç¼–ç åˆ°å™ªå£°ç©ºé—´ (åå‘è¿‡ç¨‹)
        ç”¨äºå¼‚å¸¸æ£€æµ‹
        """
        self.eval()
        current = x.clone()
        dt = 1.0 / num_steps
        
        with torch.no_grad():
            for i in range(num_steps):
                t = torch.full((x.size(0),), 1 - i * dt, device=x.device)
                t_emb = self.time_embedding(t)
                
                v = self.velocity_net(current, t_emb, condition)
                current = current - dt * v  # åå‘ç§¯åˆ†
        
        return current


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    print("ğŸ”¬ æµ‹è¯•åŸºç¡€RectifiedFlowæ¨¡å‹")
    
    # åˆ›å»ºæ¨¡å‹
    model = RectifiedFlow(latent_dim=512, hidden_dim=256, condition_dim=64)
    
    # æµ‹è¯•è¾“å…¥
    batch_size = 8
    x = torch.randn(batch_size, 512)
    condition = torch.randn(batch_size, 64)
    
    # å‰å‘ä¼ æ’­
    outputs = model(x, condition)
    print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œv_predå½¢çŠ¶: {outputs['v_pred'].shape}")
    
    # æŸå¤±è®¡ç®—
    losses = model.compute_loss(outputs)
    print(f"âœ… æŸå¤±è®¡ç®—æˆåŠŸï¼Œæ€»æŸå¤±: {losses['total_loss'].item():.6f}")
    
    # é‡‡æ ·æµ‹è¯•
    samples = model.sample(batch_size=4, condition=condition[:4], num_steps=20, device='cpu')
    print(f"âœ… é‡‡æ ·æˆåŠŸï¼Œæ ·æœ¬å½¢çŠ¶: {samples.shape}")
    
    print("ğŸ‰ åŸºç¡€RectifiedFlowæµ‹è¯•é€šè¿‡ï¼")