"""
æ¡ä»¶ç¼–ç å™¨ - ç›´æ¥ä½¿ç”¨PHM-Vibench metadata
æ”¯æŒDomain_idå’ŒDataset_idçš„å±‚æ¬¡åŒ–ç¼–ç 
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Any, Optional, Tuple
try:
    from .utils.flow_utils import MetadataExtractor
except ImportError:
    from utils.flow_utils import MetadataExtractor


class ConditionalEncoder(nn.Module):
    """
    æ¡ä»¶ç¼–ç å™¨ - ç®€åŒ–ç‰ˆæœ¬
    ç›´æ¥ä½¿ç”¨metadataä¸­çš„Domain_idå’ŒDataset_id
    """
    
    def __init__(self, embed_dim: int = 64, num_domains: int = 50, 
                 num_systems: int = 50, dropout: float = 0.1):
        super().__init__()
        
        self.embed_dim = embed_dim
        self.num_domains = num_domains
        self.num_systems = num_systems
        
        # åŸŸåµŒå…¥ (padding_idx=0 è¡¨ç¤ºæœªçŸ¥åŸŸ)
        self.domain_embedding = nn.Embedding(
            num_domains + 1,  # +1 for unknown
            embed_dim,
            padding_idx=0
        )
        
        # ç³»ç»ŸåµŒå…¥ (padding_idx=0 è¡¨ç¤ºæœªçŸ¥ç³»ç»Ÿ)
        self.system_embedding = nn.Embedding(
            num_systems + 1,  # +1 for unknown
            embed_dim, 
            padding_idx=0
        )
        
        # èåˆå±‚ - ç®€å•çš„æ³¨æ„åŠ›æœºåˆ¶
        self.fusion = nn.Sequential(
            nn.Linear(embed_dim * 2, embed_dim * 2),
            nn.LayerNorm(embed_dim * 2),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 2, embed_dim)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                torch.nn.init.normal_(module.weight, mean=0, std=0.02)
                if module.padding_idx is not None:
                    with torch.no_grad():
                        module.weight[module.padding_idx].fill_(0)
    
    def forward(self, metadata_batch: List[Dict[str, Any]]) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            metadata_batch: metadataå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹åº”æ‰¹æ¬¡ä¸­çš„ä¸€ä¸ªæ ·æœ¬
        
        Returns:
            condition_features: æ¡ä»¶ç‰¹å¾ (batch_size, embed_dim)
        """
        if not metadata_batch:
            raise ValueError("metadata_batchä¸èƒ½ä¸ºç©º")
        
        batch_size = len(metadata_batch)
        device = next(self.parameters()).device
        
        # æå–domain_idå’Œsystem_id
        domain_ids = []
        system_ids = []
        
        for metadata in metadata_batch:
            domain_id, system_id = MetadataExtractor.extract_condition_ids(metadata)
            
            # ç¡®ä¿IDåœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼Œè¶…å‡ºèŒƒå›´çš„è®¾ä¸º0(æœªçŸ¥)
            domain_id = min(domain_id, self.num_domains) if domain_id > 0 else 0
            system_id = min(system_id, self.num_systems) if system_id > 0 else 0
            
            domain_ids.append(domain_id)
            system_ids.append(system_id)
        
        # è½¬æ¢ä¸ºå¼ é‡
        domain_ids = torch.tensor(domain_ids, device=device, dtype=torch.long)
        system_ids = torch.tensor(system_ids, device=device, dtype=torch.long)
        
        # è·å–åµŒå…¥
        domain_emb = self.domain_embedding(domain_ids)  # (batch_size, embed_dim)
        system_emb = self.system_embedding(system_ids)  # (batch_size, embed_dim)
        
        # æ‹¼æ¥å¹¶èåˆ
        combined = torch.cat([domain_emb, system_emb], dim=-1)  # (batch_size, embed_dim*2)
        condition_features = self.fusion(combined)  # (batch_size, embed_dim)
        
        return condition_features
    
    def get_domain_prototype(self, domain_id: int) -> torch.Tensor:
        """è·å–ç‰¹å®šåŸŸçš„åŸå‹å‘é‡"""
        domain_id = min(max(domain_id, 0), self.num_domains)
        domain_tensor = torch.tensor([domain_id], device=next(self.parameters()).device)
        return self.domain_embedding(domain_tensor).squeeze(0)
    
    def get_system_prototype(self, system_id: int) -> torch.Tensor:
        """è·å–ç‰¹å®šç³»ç»Ÿçš„åŸå‹å‘é‡"""
        system_id = min(max(system_id, 0), self.num_systems)
        system_tensor = torch.tensor([system_id], device=next(self.parameters()).device)
        return self.system_embedding(system_tensor).squeeze(0)


class AdaptiveConditionalEncoder(ConditionalEncoder):
    """
    è‡ªé€‚åº”æ¡ä»¶ç¼–ç å™¨
    æ ¹æ®metadataè‡ªåŠ¨è°ƒæ•´åŸŸå’Œç³»ç»Ÿçš„æ•°é‡
    """
    
    @classmethod
    def from_metadata(cls, metadata_df, embed_dim: int = 64, 
                     margin: int = 10, **kwargs):
        """
        ä»metadata DataFrameåˆ›å»ºç¼–ç å™¨
        
        Args:
            metadata_df: PHM-Vibenchçš„metadata DataFrame
            embed_dim: åµŒå…¥ç»´åº¦
            margin: é¢„ç•™çš„æ‰©å±•ç©ºé—´
        """
        max_domain, max_system = MetadataExtractor.get_max_ids(metadata_df)
        
        # æ·»åŠ é¢„ç•™ç©ºé—´
        num_domains = max_domain + margin
        num_systems = max_system + margin
        
        print(f"åˆ›å»ºè‡ªé€‚åº”æ¡ä»¶ç¼–ç å™¨:")
        print(f"  - åŸŸæ•°é‡: {num_domains} (æœ€å¤§ID: {max_domain})")
        print(f"  - ç³»ç»Ÿæ•°é‡: {num_systems} (æœ€å¤§ID: {max_system})")
        
        return cls(
            embed_dim=embed_dim,
            num_domains=num_domains,
            num_systems=num_systems,
            **kwargs
        )


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    print("ğŸ”¬ æµ‹è¯•æ¡ä»¶ç¼–ç å™¨")
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = ConditionalEncoder(embed_dim=64, num_domains=10, num_systems=20)
    
    # æ¨¡æ‹Ÿmetadataæ‰¹æ¬¡
    metadata_batch = [
        {'Domain_id': 1, 'Dataset_id': 5, 'Name': 'CWRU'},
        {'Domain_id': 2, 'Dataset_id': 8, 'Name': 'XJTU'},
        {'Domain_id': None, 'Dataset_id': 3, 'Name': 'PU'},  # ç¼ºå¤±Domain_id
        {'Domain_id': 1, 'Dataset_id': None, 'Name': 'FEMTO'},  # ç¼ºå¤±Dataset_id
    ]
    
    # å‰å‘ä¼ æ’­
    condition_features = encoder(metadata_batch)
    print(f"âœ… æ¡ä»¶ç¼–ç æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {condition_features.shape}")
    print(f"âœ… ç‰¹å¾ç»Ÿè®¡: å‡å€¼={condition_features.mean().item():.4f}, æ ‡å‡†å·®={condition_features.std().item():.4f}")
    
    # æµ‹è¯•åŸå‹è·å–
    domain_proto = encoder.get_domain_prototype(1)
    system_proto = encoder.get_system_prototype(5)
    print(f"âœ… åŸŸåŸå‹å½¢çŠ¶: {domain_proto.shape}")
    print(f"âœ… ç³»ç»ŸåŸå‹å½¢çŠ¶: {system_proto.shape}")
    
    # æµ‹è¯•ä¸åŒæ¡ä»¶çš„åŒºåˆ†åº¦
    same_condition = [
        {'Domain_id': 1, 'Dataset_id': 5, 'Name': 'Test1'},
        {'Domain_id': 1, 'Dataset_id': 5, 'Name': 'Test2'},
    ]
    
    diff_condition = [
        {'Domain_id': 1, 'Dataset_id': 5, 'Name': 'Test1'},
        {'Domain_id': 2, 'Dataset_id': 8, 'Name': 'Test2'},
    ]
    
    same_features = encoder(same_condition)
    diff_features = encoder(diff_condition)
    
    same_similarity = F.cosine_similarity(same_features[0], same_features[1], dim=0)
    diff_similarity = F.cosine_similarity(diff_features[0], diff_features[1], dim=0)
    
    print(f"âœ… ç›¸åŒæ¡ä»¶ç›¸ä¼¼åº¦: {same_similarity.item():.4f}")
    print(f"âœ… ä¸åŒæ¡ä»¶ç›¸ä¼¼åº¦: {diff_similarity.item():.4f}")
    
    print("ğŸ‰ æ¡ä»¶ç¼–ç å™¨æµ‹è¯•é€šè¿‡ï¼")