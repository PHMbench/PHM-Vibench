"""
M_04_ISFM_Flow - ä¸»Flowé›†æˆæ¨¡å‹
ç»“åˆRectifiedFlow + æ¡ä»¶ç¼–ç  + ç»´åº¦é€‚é…
éµå¾ªPHM-Vibenchå·¥å‚æ¨¡å¼
"""

import torch
import torch.nn as nn
from typing import Dict, List, Any, Optional, Tuple
import numpy as np

try:
    from .layers.flow_model import RectifiedFlow
    from .layers.condition_encoder import ConditionalEncoder, AdaptiveConditionalEncoder
    from .layers.utils.flow_utils import DimensionAdapter, validate_tensor_shape
except ImportError:
    from layers.flow_model import RectifiedFlow
    from layers.condition_encoder import ConditionalEncoder, AdaptiveConditionalEncoder
    from layers.utils.flow_utils import DimensionAdapter, validate_tensor_shape


class Model(nn.Module):
    """
    M_04_ISFM_Flowä¸»æ¨¡å‹ç±»
    
    åŠŸèƒ½:
    1. å¤„ç†(B,L,C) -> (B,L*C)ç»´åº¦é€‚é…
    2. æ¡ä»¶ç¼–ç (åŸºäºmetadata)
    3. RectifiedFlowç”Ÿæˆå»ºæ¨¡
    4. æ”¯æŒè®­ç»ƒã€é‡‡æ ·ã€å¼‚å¸¸æ£€æµ‹
    """
    
    def __init__(self, args_m, metadata=None):
        super().__init__()
        
        # é…ç½®å‚æ•°
        self.sequence_length = getattr(args_m, 'sequence_length', 1024)
        self.channels = getattr(args_m, 'channels', 1)
        self.latent_dim = self.sequence_length * self.channels  # å±•å¼€åçš„ç»´åº¦
        
        # Flowæ¨¡å‹å‚æ•°
        self.hidden_dim = getattr(args_m, 'hidden_dim', 256)
        self.time_dim = getattr(args_m, 'time_dim', 64)
        self.condition_dim = getattr(args_m, 'condition_dim', 64)
        
        # æ¡ä»¶ç¼–ç å™¨å‚æ•°
        self.use_conditional = getattr(args_m, 'use_conditional', True)
        
        print(f"ğŸš€ åˆå§‹åŒ–M_04_ISFM_Flow:")
        print(f"   - åºåˆ—é•¿åº¦: {self.sequence_length}")
        print(f"   - é€šé“æ•°: {self.channels}")  
        print(f"   - æ½œåœ¨ç»´åº¦: {self.latent_dim}")
        print(f"   - ä½¿ç”¨æ¡ä»¶ç¼–ç : {self.use_conditional}")
        
        # åˆ›å»ºæ¡ä»¶ç¼–ç å™¨
        if self.use_conditional and metadata is not None:
            self.condition_encoder = AdaptiveConditionalEncoder.from_metadata(
                metadata.df,
                embed_dim=self.condition_dim
            )
        elif self.use_conditional:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.condition_encoder = ConditionalEncoder(
                embed_dim=self.condition_dim,
                num_domains=getattr(args_m, 'num_domains', 50),
                num_systems=getattr(args_m, 'num_systems', 50)
            )
        else:
            self.condition_encoder = None
            self.condition_dim = 0
        
        # åˆ›å»ºFlowæ¨¡å‹
        self.flow_model = RectifiedFlow(
            latent_dim=self.latent_dim,
            hidden_dim=self.hidden_dim,
            time_dim=self.time_dim,
            condition_dim=self.condition_dim,
            sigma_min=getattr(args_m, 'sigma_min', 0.001),
            sigma_max=getattr(args_m, 'sigma_max', 1.0)
        )
        
        # ä¿å­˜metadataå¼•ç”¨
        self.metadata = metadata
        
        print(f"   âœ… Flowæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   âœ… æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in self.parameters()):,}")
    
    def _encode_conditions(self, file_ids: List[str]) -> Optional[torch.Tensor]:
        """
        ä»file_idåˆ—è¡¨ç¼–ç æ¡ä»¶
        
        Args:
            file_ids: æ–‡ä»¶IDåˆ—è¡¨
        
        Returns:
            condition_features: æ¡ä»¶ç‰¹å¾ (batch_size, condition_dim)
        """
        if not self.use_conditional or self.condition_encoder is None:
            return None
        
        if self.metadata is None:
            raise ValueError("éœ€è¦metadataæ¥æå–æ¡ä»¶ä¿¡æ¯")
        
        # ä»metadataæå–ä¿¡æ¯
        metadata_batch = []
        for file_id in file_ids:
            if file_id in self.metadata:
                metadata_batch.append(dict(self.metadata[file_id]))
            else:
                # ä½¿ç”¨é»˜è®¤å€¼å¤„ç†ç¼ºå¤±çš„file_id
                metadata_batch.append({
                    'Domain_id': None,
                    'Dataset_id': None,
                    'Name': 'unknown'
                })
        
        return self.condition_encoder(metadata_batch)
    
    def forward(self, x: torch.Tensor, file_ids: Optional[List[str]] = None,
                return_loss: bool = True) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥æ•°æ® (B, L, C)
            file_ids: æ–‡ä»¶IDåˆ—è¡¨ï¼Œç”¨äºæ¡ä»¶ç¼–ç 
            return_loss: æ˜¯å¦è®¡ç®—å¹¶è¿”å›æŸå¤±
        
        Returns:
            outputs: åŒ…å«æ¨¡å‹è¾“å‡ºå’ŒæŸå¤±çš„å­—å…¸
        """
        validate_tensor_shape(x, 3, "input x")
        
        batch_size, seq_len, channels = x.shape
        
        # éªŒè¯è¾“å…¥ç»´åº¦
        if seq_len != self.sequence_length or channels != self.channels:
            print(f"âš ï¸  ç»´åº¦ä¸åŒ¹é…: æœŸæœ›({self.sequence_length}, {self.channels}), "
                  f"å®é™…({seq_len}, {channels})")
        
        # 1. ç»´åº¦é€‚é…: (B, L, C) -> (B, L*C)
        x_flat = DimensionAdapter.encode_3d_to_1d(x)
        
        # 2. æ¡ä»¶ç¼–ç 
        condition_features = None
        if file_ids is not None:
            condition_features = self._encode_conditions(file_ids)
        
        # 3. Flowæ¨¡å‹å‰å‘ä¼ æ’­
        flow_outputs = self.flow_model(x_flat, condition_features)
        
        # 4. è®¡ç®—æŸå¤±
        if return_loss:
            losses = self.flow_model.compute_loss(flow_outputs)
            flow_outputs.update(losses)
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        flow_outputs.update({
            'x_original': x,
            'x_flat': x_flat,
            'condition_features': condition_features,
            'batch_size': batch_size,
            'seq_len': seq_len,
            'channels': channels
        })
        
        return flow_outputs
    
    def sample(self, batch_size: int, file_ids: Optional[List[str]] = None,
               num_steps: int = 50, device: Optional[str] = None) -> torch.Tensor:
        """
        é‡‡æ ·ç”Ÿæˆæ–°æ•°æ®
        
        Args:
            batch_size: æ‰¹é‡å¤§å°
            file_ids: æ–‡ä»¶IDåˆ—è¡¨ï¼ˆç”¨äºæ¡ä»¶ç”Ÿæˆï¼‰
            num_steps: é‡‡æ ·æ­¥æ•°
            device: è®¡ç®—è®¾å¤‡
        
        Returns:
            samples: ç”Ÿæˆæ ·æœ¬ (batch_size, sequence_length, channels)
        """
        if device is None:
            device = next(self.parameters()).device
        
        # æ¡ä»¶ç¼–ç 
        condition_features = None
        if file_ids is not None:
            condition_features = self._encode_conditions(file_ids)
        
        # Flowé‡‡æ ·
        samples_flat = self.flow_model.sample(
            batch_size=batch_size,
            condition=condition_features,
            num_steps=num_steps,
            device=device
        )
        
        # ç»´åº¦æ¢å¤: (B, L*C) -> (B, L, C)
        samples = DimensionAdapter.decode_1d_to_3d(
            samples_flat, self.sequence_length, self.channels
        )
        
        return samples
    
    def encode_to_noise(self, x: torch.Tensor, file_ids: Optional[List[str]] = None,
                       num_steps: int = 50) -> torch.Tensor:
        """
        å°†æ•°æ®ç¼–ç åˆ°å™ªå£°ç©ºé—´ï¼ˆç”¨äºå¼‚å¸¸æ£€æµ‹ï¼‰
        
        Args:
            x: è¾“å…¥æ•°æ® (B, L, C)
            file_ids: æ–‡ä»¶IDåˆ—è¡¨
            num_steps: ç¼–ç æ­¥æ•°
        
        Returns:
            noise: å¯¹åº”çš„å™ªå£° (B, L, C)
        """
        validate_tensor_shape(x, 3, "input x")
        
        # ç»´åº¦é€‚é…
        x_flat = DimensionAdapter.encode_3d_to_1d(x)
        
        # æ¡ä»¶ç¼–ç 
        condition_features = None
        if file_ids is not None:
            condition_features = self._encode_conditions(file_ids)
        
        # ç¼–ç åˆ°å™ªå£°
        noise_flat = self.flow_model.encode_to_noise(
            x_flat, condition_features, num_steps
        )
        
        # ç»´åº¦æ¢å¤
        noise = DimensionAdapter.decode_1d_to_3d(
            noise_flat, self.sequence_length, self.channels
        )
        
        return noise
    
    def compute_anomaly_score(self, x: torch.Tensor, file_ids: Optional[List[str]] = None,
                             num_steps: int = 50) -> torch.Tensor:
        """
        è®¡ç®—å¼‚å¸¸åˆ†æ•°
        
        Args:
            x: è¾“å…¥æ•°æ® (B, L, C)
            file_ids: æ–‡ä»¶IDåˆ—è¡¨
            num_steps: è®¡ç®—æ­¥æ•°
        
        Returns:
            scores: å¼‚å¸¸åˆ†æ•° (B,)
        """
        # ç¼–ç åˆ°å™ªå£°ç©ºé—´
        noise = self.encode_to_noise(x, file_ids, num_steps)
        
        # è®¡ç®—å™ªå£°çš„L2èŒƒæ•°ä½œä¸ºå¼‚å¸¸åˆ†æ•°
        scores = torch.norm(noise.view(noise.size(0), -1), dim=1)
        
        return scores


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    print("ğŸ”¬ æµ‹è¯•M_04_ISFM_Flowé›†æˆæ¨¡å‹")
    
    # Mocké…ç½®
    class MockArgs:
        def __init__(self):
            self.sequence_length = 1024
            self.channels = 1
            self.hidden_dim = 128
            self.time_dim = 32
            self.condition_dim = 64
            self.use_conditional = True
    
    # Mock metadata
    class MockMetadata:
        def __init__(self):
            import pandas as pd
            self.df = pd.DataFrame({
                'Domain_id': [1, 2, 1, 3],
                'Dataset_id': [5, 8, 5, 10],
                'Name': ['CWRU', 'XJTU', 'PU', 'FEMTO']
            })
        
        def __contains__(self, key):
            return key in ['file1', 'file2', 'file3']
        
        def __getitem__(self, key):
            if key == 'file1':
                return {'Domain_id': 1, 'Dataset_id': 5, 'Name': 'CWRU'}
            elif key == 'file2':
                return {'Domain_id': 2, 'Dataset_id': 8, 'Name': 'XJTU'}
            else:
                return {'Domain_id': 3, 'Dataset_id': 10, 'Name': 'FEMTO'}
    
    args = MockArgs()
    metadata = MockMetadata()
    
    # åˆ›å»ºæ¨¡å‹
    model = Model(args, metadata)
    
    # æµ‹è¯•æ•°æ®
    batch_size = 4
    x = torch.randn(batch_size, args.sequence_length, args.channels)
    file_ids = ['file1', 'file2', 'file3', 'file1']
    
    print(f"\nğŸ“Š æµ‹è¯•è¾“å…¥:")
    print(f"   - æ•°æ®å½¢çŠ¶: {x.shape}")
    print(f"   - æ–‡ä»¶ID: {file_ids}")
    
    # å‰å‘ä¼ æ’­æµ‹è¯•
    outputs = model(x, file_ids)
    print(f"\nâœ… å‰å‘ä¼ æ’­æˆåŠŸ:")
    print(f"   - v_predå½¢çŠ¶: {outputs['v_pred'].shape}")
    print(f"   - æ€»æŸå¤±: {outputs['total_loss'].item():.6f}")
    
    # é‡‡æ ·æµ‹è¯•
    samples = model.sample(batch_size=2, file_ids=['file1', 'file2'], num_steps=20)
    print(f"\nâœ… é‡‡æ ·æˆåŠŸ:")
    print(f"   - æ ·æœ¬å½¢çŠ¶: {samples.shape}")
    
    # å¼‚å¸¸æ£€æµ‹æµ‹è¯•
    anomaly_scores = model.compute_anomaly_score(x, file_ids, num_steps=20)
    print(f"\nâœ… å¼‚å¸¸æ£€æµ‹æˆåŠŸ:")
    print(f"   - å¼‚å¸¸åˆ†æ•°å½¢çŠ¶: {anomaly_scores.shape}")
    print(f"   - åˆ†æ•°èŒƒå›´: [{anomaly_scores.min().item():.3f}, {anomaly_scores.max().item():.3f}]")
    
    print(f"\nğŸ‰ M_04_ISFM_Flowé›†æˆæµ‹è¯•é€šè¿‡ï¼")