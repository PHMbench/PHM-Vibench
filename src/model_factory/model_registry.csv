model.type,model.name,module_path,args,notes,test_status
ISFM,M_01_ISFM,src/model_factory/ISFM/M_01_ISFM.py,"embedding, backbone, task_head, num_classes, patch_size_L, patch_size_C, num_patches, output_dim","v0.1.0 recommended demo config; CDDG classification","/"
ISFM,M_02_ISFM,src/model_factory/ISFM/M_02_ISFM.py,"embedding, backbone, task_head, num_classes, num_channels, patch_size_L, patch_size_C, num_patches, output_dim","system-aware HSE; multi-task experiments","/"
ISFM,M_02_ISFM_heterogeneous_batch,src/model_factory/ISFM/M_02_ISFM_heterogeneous_batch.py,"embedding, backbone, task_head, num_classes, num_channels, patch_size_L, patch_size_C, num_patches, output_dim","heterogeneous batch classification; per-sample system_id","/"
ISFM,M_03_ISFM,src/model_factory/ISFM/M_03_ISFM.py,"embedding, backbone, task_head, patch_size_L, patch_size_C, num_patches, output_dim","lightweight research/prototype; prediction head","/"
ISFM_Prompt,M_02_ISFM_Prompt,src/model_factory/ISFM_Prompt/M_02_ISFM_Prompt.py,"prompt-specific args (see implementation)","prompt-style ISFM variant for HSE-Prompt experiments","/"
CNN,ResNet1D,src/model_factory/CNN/ResNet1D.py,"input_dim, block_type, layers, initial_channels, num_classes/output_dim","1D ResNet baseline for time-series classification","/"
CNN,AttentionCNN,src/model_factory/CNN/AttentionCNN.py,"input_dim, depth, num_classes/output_dim","CNN with attention modules for temporal patterns","/"
CNN,MobileNet1D,src/model_factory/CNN/MobileNet1D.py,"input_dim, depth, width_mult, num_classes/output_dim","lightweight 1D CNN for resource-constrained settings","/"
CNN,MultiScaleCNN,src/model_factory/CNN/MultiScaleCNN.py,"input_dim, num_scales, num_classes/output_dim","multi-scale CNN with Inception-style blocks","/"
CNN,TCN,src/model_factory/CNN/TCN.py,"input_dim, num_channels, kernel_size, dilation, num_classes/output_dim","temporal convolutional network with dilations","/"
RNN,AttentionLSTM,src/model_factory/RNN/AttentionLSTM.py,"input_dim, hidden_dim, num_layers, bidirectional, num_classes/output_dim","LSTM with attention for sequence modeling","/"
RNN,AttentionGRU,src/model_factory/RNN/AttentionGRU.py,"input_dim, hidden_dim, num_layers, bidirectional, num_classes/output_dim","GRU with attention mechanisms","/"
RNN,ConvLSTM,src/model_factory/RNN/ConvLSTM.py,"input_dim, hidden_dim, kernel_size, num_layers, num_classes/output_dim","convolutional LSTM for spatio-temporal signals","/"
RNN,ResidualRNN,src/model_factory/RNN/ResidualRNN.py,"input_dim, hidden_dim, num_layers, num_classes/output_dim","deep RNN with residual connections","/"
RNN,TransformerRNN,src/model_factory/RNN/TransformerRNN.py,"input_dim, hidden_dim, num_layers, num_heads, num_classes/output_dim","hybrid transformer-RNN architecture","/"
Transformer,PatchTST,src/model_factory/Transformer/PatchTST.py,"input_dim, patch_size, stride, d_model, n_heads, num_layers, d_ff, dropout, num_classes/output_dim","patch-based transformer for long-term forecasting","/"
Transformer,Autoformer,src/model_factory/Transformer/Autoformer.py,"enc_in, dec_in, c_out, d_model, n_heads, e_layers, d_layers, seq_len, label_len, pred_len","decomposition transformer for time-series forecasting","/"
Transformer,Informer,src/model_factory/Transformer/Informer.py,"enc_in, dec_in, c_out, d_model, n_heads, e_layers, d_layers, seq_len, label_len, pred_len, factor, distil","efficient transformer with ProbSparse attention","/"
Transformer,Linformer,src/model_factory/Transformer/Linformer.py,"input_dim, d_model, n_heads, num_layers, k, seq_len, num_classes/output_dim","linear-complexity transformer variant","/"
Transformer,ConvTransformer,src/model_factory/Transformer/ConvTransformer.py,"input_dim, d_model, n_heads, num_layers, kernel_size, num_classes/output_dim","hybrid convolutional transformer","/"
Transformer,Transformer_Dummy,src/model_factory/Transformer/Transformer_Dummy.py,"input_dim, d_model, n_heads, num_layers, num_classes/output_dim","simple transformer baseline / sanity check","/"
MLP,ResNetMLP,src/model_factory/MLP/ResNetMLP.py,"input_dim, hidden_dim, num_layers, activation, dropout, num_classes","MLP with residual connections for time series","/"
MLP,MLPMixer,src/model_factory/MLP/MLPMixer.py,"input_dim, patch_size, hidden_dim, num_layers, mlp_ratio, dropout, num_classes","token- and channel-mixing MLP architecture","/"
MLP,gMLP,src/model_factory/MLP/gMLP.py,"input_dim, hidden_dim, num_layers, seq_len, dropout, num_classes","gated MLP with spatial gating units","/"
MLP,DenseNetMLP,src/model_factory/MLP/DenseNetMLP.py,"input_dim, growth_rate, num_layers, hidden_dim, dropout, num_classes","MLP with dense connectivity for feature reuse","/"
MLP,Dlinear,src/model_factory/MLP/Dlinear.py,"input_dim, seq_len, individual, kernel_size","decomposition-based linear forecasting model","/"
NO,FNO,src/model_factory/NO/FNO.py,"input_dim, output_dim, modes, width, num_layers","Fourier Neural Operator for operator learning","/"
NO,DeepONet,src/model_factory/NO/DeepONet.py,"input_dim, branch_depth, trunk_depth, width, trunk_dim, output_dim","DeepONet for nonlinear operator approximation","/"
NO,NeuralODE,src/model_factory/NO/NeuralODE.py,"input_dim, hidden_dim, num_layers, solver, rtol, atol, num_classes/output_dim","neural ordinary differential equation model","/"
NO,GraphNO,src/model_factory/NO/GraphNO.py,"input_dim, hidden_dim, num_layers, num_eigenvectors, graph_size, num_classes/output_dim","graph-based neural operator","/"
NO,WaveletNO,src/model_factory/NO/WaveletNO.py,"input_dim, hidden_dim, num_layers, wavelet, num_levels, mode, num_classes/output_dim","multi-scale wavelet neural operator","/"
X_model,MWA_CNN,src/model_factory/X_model/MWA_CNN.py,"input_dim, num_windows, kernel_size, num_classes/output_dim","multi-window attention CNN for auxiliary analysis","/"
X_model,TSPN,src/model_factory/X_model/TSPN.py,"input_dim, hidden_dim, num_prototypes, num_classes/output_dim","time-series prototype/explainability network","/"
