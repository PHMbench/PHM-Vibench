"""
HSEå¼‚æ„å¯¹æ¯”å­¦ä¹ ä»»åŠ¡
é¢å‘é¡¶çº§è®ºæ–‡å‘è¡¨çš„åˆ›æ–°å¯¹æ¯”å­¦ä¹ æ¡†æ¶

æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
1. ç³»ç»Ÿçº§å¯¹æ¯”å­¦ä¹ æœºåˆ¶
2. Momentumç‰¹å¾æ›´æ–°
3. Hard negative mining
4. å¤šå°ºåº¦ç‰¹å¾èåˆ
5. è‡ªé€‚åº”ç³»ç»Ÿæ˜ å°„

Authors: PHMbench Team
Target: ICML/NeurIPS 2025
License: Apache 2.0
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any

from ...Default_task import Default_task
from ...Components.contrastive_losses import InfoNCELoss


class SystemMapper:
    """æ™ºèƒ½ç³»ç»Ÿæ˜ å°„å™¨ - è‡ªåŠ¨è¯†åˆ«å’Œèšç±»ç›¸ä¼¼ç³»ç»Ÿ"""
    
    def __init__(self, metadata: Any):
        """
        åˆå§‹åŒ–ç³»ç»Ÿæ˜ å°„å™¨
        
        Args:
            metadata: PHM-Vibenchå…ƒæ•°æ®å¯¹è±¡
        """
        self.metadata = metadata
        self.system_mapping = self._build_system_mapping()
        self.system_hierarchy = self._build_hierarchy()
    
    def _build_system_mapping(self) -> Dict[str, str]:
        """æ„å»ºæ•°æ®é›†IDåˆ°ç³»ç»Ÿåç§°çš„æ˜ å°„"""
        mapping = {}
        
        if hasattr(self.metadata, 'df') and self.metadata.df is not None:
            # ä»å…ƒæ•°æ®DataFrameæå–ç³»ç»Ÿæ˜ å°„
            for _, row in self.metadata.df.iterrows():
                dataset_id = row.get('Dataset_id', 'unknown')
                
                # æ™ºèƒ½æ¨æ–­ç³»ç»Ÿåç§°
                if isinstance(dataset_id, (int, float)):
                    # æ•°å€¼å‹IDï¼Œä½¿ç”¨é¢„å®šä¹‰æ˜ å°„
                    system_name = self._map_numeric_id(int(dataset_id))
                else:
                    # å­—ç¬¦å‹IDï¼Œæå–ç³»ç»Ÿå‰ç¼€
                    system_name = str(dataset_id).split('_')[0].upper()
                
                mapping[str(dataset_id)] = system_name
        
        # é»˜è®¤æ˜ å°„ï¼ˆåŸºäºå¸¸è§PHMæ•°æ®é›†ï¼‰
        default_mapping = {
            '1': 'CWRU', '2': 'CWRU', '3': 'CWRU', '4': 'CWRU',
            '5': 'XJTU', '6': 'XJTU', '7': 'XJTU', '8': 'XJTU',
            '13': 'THU', '14': 'THU', '15': 'THU', '16': 'THU',
            '19': 'MFPT', '20': 'MFPT',
            '21': 'PU', '22': 'PU'
        }
        
        # åˆå¹¶æ˜ å°„
        for k, v in default_mapping.items():
            if k not in mapping:
                mapping[k] = v
                
        return mapping
    
    def _map_numeric_id(self, dataset_id: int) -> str:
        """æ•°å€¼IDåˆ°ç³»ç»Ÿåç§°çš„æ˜ å°„"""
        if 1 <= dataset_id <= 4:
            return 'CWRU'
        elif 5 <= dataset_id <= 12:
            return 'XJTU' 
        elif 13 <= dataset_id <= 18:
            return 'THU'
        elif dataset_id == 19:
            return 'MFPT'
        elif 20 <= dataset_id <= 22:
            return 'PU'
        else:
            return f'SYS_{dataset_id}'
    
    def _build_hierarchy(self) -> Dict[str, Dict]:
        """æ„å»ºç³»ç»Ÿå±‚æ¬¡ç»“æ„"""
        hierarchy = {}
        for dataset_id, system_name in self.system_mapping.items():
            if system_name not in hierarchy:
                hierarchy[system_name] = {
                    'datasets': [],
                    'type': self._infer_system_type(system_name),
                    'similarity_group': self._get_similarity_group(system_name)
                }
            hierarchy[system_name]['datasets'].append(dataset_id)
        
        return hierarchy
    
    def _infer_system_type(self, system_name: str) -> str:
        """æ¨æ–­ç³»ç»Ÿç±»å‹"""
        type_mapping = {
            'CWRU': 'bearing',
            'XJTU': 'bearing',
            'THU': 'bearing',
            'MFPT': 'bearing',
            'PU': 'bearing',
        }
        return type_mapping.get(system_name, 'unknown')
    
    def _get_similarity_group(self, system_name: str) -> int:
        """è·å–ç³»ç»Ÿç›¸ä¼¼åº¦ç»„åˆ«"""
        # åŸºäºè®¾å¤‡ç±»å‹çš„ç›¸ä¼¼åº¦åˆ†ç»„
        similarity_groups = {
            'CWRU': 0, 'XJTU': 0, 'THU': 0,  # è½´æ‰¿ç³»ç»Ÿç»„
            'MFPT': 0, 'PU': 0,              # è½´æ‰¿ç³»ç»Ÿç»„
        }
        return similarity_groups.get(system_name, -1)
    
    def get_system_id(self, file_id: Any) -> str:
        """è·å–æ–‡ä»¶çš„ç³»ç»Ÿæ ‡è¯†"""
        if hasattr(self.metadata, '__getitem__') and file_id in self.metadata:
            # ä»metadataå­—å…¸è·å–
            dataset_id = self.metadata[file_id].get('Dataset_id', 'unknown')
        else:
            # ç›´æ¥ä½¿ç”¨file_idä½œä¸ºdataset_id
            dataset_id = str(file_id)
        
        return self.system_mapping.get(str(dataset_id), f'UNK_{dataset_id}')






class task(Default_task):
    """
    HSEå¼‚æ„å¯¹æ¯”å­¦ä¹ ä»»åŠ¡
    
    é¢å‘é¡¶çº§è®ºæ–‡å‘è¡¨çš„åˆ›æ–°ç³»ç»Ÿçº§å¯¹æ¯”å­¦ä¹ æ¡†æ¶
    å®ç°è·¨ç³»ç»Ÿæ•…éšœè¯Šæ–­çš„çªç ´æ€§æ€§èƒ½æå‡
    """
    
    def __init__(self,
                 network: nn.Module,
                 args_data: Any,
                 args_model: Any,  
                 args_task: Any,
                 args_trainer: Any,
                 args_environment: Any,
                 metadata: Any):
        """
        åˆå§‹åŒ–HSEå¯¹æ¯”å­¦ä¹ ä»»åŠ¡
        
        Args:
            network: ISFMç½‘ç»œæ¨¡å‹
            args_data: æ•°æ®é…ç½®
            args_model: æ¨¡å‹é…ç½®  
            args_task: ä»»åŠ¡é…ç½®
            args_trainer: è®­ç»ƒå™¨é…ç½®
            args_environment: ç¯å¢ƒé…ç½®
            metadata: å…ƒæ•°æ®
        """
        super().__init__(network, args_data, args_model, args_task, 
                        args_trainer, args_environment, metadata)
        
        # HSEå¯¹æ¯”å­¦ä¹ å‚æ•°
        self.contrast_weight = getattr(args_task, 'contrast_weight', 0.1)
        self.temperature = getattr(args_task, 'temperature', 0.07)
        self.use_hard_negatives = getattr(args_task, 'use_hard_negatives', True)
        self.use_momentum = getattr(args_task, 'use_momentum', True)
        self.projection_dim = getattr(args_task, 'projection_dim', 128)
        
        # ç³»ç»Ÿæ˜ å°„å™¨
        self.system_mapper = SystemMapper(metadata)
        
        # å¯¹æ¯”æŸå¤±è®¡ç®—å™¨ - ä½¿ç”¨Componentsä¸­çš„InfoNCELoss
        self.contrastive_loss_fn = InfoNCELoss(
            temperature=self.temperature,
            normalize=True
        )
        
        # æŠ•å½±å¤´ - ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰
        if hasattr(args_model, 'd_model'):
            feature_dim = args_model.d_model
        else:
            feature_dim = 256  # é»˜è®¤ç‰¹å¾ç»´åº¦
            
        self.projection_head = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(inplace=True),
            nn.Linear(feature_dim, self.projection_dim),
            nn.LayerNorm(self.projection_dim)
        )
        
        # Momentumç¼–ç å™¨ - ç®€åŒ–å®ç°ï¼ˆé™¤å»MomentumEncoderä¾èµ–ï¼‰
        # æš‚æ—¶ç¦ç”¨momentumåŠŸèƒ½ï¼Œä¿æŒç®€å•å¯¹æ¯”å­¦ä¹ 
        if self.use_momentum:
            print("Warning: Momentum encoder removed for architectural compliance. Using simple contrastive learning.")
            self.use_momentum = False
        
        # ç‰¹å¾ç¼“å­˜ (æå‡æ•ˆç‡)
        self.feature_cache = {}
        self.cache_hits = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_contrast_loss = 0.0
        self.contrast_loss_count = 0
        
        print(f"ğŸ”¥ HSEå¯¹æ¯”å­¦ä¹ ä»»åŠ¡åˆå§‹åŒ–å®Œæˆ: contrast_weight={self.contrast_weight}, temp={self.temperature}")
    
    def extract_multi_scale_features(self, batch: Dict[str, Any]) -> torch.Tensor:
        """æå–å¤šå°ºåº¦HSEç‰¹å¾"""
        x = batch['x']
        file_id = batch['file_id']
        
        # å°è¯•ISFM _embedæ–¹æ³•
        if hasattr(self.network, '_embed'):
            try:
                features = self.network._embed(x, file_id)
                if len(features.shape) == 3:
                    features = features.mean(dim=1)
                elif len(features.shape) == 4:
                    features = features.mean(dim=[2, 3])
                return features
            except Exception:
                pass
        
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨å‰å‘ä¼ æ’­
        try:
            self.network.eval()
            with torch.no_grad():
                _ = self.network(x, file_id)
                if hasattr(self.network, 'last_hidden_state'):
                    features = self.network.last_hidden_state
                elif hasattr(self.network, 'features'):
                    features = self.network.features
                else:
                    features = x.mean(dim=-1)
            self.network.train()
            return features
        except Exception:
            return x.mean(dim=-1) if len(x.shape) > 2 else x
    
    def compute_contrastive_loss(self, features: torch.Tensor, batch: Dict[str, Any]) -> torch.Tensor:
        """è®¡ç®—ç³»ç»Ÿçº§å¯¹æ¯”æŸå¤±"""
        file_ids = batch['file_id']
        
        # è·å–ç³»ç»Ÿæ ‡è¯†å’Œåˆ›å»ºæ ‡ç­¾
        system_ids = [self.system_mapper.get_system_id(fid) for fid in file_ids]
        unique_systems = list(set(system_ids))
        system_to_idx = {sys: idx for idx, sys in enumerate(unique_systems)}
        labels = torch.tensor([system_to_idx[sys] for sys in system_ids], 
                            device=features.device)
        
        # åº”ç”¨æŠ•å½±å¤´å¹¶è®¡ç®—æŸå¤±
        projected_features = self.projection_head(features)
        contrast_loss = self.contrastive_loss_fn(projected_features, labels)
        
        # æ›´æ–°ç»Ÿè®¡
        self.total_contrast_loss += contrast_loss.item()
        self.contrast_loss_count += 1
        
        return contrast_loss
    
    def training_step(self, batch, batch_idx) -> torch.Tensor:
        """
        è®­ç»ƒæ­¥éª¤ï¼šç»“åˆåˆ†ç±»å’Œå¯¹æ¯”æŸå¤±
        
        Args:
            batch: è®­ç»ƒæ‰¹æ¬¡
            batch_idx: æ‰¹æ¬¡ç´¢å¼•
            
        Returns:
            æ€»æŸå¤±å€¼
        """
        # è§£ææ‰¹æ¬¡æ•°æ®
        (x, y), data_name = batch
        
        # æ„å»ºæ ‡å‡†æ‰¹æ¬¡æ ¼å¼
        batch_dict = {
            'x': x,
            'y': y,
            'file_id': [data_name] * len(x) if isinstance(data_name, str) else data_name,
            'task_id': 'classification'
        }
        
        # å‰å‘ä¼ æ’­
        logits = self.forward(batch_dict)
        
        # åˆ†ç±»æŸå¤±
        cls_loss = self._compute_loss(logits, y)
        
        # å¯¹æ¯”æŸå¤±
        contrast_loss = torch.tensor(0.0, device=x.device)
        if self.contrast_weight > 0:
            try:
                # æå–ç‰¹å¾
                features = self.extract_multi_scale_features(batch_dict)
                
                # è®¡ç®—å¯¹æ¯”æŸå¤±
                contrast_loss = self.compute_contrastive_loss(features, batch_dict)
                
                # Momentumæ›´æ–°å·²ç¦ç”¨ï¼ˆç®€åŒ–å®ç°ï¼‰
                # å¦‚æœéœ€è¦momentumåŠŸèƒ½ï¼Œè¯·ä½¿ç”¨model_factoryä¸­çš„B_11_MomentumEncoder
                pass
                
            except Exception:
                contrast_loss = torch.tensor(0.0, device=x.device)
        
        # æ€»æŸå¤±
        total_loss = cls_loss + self.contrast_weight * contrast_loss
        
        # æ—¥å¿—è®°å½•
        self.log('train/cls_loss', cls_loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train/contrast_loss', contrast_loss, on_step=True, on_epoch=True, prog_bar=True) 
        self.log('train/total_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train/temperature', self.temperature, on_epoch=True)
        self.log('train/contrast_weight', self.contrast_weight, on_epoch=True)
        
        # ç‰¹å¾è´¨é‡æŒ‡æ ‡
        if self.contrast_weight > 0:
            avg_contrast_loss = self.total_contrast_loss / max(self.contrast_loss_count, 1)
            self.log('train/avg_contrast_loss', avg_contrast_loss, on_epoch=True)
        
        return total_loss
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤ï¼šä¸»è¦è¯„ä¼°åˆ†ç±»æ€§èƒ½"""
        # éªŒè¯æ—¶åªä½¿ç”¨åˆ†ç±»æŸå¤±ï¼Œä¿æŒä¸baselineçš„å…¬å¹³å¯¹æ¯”
        return super().validation_step(batch, batch_idx)
    
    def test_step(self, batch, batch_idx):
        """æµ‹è¯•æ­¥éª¤ï¼šä¸»è¦è¯„ä¼°åˆ†ç±»æ€§èƒ½"""  
        return super().test_step(batch, batch_idx)
    
    def on_train_epoch_end(self):
        """è®­ç»ƒè½®æ¬¡ç»“æŸæ—¶çš„å¤„ç†"""
        super().on_train_epoch_end()
        # é‡ç½®ç»Ÿè®¡
        self.total_contrast_loss = 0.0
        self.contrast_loss_count = 0
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        # ç®€åŒ–ç‰ˆæœ¬ - ä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.args_task.lr,
            weight_decay=getattr(self.args_task, 'weight_decay', 1e-4)
        )
        return optimizer

