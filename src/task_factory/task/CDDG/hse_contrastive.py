"""
HSE Contrastive Learning Task for Cross-Dataset Domain Generalization

Task implementation that integrates Prompt-guided Hierarchical Signal Embedding (HSE)
with state-of-the-art contrastive learning for industrial fault diagnosis.

Core Innovation: First work to combine system metadata prompts with contrastive learning
for cross-system industrial fault diagnosis, targeting ICML/NeurIPS 2025.

Key Features:
1. Prompt-guided contrastive learning with system-aware sampling
2. Two-stage training support (pretrain/finetune)
3. Cross-dataset domain generalization (CDDG)
4. Integration with all 6 SOTA contrastive losses
5. System-invariant representation learning

Authors: PHM-Vibench Team
Date: 2025-01-06
License: Apache 2.0
"""

import torch
import torch.nn.functional as F
import pytorch_lightning as pl
from typing import Dict, Any, Optional, List, Tuple
import logging
from collections import defaultdict

from ..Default_task import Default_task
from ...Components.prompt_contrastive import PromptGuidedContrastiveLoss
from ...Components.loss import get_loss_fn
from ...Components.metrics import get_metrics

logger = logging.getLogger(__name__)


class task(Default_task):
    """
    HSE Prompt-guided Contrastive Learning Task
    
    Inherits from Default_task and extends with:
    1. Prompt-guided contrastive learning capabilities
    2. System-aware positive/negative sampling
    3. Two-stage training workflow support
    4. Cross-dataset domain generalization
    
    Training Stages:
    - **Pretrain**: Multi-system contrastive learning with prompt guidance
    - **Finetune**: Task-specific adaptation with frozen prompts
    """
    
    def __init__(
        self, 
        network, 
        args_data, 
        args_model, 
        args_task, 
        args_trainer, 
        args_environment, 
        metadata
    ):
        """Initialize HSE contrastive learning task."""
        
        # Initialize parent class
        super().__init__(network, args_data, args_model, args_task, args_trainer, args_environment, metadata)
        
        # HSE-specific configuration
        self.args_task = args_task
        self.args_model = args_model
        self.metadata = metadata
        
        # Training stage control
        self.training_stage = getattr(args_model, 'training_stage', 'pretrain')
        self.freeze_prompt = getattr(args_model, 'freeze_prompt', False)
        
        # Contrastive learning setup
        self.enable_contrastive = getattr(args_task, 'contrast_weight', 0.0) > 0
        self.contrast_weight = getattr(args_task, 'contrast_weight', 0.15)
        
        if self.enable_contrastive:
            # Initialize prompt-guided contrastive loss
            self.contrastive_loss = PromptGuidedContrastiveLoss(
                base_loss_type=getattr(args_task, 'contrast_loss', 'INFONCE'),
                temperature=getattr(args_task, 'temperature', 0.07),
                prompt_similarity_weight=getattr(args_task, 'prompt_weight', 0.1),
                system_aware_sampling=getattr(args_task, 'use_system_sampling', True),
                enable_cross_system_contrast=getattr(args_task, 'cross_system_contrast', True),
                # Pass additional arguments for specific loss types
                margin=getattr(args_task, 'margin', 0.3),  # For Triplet loss
                lambda_param=getattr(args_task, 'barlow_lambda', 5e-3),  # For Barlow Twins
            )
            
            logger.info(f"‚úì Contrastive learning enabled: {args_task.contrast_loss} (weight: {self.contrast_weight})")
        else:
            self.contrastive_loss = None
            logger.info("‚Ñπ Contrastive learning disabled (weight: 0.0)")
        
        # Domain generalization setup
        self.source_domain_id = getattr(args_task, 'source_domain_id', [])
        self.target_domain_id = getattr(args_task, 'target_domain_id', [])
        
        # Metrics tracking
        self.train_metrics_dict = defaultdict(list)
        self.val_metrics_dict = defaultdict(list)
        
        # Log configuration
        self._log_task_config()
    
    def _log_task_config(self):
        """Log task configuration for debugging."""
        logger.info("HSE Contrastive Learning Task Configuration:")
        logger.info(f"  - Training stage: {self.training_stage}")
        logger.info(f"  - Contrastive learning: {self.enable_contrastive}")
        logger.info(f"  - Source domains: {self.source_domain_id}")
        logger.info(f"  - Target domains: {self.target_domain_id}")
        logger.info(f"  - Prompt frozen: {self.freeze_prompt}")
        if self.enable_contrastive:
            logger.info(f"  - Contrastive loss: {self.args_task.contrast_loss}")
            logger.info(f"  - Contrastive weight: {self.contrast_weight}")
    
    def training_step(self, batch, batch_idx):
        """Training step with prompt-guided contrastive learning."""
        return self._shared_step(batch, batch_idx, stage='train')
    
    def validation_step(self, batch, batch_idx):
        """Validation step."""
        return self._shared_step(batch, batch_idx, stage='val')
    
    def _shared_step(self, batch, batch_idx, stage='train'):
        """Shared step logic for training and validation."""
        # Unpack batch
        (x, y), data_name = batch
        batch_size = x.size(0)
        
        # Extract system information from data_name if available
        system_ids = self._extract_system_ids(data_name) if data_name is not None else None
        
        # Forward pass through network
        network_output = self.network(x, metadata=self._create_metadata_batch(data_name, batch_size))
        
        # Handle network output (may include prompt features)
        if isinstance(network_output, tuple):
            # HSE model returns (features, prompts)
            features, prompts = network_output
        else:
            # Fallback: no prompt features available
            features = network_output
            prompts = None
        
        # 1. Classification loss (always computed)
        classification_loss = self.loss_fn(features, y)
        total_loss = classification_loss
        
        # 2. Contrastive loss (if enabled)
        contrastive_loss_value = torch.tensor(0.0, device=x.device)
        if self.enable_contrastive and prompts is not None:
            try:
                contrastive_losses = self.contrastive_loss(
                    features=features,
                    prompts=prompts,
                    labels=y,
                    system_ids=system_ids
                )
                contrastive_loss_value = contrastive_losses['total_loss']
                total_loss += self.contrast_weight * contrastive_loss_value
                
                # Log individual contrastive loss components
                if stage == 'train':
                    self.log('train_contrastive_loss', contrastive_loss_value, prog_bar=True)
                    self.log('train_base_loss', contrastive_losses['base_loss'])
                    self.log('train_prompt_loss', contrastive_losses['prompt_loss'])
                    self.log('train_system_loss', contrastive_losses['system_loss'])
                else:
                    self.log('val_contrastive_loss', contrastive_loss_value, prog_bar=True)
            
            except Exception as e:
                logger.warning(f"Contrastive loss computation failed: {e}")
                # Fallback: continue with classification loss only
        
        # 3. Compute metrics
        self._compute_and_log_metrics(features, y, stage)
        
        # 4. Log losses
        self.log(f'{stage}_classification_loss', classification_loss)
        self.log(f'{stage}_total_loss', total_loss, prog_bar=True)
        self.log(f'{stage}_loss', total_loss)  # Standard name for callbacks
        
        # 5. Additional logging for two-stage training
        if stage == 'train':
            self.log('contrast_weight', self.contrast_weight)
            if prompts is not None:
                self.log('prompt_norm', prompts.norm().mean())
        
        return total_loss
    
    def _extract_system_ids(self, data_name) -> Optional[torch.Tensor]:
        """Extract system IDs from data_name for cross-system learning."""
        if data_name is None:
            return None
        
        try:
            # data_name format: ['dataset_domain_sample', ...] or similar
            system_ids = []
            
            for name in data_name:
                if isinstance(name, str):
                    # Parse dataset ID from name (e.g., 'CWRU_0_123' -> dataset_id=1 for CWRU)
                    if 'CWRU' in name:
                        system_ids.append(1)
                    elif 'XJTU' in name:
                        system_ids.append(6)
                    elif 'THU' in name:
                        system_ids.append(13)
                    elif 'MFPT' in name:
                        system_ids.append(19)
                    else:
                        # Parse from numeric prefix if available
                        parts = name.split('_')
                        if len(parts) > 0 and parts[0].isdigit():
                            system_ids.append(int(parts[0]))
                        else:
                            system_ids.append(0)  # Unknown system
                else:
                    system_ids.append(0)  # Default
            
            return torch.tensor(system_ids, device=self.device)
            
        except Exception as e:
            logger.warning(f"Failed to extract system IDs: {e}")
            return None
    
    def _create_metadata_batch(self, data_name, batch_size) -> Optional[List[Dict[str, Any]]]:
        """Create metadata batch for prompt encoding."""
        if data_name is None or not hasattr(self.network, 'embedding'):
            return None
        
        try:
            metadata_batch = []
            system_ids = self._extract_system_ids(data_name)
            
            for i in range(batch_size):
                # Create metadata dict for each sample (NO fault label - it's prediction target!)
                meta_dict = {
                    'Dataset_id': system_ids[i].item() if system_ids is not None else 0,
                    'Domain_id': 0,  # Default domain
                    'Sample_rate': 1000.0,  # Default sampling rate
                    # CRITICAL: NO Label field - fault type is prediction target, not prompt input!
                }
                metadata_batch.append(meta_dict)
            
            return metadata_batch
            
        except Exception as e:
            logger.warning(f"Failed to create metadata batch: {e}")
            return None
    
    def _compute_and_log_metrics(self, logits, labels, stage):
        """Compute and log classification metrics."""
        with torch.no_grad():
            # Accuracy
            preds = torch.argmax(logits, dim=1)
            acc = (preds == labels).float().mean()
            self.log(f'{stage}_acc', acc, prog_bar=True)
            
            # F1 score (if multi-class)
            if len(torch.unique(labels)) > 1:
                try:
                    from sklearn.metrics import f1_score
                    f1 = f1_score(
                        labels.cpu().numpy(), 
                        preds.cpu().numpy(), 
                        average='macro',
                        zero_division=0
                    )
                    self.log(f'{stage}_f1', f1)
                except ImportError:
                    pass  # Skip F1 if sklearn not available
    
    def configure_optimizers(self):
        """Configure optimizers with support for fine-grained learning rates."""
        # Get base configuration from parent
        optimizer_config = super().configure_optimizers()
        
        # Handle fine-grained learning rates for two-stage training
        if hasattr(self.args_task, 'backbone_lr_multiplier') and self.training_stage == 'finetune':
            # Different learning rates for different components
            param_groups = []
            
            # Backbone parameters (lower LR)
            backbone_params = []
            # Task head parameters (full LR)  
            head_params = []
            # Other parameters (full LR)
            other_params = []
            
            for name, param in self.network.named_parameters():
                if not param.requires_grad:
                    continue  # Skip frozen parameters
                    
                if 'backbone' in name.lower() or 'embedding' in name.lower():
                    backbone_params.append(param)
                elif 'head' in name.lower() or 'classifier' in name.lower():
                    head_params.append(param)
                else:
                    other_params.append(param)
            
            # Create parameter groups
            base_lr = self.args_task.lr
            backbone_lr = base_lr * getattr(self.args_task, 'backbone_lr_multiplier', 0.1)
            
            if backbone_params:
                param_groups.append({'params': backbone_params, 'lr': backbone_lr})
            if head_params:
                param_groups.append({'params': head_params, 'lr': base_lr})
            if other_params:
                param_groups.append({'params': other_params, 'lr': base_lr})
            
            if param_groups:
                optimizer = torch.optim.AdamW(
                    param_groups,
                    weight_decay=getattr(self.args_task, 'weight_decay', 1e-4)
                )
                
                logger.info(f"Fine-grained LR: backbone={backbone_lr:.1e}, head={base_lr:.1e}")
                
                # Return with scheduler if specified
                if hasattr(self.args_task, 'scheduler') and self.args_task.scheduler:
                    scheduler = self._create_scheduler(optimizer)
                    return [optimizer], [scheduler]
                else:
                    return optimizer
        
        # Fallback to parent configuration
        return optimizer_config
    
    def _create_scheduler(self, optimizer):
        """Create learning rate scheduler."""
        scheduler_type = getattr(self.args_task, 'scheduler_type', 'cosine')
        
        if scheduler_type == 'cosine':
            from torch.optim.lr_scheduler import CosineAnnealingLR
            return CosineAnnealingLR(
                optimizer, 
                T_max=getattr(self.args_task, 'epochs', 50)
            )
        elif scheduler_type == 'step':
            from torch.optim.lr_scheduler import StepLR
            return StepLR(
                optimizer,
                step_size=getattr(self.args_task, 'step_size', 15),
                gamma=getattr(self.args_task, 'gamma', 0.5)
            )
        else:
            # Fallback to no scheduler
            return None
    
    def on_train_epoch_end(self):
        """Called at the end of training epoch."""
        super().on_train_epoch_end()
        
        # Log training stage
        self.log('training_stage', 1.0 if self.training_stage == 'pretrain' else 0.0)
        
        # Additional HSE-specific logging
        current_epoch = self.current_epoch
        
        if current_epoch % 10 == 0:  # Log every 10 epochs
            logger.info(f"Epoch {current_epoch}: Stage={self.training_stage}, "
                       f"Contrastive={self.enable_contrastive}, "
                       f"Frozen_prompt={self.freeze_prompt}")
    
    def on_validation_epoch_end(self):
        """Called at the end of validation epoch.""" 
        super().on_validation_epoch_end()
        
        # Could add epoch-level validation logging here if needed
        pass
    
    def set_training_stage(self, stage: str):
        """Set training stage (for two-stage training)."""
        self.training_stage = stage
        
        # Update contrastive learning based on stage
        if stage == 'finetune':
            # Disable contrastive learning for finetuning
            self.contrast_weight = 0.0
            self.enable_contrastive = False
            logger.info("Switched to finetuning: disabled contrastive learning")
        elif stage == 'pretrain':
            # Enable contrastive learning for pretraining
            self.contrast_weight = getattr(self.args_task, 'contrast_weight', 0.15)
            self.enable_contrastive = self.contrast_weight > 0
            logger.info(f"Switched to pretraining: enabled contrastive learning (weight: {self.contrast_weight})")
        
        # Update network training stage if supported
        if hasattr(self.network, 'set_training_stage'):
            self.network.set_training_stage(stage)
    
    def get_contrastive_features(self, batch) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Extract features and prompts for contrastive learning analysis."""
        (x, y), data_name = batch
        batch_size = x.size(0)
        
        with torch.no_grad():
            network_output = self.network(x, metadata=self._create_metadata_batch(data_name, batch_size))
            
            if isinstance(network_output, tuple):
                features, prompts = network_output
            else:
                features = network_output
                prompts = None
            
            return features, prompts


# Self-testing section  
if __name__ == "__main__":
    print("üéØ Testing HSE Contrastive Learning Task")
    
    # Mock arguments for testing
    class MockArgs:
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)
    
    # Test configuration
    args_model = MockArgs(
        embedding='E_01_HSE_Prompt',
        training_stage='pretrain',
        freeze_prompt=False,
        prompt_dim=128
    )
    
    args_task = MockArgs(
        loss='CE',
        contrast_loss='INFONCE',
        contrast_weight=0.15,
        temperature=0.07,
        lr=5e-4,
        epochs=50,
        source_domain_id=[1, 13, 19],
        target_domain_id=[6],
        use_system_sampling=True,
        cross_system_contrast=True
    )
    
    args_data = MockArgs(batch_size=32)
    args_trainer = MockArgs(gpus=1)
    args_environment = MockArgs(output_dir='test')
    
    print("1. Testing Task Initialization:")
    try:
        # Create mock network
        import torch.nn as nn
        
        class MockNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                self.backbone = nn.Linear(256, 512)
                self.head = nn.Linear(512, 10)
                
            def forward(self, x, metadata=None):
                # Simulate HSE model output: (features, prompts)
                features = self.head(self.backbone(x.view(x.size(0), -1)))
                prompts = torch.randn(x.size(0), 128, device=x.device)
                return features, prompts
            
            def set_training_stage(self, stage):
                pass  # Mock implementation
        
        mock_network = MockNetwork()
        mock_metadata = {'num_classes': 10}
        
        # Initialize task
        hse_task = task(
            mock_network, args_data, args_model, args_task, 
            args_trainer, args_environment, mock_metadata
        )
        
        print("   ‚úì HSE contrastive task initialized successfully")
        print(f"   ‚úì Training stage: {hse_task.training_stage}")
        print(f"   ‚úì Contrastive learning: {hse_task.enable_contrastive}")
        print(f"   ‚úì Contrastive weight: {hse_task.contrast_weight}")
        
    except Exception as e:
        print(f"   ‚úó Task initialization failed: {e}")
    
    print("\n2. Testing System ID Extraction:")
    try:
        # Test system ID extraction
        data_names = ['CWRU_0_123', 'XJTU_1_456', 'THU_2_789', 'unknown_file']
        system_ids = hse_task._extract_system_ids(data_names)
        
        print(f"   ‚úì Data names: {data_names}")
        print(f"   ‚úì Extracted system IDs: {system_ids}")
        
    except Exception as e:
        print(f"   ‚úó System ID extraction failed: {e}")
    
    print("\n3. Testing Metadata Batch Creation:")
    try:
        metadata_batch = hse_task._create_metadata_batch(data_names, len(data_names))
        
        print(f"   ‚úì Created metadata batch: {len(metadata_batch)} samples")
        print(f"   ‚úì First sample metadata: {metadata_batch[0] if metadata_batch else 'None'}")
        
        # Verify no Label field (critical requirement)
        if metadata_batch and 'Label' in metadata_batch[0]:
            print("   ‚ö†Ô∏è WARNING: Label found in metadata - this should not happen!")
        else:
            print("   ‚úì Correctly excluded Label from metadata (prediction target)")
            
    except Exception as e:
        print(f"   ‚úó Metadata batch creation failed: {e}")
    
    print("\n4. Testing Training Stage Switching:")
    try:
        # Test stage switching
        original_weight = hse_task.contrast_weight
        
        hse_task.set_training_stage('finetune')
        print(f"   ‚úì Switched to finetune: contrast_weight={hse_task.contrast_weight}")
        
        hse_task.set_training_stage('pretrain')
        print(f"   ‚úì Switched to pretrain: contrast_weight={hse_task.contrast_weight}")
        
    except Exception as e:
        print(f"   ‚úó Training stage switching failed: {e}")
    
    print("\n5. Testing Mock Forward Pass:")
    try:
        # Create mock batch
        batch_size = 4
        x = torch.randn(batch_size, 2, 1024)  # (B, C, L) format
        y = torch.randint(0, 10, (batch_size,))
        data_names = ['CWRU_0_1', 'XJTU_1_2', 'THU_2_3', 'CWRU_0_4']
        
        batch = ((x, y), data_names)
        
        # Mock training step (would normally require full Lightning setup)
        print(f"   ‚úì Created mock batch: {x.shape}, labels: {y}")
        print(f"   ‚úì Data names: {data_names}")
        
        # Test metadata creation
        metadata = hse_task._create_metadata_batch(data_names, batch_size)
        print(f"   ‚úì Generated metadata for {len(metadata)} samples")
        
    except Exception as e:
        print(f"   ‚úó Mock forward pass test failed: {e}")
    
    print("\n" + "="*70)
    print("‚úÖ HSE Contrastive Learning Task tests completed!")
    print("üöÄ Ready for integration with PHM-Vibench training pipeline.")
    
    # Configuration example
    print("\nüí° Configuration Example:")
    print("""
    # configs/demo/HSE_Contrastive/hse_prompt_pretrain.yaml
    task:
      type: "CDDG"
      name: "hse_contrastive"
      
      # Cross-dataset domain generalization
      source_domain_id: [1, 13, 19]  # CWRU, THU, MFPT
      target_domain_id: [6]          # XJTU
      
      # Contrastive learning
      contrast_loss: "INFONCE"
      contrast_weight: 0.15
      temperature: 0.07
      use_system_sampling: true
      cross_system_contrast: true
      
      # Standard training parameters
      loss: "CE"
      lr: 5e-4
      epochs: 50
    """)