# CDDG Task Module

## ğŸš§ å®ç°çŠ¶æ€ (Implementation Status)

### âœ… å·²å®ç° (Fully Implemented)
- **HSEå¯¹æ¯”å­¦ä¹ **: `hse_contrastive.py` - 1037è¡Œå¤æ‚å®ç°ï¼Œæ”¯æŒé˜¶æ®µæ„ŸçŸ¥è®­ç»ƒ
- **å¯¹æ¯”ç­–ç•¥ç³»ç»Ÿ**: `contrastive_strategies.py` - 1225è¡Œå®Œæ•´çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶
- **å¤šé˜¶æ®µè®­ç»ƒ**: ä¸¤é˜¶æ®µé¢„è®­ç»ƒ+å¾®è°ƒï¼ŒçœŸæ­£çš„æµåˆ†ç¦»æ¶æ„
- **ç³»ç»Ÿæç¤ºèåˆ**: attention/gate/concatç­‰èåˆæœºåˆ¶
- **åŒè§†å›¾å¯¹æ¯”å­¦ä¹ **: SimCSEé£æ ¼çš„æ•°æ®å¢å¼ºç­–ç•¥

### ğŸš§ éƒ¨åˆ†å®ç° (Partially Implemented)
- **æ ‡å‡†åˆ†ç±»åŒ…è£…å™¨**: `classification.py` - 21è¡ŒåŸºç¡€Default_taskåŒ…è£…å™¨
- **è·¨åŸŸåŸºæœ¬åŠŸèƒ½**: åŸºç¡€çš„source_domain_id/target_domain_idæ”¯æŒ

### âŒ TODO: å¾…å®ç° (Not Yet Implemented)
- **å…¶ä»–å¯¹æ¯”æŸå¤±**: SimCLR, SwAV, MoCo, VICReg, BarlowTwins, BYOLç­‰6ç§SOTAæ–¹æ³•
- **åŸŸè‡ªé€‚åº”æŸå¤±**: MMD, CORAL, DANNç­‰åŸŸå¯¹é½æŠ€æœ¯
- **ç³»ç»Ÿæ„ŸçŸ¥é‡‡æ ·**: è·¨ç³»ç»Ÿæ­£è´Ÿæ ·æœ¬å¯¹çš„æ™ºèƒ½é‡‡æ ·ç­–ç•¥
- **å¤šæºåŸŸè‡ªé€‚åº”**: çœŸæ­£çš„å¤šæºåŸŸè®­ç»ƒå’ŒåŸŸé—´å¯¹é½

> **æ³¨æ„**: HSEå¯¹æ¯”å­¦ä¹ æ˜¯æ ¸å¿ƒå®ç°ï¼Œå…¶ä»–åŠŸèƒ½ä¸ºè®¾è®¡ç›®æ ‡æˆ–åŸºç¡€åŒ…è£…ã€‚

## Overview

The CDDG (Cross-Dataset Domain Generalization) task module implements tasks designed for training models that can generalize across different datasets and domains. This is critical for industrial fault diagnosis where models need to work reliably across different equipment types, operating conditions, and measurement systems.

## Architecture

The CDDG module focuses on learning domain-invariant representations that maintain performance when transferring from source domains (training datasets) to target domains (testing datasets).

## Available Tasks

### 1. classification.py
**Standard cross-dataset classification task**

- **Purpose**: Basic classification with cross-dataset domain adaptation
- **Use Case**: When you need simple domain transfer without specialized techniques
- **Features**:
  - Multi-source domain training
  - Domain adaptation losses (MMD, CORAL, etc.)
  - Cross-entropy loss with domain penalty

### 2. hse_contrastive.py â­ **Innovation Task**
**HSE Prompt-guided Contrastive Learning for Cross-Dataset Domain Generalization**

- **Purpose**: Novel contrastive learning approach with system metadata prompts
- **Innovation**: First work combining system prompts with contrastive learning for industrial fault diagnosis
- **Target**: ICML/NeurIPS 2025 submission
- **Features**:
  - Prompt-guided contrastive learning with system-aware sampling
  - Two-stage training support (pretrain/finetune)
  - Integration with 6 SOTA contrastive losses
  - System-invariant representation learning
  - Cross-system domain generalization

## Configuration Examples

### Standard CDDG Classification
```yaml
task:
  type: "CDDG"
  name: "classification"
  source_domain_id: [1, 5, 6]    # Training domains (BASIC SUPPORT)
  target_domain_id: 19           # Test domain (BASIC SUPPORT)
  loss: "CE"                     # Cross-entropy loss (WORKS)
  # TODO: domain_adaptation_loss: "MMD"  # Domain adaptation - NOT IMPLEMENTED
  # TODO: domain_weight: 0.1             # Weight for domain loss - NOT IMPLEMENTED
```

### HSE Contrastive Learning âœ… IMPLEMENTED
```yaml
task:
  type: "CDDG"
  name: "hse_contrastive"
  source_domain_id: [1, 5, 6]    # WORKS
  target_domain_id: 19           # WORKS

  # Contrastive learning settings (WORKING)
  contrast_weight: 1.0           # âœ… WORKS (renamed from contrastive_weight)
  classification_weight: 0.1    # âœ… WORKS
  temperature: 0.07              # âœ… WORKS

  # Prompt settings (WORKING)
  prompt_fusion: "attention"     # âœ… WORKS (attention/gate/add/none)

  # Two-stage training (WORKING)
  training_stage: "pretrain"     # âœ… WORKS ("pretrain" or "finetune")
  # freeze_prompts: false          # Controlled by stage settings
```

## Key Parameters

### Domain Configuration
- `source_domain_id`: âœ… List of source domain IDs for training (BASIC SUPPORT)
- `target_domain_id`: âœ… Target domain ID for evaluation (BASIC SUPPORT)
- `domain_adaptation_loss`: âŒ TODO: Type of domain adaptation loss ("MMD", "CORAL", "DANN") - NOT IMPLEMENTED
- `domain_weight`: âŒ TODO: Weight for domain adaptation loss - NOT IMPLEMENTED

### Contrastive Learning (HSE) âœ… IMPLEMENTED
- `contrast_weight`: âœ… Weight for contrastive loss term (WORKS)
- `classification_weight`: âœ… Weight for classification loss term (WORKS)
- `temperature`: âœ… Temperature parameter for contrastive loss (WORKS)
- `num_negatives`: âŒ TODO: Number of negative samples per positive - NOT NEEDED (auto-handled)
- `contrast_loss`: âŒ TODO: Other loss types ("SimCLR", "SwAV", "MoCo", "VICReg", "BarlowTwins", "BYOL") - NOT IMPLEMENTED

### Prompt System (HSE) âœ… IMPLEMENTED
- `prompt_fusion`: âœ… Method for fusing prompts ("attention", "gate", "add", "none") (WORKS)
- `training_stage`: âœ… Stage for training behavior ("pretrain", "finetune") (WORKS)
- `prompt_dim`: âŒ TODO: Dimension of prompt embeddings - AUTO-CONFIGURED
- `freeze_prompts`: âŒ TODO: Whether to freeze prompt parameters - CONTROLLED BY STAGE

## Usage Examples

### Basic CDDG Experiment
```bash
# Train on CWRU, test on THU
python main.py --config configs/demo/Multiple_DG/CWRU_THU_basic.yaml
```

### HSE Contrastive Learning Pipeline
```bash
# Stage 1: Pretraining with contrastive learning
python main.py --config configs/hse/pretrain_contrastive.yaml

# Stage 2: Fine-tuning for classification
python main.py --config configs/hse/finetune_classification.yaml
```

### Multi-Domain Training
```bash
# Train on multiple sources, test on single target
python main.py --config configs/demo/Multiple_DG/all_to_THU.yaml
```

## Integration with Framework

### Task Registration
Tasks are automatically registered when imported. The HSE contrastive task extends the `Default_task` class with specialized contrastive learning capabilities.

### Model Compatibility
- **ISFM Models**: Full support with prompt integration
- **Backbone Networks**: Compatible with all backbone architectures
- **Task Heads**: Works with classification and multi-task heads

### Data Pipeline
- Supports all 30+ datasets in PHM-Vibench
- Automatic domain splitting based on metadata
- System-aware sampling for contrastive learning

## Advanced Features

### System-Aware Sampling (HSE) âŒ TODO: NOT IMPLEMENTED
The HSE contrastive task plans to implement intelligent sampling strategies:
- **Positive pairs**: Same fault type, different systems (cross-system invariance)
- **Negative pairs**: Different fault types with system awareness
- **Hard negatives**: Similar faults from different systems

> **å½“å‰å®ç°**: ä½¿ç”¨åŸºç¡€çš„åŒè§†å›¾å¯¹æ¯”å­¦ä¹ ï¼Œæœªå®ç°é«˜çº§ç³»ç»Ÿæ„ŸçŸ¥é‡‡æ ·ç­–ç•¥ã€‚

### Two-Stage Training
1. **Pretraining Stage**: Learn system-invariant representations via contrastive learning
2. **Fine-tuning Stage**: Adapt to specific classification tasks with frozen prompts

### Metrics and Evaluation
- Standard classification metrics (accuracy, F1-score, precision, recall)
- Domain-specific metrics (per-domain accuracy, domain gap)
- Contrastive learning metrics (alignment, uniformity)

## Research Context

The HSE contrastive learning task implements our core research contribution:
- **Innovation**: Prompt-guided contrastive learning for industrial domains
- **Novelty**: First to combine system metadata with contrastive learning
- **Impact**: Addresses critical cross-system generalization challenge
- **Validation**: Comprehensive evaluation across 30+ industrial datasets

## References

- [Task Factory Documentation](../CLAUDE.md)
- [HSE Innovation Specification](.claude/specs/hse-complete-publication-pipeline/)
- [Configuration System](../../../configs/CLAUDE.md)
- [Model Factory](../../../model_factory/CLAUDE.md)