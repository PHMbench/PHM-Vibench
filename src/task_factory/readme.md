

-----

# Task Factory Module (Continue updating)

## ğŸ¯ Purpose

The **Task Factory** is the central orchestrator of the Vibench training pipeline. Its primary role is to assemble a **PyTorch Lightning `LightningModule`**, which encapsulates the entire training, validation, and testing logic. It acts as the glue that connects the neural network (`model`), the data (`DataLoaders`), and the specific training procedures, such as loss calculations, metric logging, and optimization steps.

This factory-based approach allows Vibench to handle diverse tasksâ€”from standard classification to complex domain generalizationâ€”by simply swapping out the task configuration.

> æç¤ºï¼š`CLAUDE.md` ä»…ä½œå¯¼èˆªï¼Œæœ€æ–°å­—æ®µè¯´æ˜ä¸ç¤ºä¾‹è¯·ä»¥æœ¬ README ä¸ºå‡†ã€‚

-----

## ğŸ“‚ Module Structure

The module is composed of a main factory function and a structured set of directories for different task implementations.

| File / Directory | Description |
| :--- | :--- |
| `task_factory.py` | The main entry point. It contains the `task_factory(...)` function that receives the model, all configuration arguments, and metadata to build the final `LightningModule`. |
| `Default_task.py` | Baseline Lightning task wrapper, used as default single-task implementation and as a base class for many custom tasks. |
| `task/` | Subfolders for concrete task families: `DG/`, `CDDG/`, `pretrain/`, `FS/`, `GFS/`, `ID/` (e.g. `ID_task`) and `MT/` (multi-task Lightning modules). Each subfolder contains one or more `LightningModule` implementations. |
| `Components/` | A collection of reusable modules for building tasks, such as specialized loss functions (`loss.py`), performance metrics (`metrics.py`), and regularization techniques. |

-----

## âš™ï¸ Configuration

The behavior of the `Task_Factory` is controlled via the `task` section in your YAML configuration file.

**Key Configuration Fields:**

  * **`type`**: Specifies the task category. This corresponds to a subfolder within the `src/task_factory/task/` directory (e.g., `DG`, `CDDG`, `FS`, `pretrain`).
  * **`name`**: The name of the Python file within the `type` subfolder that contains the task logicï¼ˆä¾‹å¦‚ `classification.py` â†’ `name: "classification"`ï¼Œ`hse_contrastive.py` â†’ `name: "hse_contrastive"`ï¼‰ã€‚
  * **Task-Specific Options**: Any other parameters needed by the task, such as the names of loss functions, metric choices, regularization strengths, or learning algorithm hyperparameters.

**Example Configuration (`.yaml`):**

```yaml
task:
  name: "classification"
  type: "DG"   # æˆ– "CDDG" / "FS" / "pretrain"

  task_list: ['classification', 'prediction']
  target_domain_num: 1

  loss: "CE" # cross_entropy
  metrics: ["acc"]
  target_system_id: [1,13,6,12,19]

  
  optimizer: "adam"

  lr: 0.0001
  weight_decay: 0.0001

  scheduler: true
  scheduler_type: "reduceonplateau"

  patience: 20

  step_size: 3
  gamma: 0.5

  regularization: 
    l2: 1e-5
    l1: 1e-5
  alpha_prediction: 1

  # prediction args
  mask_ratio: 0.1
  forecast_part: 0.1

  num_systems: 1
  num_domains: 1
  num_labels: 3 # n_way to set num_labels, should be equal to the number of 
  num_support: 1
  num_query: 1
  num_episodes: 5

-----

## ğŸ”– Common `type` / `name` combinations (v0.1.0)

ä¸‹é¢çš„ CSV è¡¨æ˜¯å½“å‰ç‰ˆæœ¬ä¸­ **æ¨è/å·²æœ‰å®ç°** çš„ `task.type` ä¸ `task.name` ç»„åˆï¼Œä¸€è¡Œå¯¹åº”ä¸€ç§å¯é€‰ä»»åŠ¡ï¼›ä½ åªéœ€è¦åœ¨é…ç½®é‡Œå¡«è¿™ä¸¤åˆ—ï¼Œå°±èƒ½é€šè¿‡ Task Factory æ­£ç¡®åŠ è½½å¯¹åº”æ¨¡å—ã€‚  
åŒæ—¶ç»™å‡ºäº† Task çš„æ„é€ å‡½æ•° `args`ã€å¯¹åº”çš„ `dataset_task` è·¯å¾„ä¸æ„é€ å‚æ•°ï¼Œä»¥åŠé¢„ç•™çš„ `test_status` åˆ—æ–¹ä¾¿ä½ æ ‡è®°æµ‹è¯•æƒ…å†µã€‚  
å®Œæ•´è¡¨æ ¼ç»´æŠ¤åœ¨ `src/task_factory/task_registry.csv` ä¸­ï¼Œä¸‹é¢æ˜¯å…¶ç»“æ„ç¤ºæ„ï¼š

```csv
id,task.type,task.name,path,args,dataset_path,dataset_args,batch_format,notes,test_status
1,Default_task,Default_task,Default_task.py,"(network, args_data, args_model, args_task, args_trainer, args_environment, metadata)",dataset_task/Default_dataset.py,"(data, metadata, args_data, args_task, mode)","Default windows: {'x','y'}","Baseline single-task Lightning wrapper",
2,Default_task,ID_task,task/ID/ID_task.py,"(network, args_data, args_model, args_task, args_trainer, args_environment, metadata)",dataset_task/ID/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","ID-based windows: {'x','y','file_id',...}","ID_dataset / ID_task pipeline with flexible windowing",
3,DG,classification,task/DG/classification.py,"(network, args_data, args_model, args_task, args_trainer, args_environment, metadata)",dataset_task/DG/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","{'x','y','file_id',...}","Single-dataset domain generalization classification",
4,CDDG,classification,task/CDDG/classification.py,"(network, args_data, args_model, args_task, args_trainer, args_environment, metadata)",dataset_task/CDDG/classification_dataset.py,"(data, metadata, args_data, args_task, mode)","{'x','y','file_id','domain_id',...}","Cross-dataset (CDDG) classification",
...
```
```

- å½“ä½ ç¼–å†™ config æ—¶ï¼Œåªè¦ä¿è¯ï¼š
  - `task.type` æ­£å¥½æ˜¯ä¸Šè¡¨ä¸­çš„ `task.type` å­—æ®µ
  - `task.name` æ­£å¥½æ˜¯ä¸Šè¡¨ä¸­çš„ `task.name` å­—æ®µ  
 å…¶ä½™è¶…å‚æ•°ï¼ˆå¦‚ lossã€metricsã€mask_ratio ç­‰ï¼‰ä¼šåœ¨å„è‡ªçš„ä»»åŠ¡ README ä¸­è¯¦ç»†è¯´æ˜ï¼ˆä¾‹å¦‚ `task/pretrain/README.md`ã€`task/FS/README.md` ç­‰ï¼‰ã€‚

-----

## ğŸŒŠ Workflow

The factory follows a clear, step-by-step process to build the task module.

1.  **Receive Inputs**: The factory is called after the `Model_Factory` has created the neural network. It takes the `network` (`nn.Module`) and all relevant configuration objects (`args_task`, `args_data`, `args_model`, etc.) as input.

2.  **Dynamic Import**: Using the `type` and `name` from the configuration, the factory constructs the import path for the desired task module. For example, a `type` of "DG" and `name` of "classification" resolves to `src.task_factory.task.DG.classification`.

3.  **Instantiation**: The factory imports the `task` class from the selected module and creates an instance of it. It passes the `network`, all necessary configurations, and the dataset `metadata` to the class constructor.

4.  **Return `LightningModule`**: The fully initialized `LightningModule` is returned to the main pipeline.

-----

## ğŸ Returned Object

The `task_factory` function returns a single, powerful object:

  * **A `pytorch_lightning.LightningModule` instance**: This object is now ready for the PyTorch Lightning `Trainer`. It contains all the necessary logic, including:
      * `training_step`: Defines what happens for each batch during training.
      * `validation_step` & `test_step`: Defines the logic for evaluation.
      * `configure_optimizers`: Sets up the optimizer(s) and learning rate scheduler(s).
      * Logging of losses and metrics to the specified logger (e.g., TensorBoard, W\&B).


## TODO 
# 1é¢†åŸŸæ³›åŒ–(DG)ä»»åŠ¡
task:
  name: classification
  type: DG
  simpler: default  # ä½¿ç”¨é»˜è®¤çš„DGé€‰æ‹©å™¨
  target_system_id: [RM_001_CWRU]
  source_domain_id: [0, 1, 2]
  target_domain_id: [3, 4]

# 2å°æ ·æœ¬å­¦ä¹ (Few-Shot)ä»»åŠ¡
task:
  name: classification
  type: few_shot
  simpler: few_shot
  target_system_id: [RM_001_CWRU]
  n_way: 5         # 5ç±»åˆ†ç±»
  k_shot: 1        # æ¯ç±»1ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒ
  n_query: 15      # æ¯ç±»æœ€å¤š15ä¸ªæ ·æœ¬ç”¨äºæµ‹è¯•
  label_column: Label

# 3. ä¸å¹³è¡¡æ•°æ®ä»»åŠ¡
task:
  name: classification
  type: imbalanced
  simpler: imbalanced
  target_system_id: [RM_001_CWRU]
  imbalance_ratio: 0.1  # å°‘æ•°ç±»ä¸å¤šæ•°ç±»çš„æ¯”ä¾‹
  minority_labels: [2, 4]  # æŒ‡å®šå°‘æ•°ç±»æ ‡ç­¾
  stratify: true  # ä½¿ç”¨åˆ†å±‚æŠ½æ ·
