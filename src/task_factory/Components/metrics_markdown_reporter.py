"""
MarkdownæŒ‡æ ‡æŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆè¯¦ç»†çš„ç³»ç»Ÿçº§æŒ‡æ ‡åˆ†ææŠ¥å‘Š
"""
import os
from datetime import datetime
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional


class MetricsMarkdownReporter:
    """ç”ŸæˆMarkdownæ ¼å¼çš„æŒ‡æ ‡æŠ¥å‘Š"""
    
    def __init__(self, save_dir: str = "metrics_reports"):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        
        Args:
            save_dir: æŠ¥å‘Šä¿å­˜ç›®å½•
        """
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def generate_report(self, 
                       system_metrics: Dict[str, Dict[str, float]],
                       global_metrics: Dict[str, float] = None,
                       phase: str = 'test',
                       experiment_name: str = 'multi_task_phm',
                       config_info: Dict[str, Any] = None) -> Path:
        """ç”Ÿæˆå®Œæ•´çš„MarkdownæŠ¥å‘Š
        
        Args:
            system_metrics: ç³»ç»Ÿçº§æŒ‡æ ‡ {system_id: {metric: value}}
            global_metrics: å…¨å±€æŒ‡æ ‡ {metric: value}
            phase: å®éªŒé˜¶æ®µ (train/val/test)
            experiment_name: å®éªŒåç§°
            config_info: é…ç½®ä¿¡æ¯
            
        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        
        report_lines = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        report_lines.append(f"# Multi-Task PHM Metrics Report")
        report_lines.append(f"\n**Experiment**: {experiment_name}")
        report_lines.append(f"**Phase**: {phase}")
        report_lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ·»åŠ é…ç½®ä¿¡æ¯
        if config_info:
            report_lines.append(f"\n**Configuration**:")
            for key, value in config_info.items():
                report_lines.append(f"- {key}: {value}")
                
        report_lines.append("\n---\n")
        
        # 1. æ‰§è¡Œæ‘˜è¦
        report_lines.append("## ğŸ“‹ Executive Summary\n")
        report_lines.extend(self._generate_executive_summary(system_metrics, global_metrics))
        
        # 2. å…¨å±€æŒ‡æ ‡æ±‡æ€»
        if global_metrics:
            report_lines.append("\n## ğŸ“Š Global Metrics Summary\n")
            report_lines.extend(self._format_global_metrics(global_metrics))
        
        # 3. ç³»ç»Ÿçº§æŒ‡æ ‡è¯¦æƒ…
        if system_metrics:
            report_lines.append("\n## ğŸ” System-Level Metrics\n")
            report_lines.extend(self._format_system_metrics(system_metrics))
            
            # 4. ä»»åŠ¡æ€§èƒ½å¯¹æ¯”
            report_lines.append("\n## ğŸ“ˆ Task Performance Comparison\n")
            report_lines.extend(self._format_task_comparison(system_metrics))
            
            # 5. ç³»ç»Ÿæ€§èƒ½æ’å
            report_lines.append("\n## ğŸ† System Performance Ranking\n")
            report_lines.extend(self._format_system_ranking(system_metrics))
            
            # 6. é—®é¢˜è¯Šæ–­
            report_lines.append("\n## âš ï¸ Diagnostic Insights\n")
            report_lines.extend(self._generate_diagnostics(system_metrics))
        
        # 7. è¯¦ç»†æ•°æ®
        report_lines.append("\n## ğŸ“ˆ Detailed Data\n")
        report_lines.extend(self._format_detailed_data(system_metrics))
        
        # ä¿å­˜æŠ¥å‘Š
        filename = f"{experiment_name}_{phase}_{self.timestamp}.md"
        filepath = self.save_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"âœ… Metrics report saved to: {filepath}")
        return filepath
    
    def _generate_executive_summary(self, system_metrics: Dict, global_metrics: Dict = None) -> List[str]:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        lines = []
        
        if not system_metrics:
            lines.append("*No system metrics available for analysis.*")
            return lines
        
        # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
        num_systems = len(system_metrics)
        all_metrics = set()
        for metrics in system_metrics.values():
            all_metrics.update(metrics.keys())
        
        # è¯†åˆ«ä»»åŠ¡
        tasks = set()
        for metric in all_metrics:
            if '_' in metric:
                task = metric.split('_')[0]
                tasks.add(task)
        
        lines.append(f"**Systems Evaluated**: {num_systems}")
        lines.append(f"**Tasks Analyzed**: {', '.join(sorted(tasks))}")
        lines.append(f"**Total Metrics**: {len(all_metrics)}")
        
        # å¿«é€Ÿæ´å¯Ÿ
        lines.append(f"\n**Key Findings**:")
        
        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®ç³»ç»Ÿ
        if len(system_metrics) > 1:
            system_scores = self._calculate_system_scores(system_metrics)
            if system_scores:
                best_system = max(system_scores.items(), key=lambda x: x[1])
                worst_system = min(system_scores.items(), key=lambda x: x[1])
                lines.append(f"- ğŸ¥‡ Best performing system: **{best_system[0]}** (score: {best_system[1]:.3f})")
                lines.append(f"- ğŸ”´ Worst performing system: **{worst_system[0]}** (score: {worst_system[1]:.3f})")
        
        # æ£€æŸ¥å¼‚å¸¸æŒ‡æ ‡
        anomalies = self._detect_metric_anomalies(system_metrics)
        if anomalies:
            lines.append(f"- âš ï¸ **{len(anomalies)} anomalies detected** requiring attention")
        else:
            lines.append(f"- âœ… No significant anomalies detected")
        
        return lines
    
    def _format_global_metrics(self, metrics: Dict[str, float]) -> List[str]:
        """æ ¼å¼åŒ–å…¨å±€æŒ‡æ ‡ä¸ºMarkdownè¡¨æ ¼"""
        lines = []
        
        # æŒ‰ä»»åŠ¡åˆ†ç»„
        task_metrics = {}
        for key, value in metrics.items():
            # å°è¯•ä»keyä¸­æå–ä»»åŠ¡å
            if '_' in key:
                # å‡è®¾æ ¼å¼ä¸º task_metric æˆ– phase_task_metric
                parts = key.split('_')
                if len(parts) >= 2:
                    # æ‰¾åˆ°ä»»åŠ¡åï¼ˆè·³è¿‡phaseå‰ç¼€å¦‚'test_'ï¼‰
                    task_start = 1 if parts[0] in ['train', 'val', 'test'] else 0
                    if task_start < len(parts) - 1:
                        task = parts[task_start]
                        metric = '_'.join(parts[task_start + 1:])
                        if task not in task_metrics:
                            task_metrics[task] = {}
                        task_metrics[task][metric] = value
        
        if not task_metrics:
            # å¦‚æœæ— æ³•åˆ†ç»„ï¼Œç›´æ¥æ˜¾ç¤º
            lines.append("| Metric | Value |")
            lines.append("|--------|-------|")
            for key, value in sorted(metrics.items()):
                if isinstance(value, float):
                    lines.append(f"| {key} | {value:.4f} |")
                else:
                    lines.append(f"| {key} | {value} |")
        else:
            # åˆ›å»ºåˆ†ç»„è¡¨æ ¼
            lines.append("| Task | Metric | Value | Status |")
            lines.append("|------|--------|-------|--------|")
            
            for task, task_metrics_dict in sorted(task_metrics.items()):
                for metric, value in sorted(task_metrics_dict.items()):
                    if isinstance(value, float):
                        status = self._get_metric_status(metric, value)
                        lines.append(f"| {task} | {metric} | {value:.4f} | {status} |")
                    else:
                        lines.append(f"| {task} | {metric} | {value} | - |")
        
        return lines
    
    def _format_system_metrics(self, system_metrics: Dict[str, Dict[str, float]]) -> List[str]:
        """æ ¼å¼åŒ–ç³»ç»Ÿçº§æŒ‡æ ‡ä¸ºè¯¦ç»†è¡¨æ ¼"""
        lines = []
        
        if not system_metrics:
            lines.append("*No system-level metrics available*")
            return lines
        
        # è·å–æ‰€æœ‰æŒ‡æ ‡åç§°
        all_metrics = set()
        for sys_metrics in system_metrics.values():
            all_metrics.update(sys_metrics.keys())
        
        # æŒ‰ä»»åŠ¡åˆ†ç»„æŒ‡æ ‡
        task_groups = {}
        for metric in all_metrics:
            task = metric.split('_')[0] if '_' in metric else 'general'
            if task not in task_groups:
                task_groups[task] = []
            task_groups[task].append(metric)
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºè¡¨æ ¼
        for task, metrics in sorted(task_groups.items()):
            lines.append(f"\n### {task.capitalize()} Task Metrics\n")
            
            # è¡¨å¤´
            header = "| System ID |"
            separator = "|-----------|"
            for metric in sorted(metrics):
                metric_name = metric.replace(f"{task}_", "").replace('_', ' ').title()
                header += f" {metric_name} |"
                separator += "-----------|"
            header += " Status |"
            separator += "--------|"
            
            lines.append(header)
            lines.append(separator)
            
            # æ•°æ®è¡Œ
            for sys_id, sys_metrics in sorted(system_metrics.items()):
                row = f"| **{sys_id}** |"
                system_status = "âœ…"
                
                for metric in sorted(metrics):
                    value = sys_metrics.get(metric, 'N/A')
                    if isinstance(value, float):
                        row += f" {value:.4f} |"
                        # æ£€æŸ¥å¼‚å¸¸å€¼
                        if self._is_metric_anomaly(metric, value):
                            system_status = "âš ï¸"
                    else:
                        row += f" {value} |"
                
                row += f" {system_status} |"
                lines.append(row)
        
        return lines
    
    def _format_task_comparison(self, system_metrics: Dict[str, Dict[str, float]]) -> List[str]:
        """åˆ›å»ºä»»åŠ¡é—´æ€§èƒ½å¯¹æ¯”"""
        lines = []
        
        # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„ç»Ÿè®¡ä¿¡æ¯
        task_stats = {}
        
        for sys_metrics in system_metrics.values():
            for metric, value in sys_metrics.items():
                if isinstance(value, (int, float)) and not (np.isnan(value) or np.isinf(value)):
                    task = metric.split('_')[0] if '_' in metric else 'general'
                    if task not in task_stats:
                        task_stats[task] = []
                    task_stats[task].append(value)
        
        # è®¡ç®—ç»Ÿè®¡å€¼
        lines.append("| Task | Avg Performance | Std Dev | Min | Max | Status |")
        lines.append("|------|----------------|---------|-----|-----|--------|")
        
        for task, values in sorted(task_stats.items()):
            if values:
                avg = np.mean(values)
                std = np.std(values)
                min_val = np.min(values)
                max_val = np.max(values)
                
                # åˆ¤æ–­çŠ¶æ€
                if avg > 0.8:
                    status = "âœ… Excellent"
                elif avg > 0.6:
                    status = "ğŸŸ¡ Good"
                elif avg > 0.4:
                    status = "ğŸŸ  Fair"
                else:
                    status = "ğŸ”´ Poor"
                
                lines.append(f"| {task} | {avg:.4f} | {std:.4f} | {min_val:.4f} | {max_val:.4f} | {status} |")
        
        return lines
    
    def _format_system_ranking(self, system_metrics: Dict[str, Dict[str, float]]) -> List[str]:
        """åˆ›å»ºç³»ç»Ÿæ€§èƒ½æ’å"""
        lines = []
        
        # è®¡ç®—æ¯ä¸ªç³»ç»Ÿçš„ç»¼åˆå¾—åˆ†
        system_scores = self._calculate_system_scores(system_metrics)
        
        if not system_scores:
            lines.append("*Cannot calculate system ranking due to insufficient data*")
            return lines
        
        # æ’åº
        sorted_systems = sorted(system_scores.items(), key=lambda x: x[1], reverse=True)
        
        lines.append("| Rank | System ID | Overall Score | Performance Level |")
        lines.append("|------|-----------|---------------|-------------------|")
        
        for rank, (sys_id, score) in enumerate(sorted_systems, 1):
            # æ·»åŠ å¥–ç‰Œ
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else ""
            
            # æ€§èƒ½ç­‰çº§
            if score > 0.8:
                level = "Excellent"
            elif score > 0.6:
                level = "Good"
            elif score > 0.4:
                level = "Fair"
            else:
                level = "Needs Improvement"
            
            lines.append(f"| {medal} {rank} | **{sys_id}** | {score:.4f} | {level} |")
        
        return lines
    
    def _generate_diagnostics(self, system_metrics: Dict[str, Dict[str, float]]) -> List[str]:
        """ç”Ÿæˆè¯Šæ–­å»ºè®®"""
        lines = []
        issues = []
        recommendations = set()
        
        # æ£€æŸ¥å„ç§é—®é¢˜
        for sys_id, metrics in system_metrics.items():
            for metric, value in metrics.items():
                if not isinstance(value, (int, float)):
                    continue
                
                # æ£€æŸ¥AUROCå¼‚å¸¸
                if 'auroc' in metric.lower() and value < 0.5:
                    issues.append(f"- **System {sys_id}**: {metric} = {value:.4f} (< 0.5) - possible label inversion or severe class imbalance")
                    recommendations.add("Review anomaly detection labels and class balance")
                
                # æ£€æŸ¥R2å¼‚å¸¸
                if 'r2' in metric.lower() and value < -1:
                    issues.append(f"- **System {sys_id}**: {metric} = {value:.4f} (< -1) - poor regression performance")
                    recommendations.add("Check RUL target scaling and data quality")
                
                # æ£€æŸ¥å‡†ç¡®ç‡å¼‚å¸¸
                if 'acc' in metric.lower() and value < 0.3:
                    issues.append(f"- **System {sys_id}**: {metric} = {value:.4f} (< 0.3) - very poor classification performance")
                    recommendations.add("Review classification model and data preprocessing")
                
                # æ£€æŸ¥F1å¼‚å¸¸
                if 'f1' in metric.lower() and value < 0.2:
                    issues.append(f"- **System {sys_id}**: {metric} = {value:.4f} (< 0.2) - poor precision/recall balance")
                    recommendations.add("Address class imbalance or model calibration")
                
                # æ£€æŸ¥æå€¼
                if isinstance(value, float) and (np.isnan(value) or np.isinf(value)):
                    issues.append(f"- **System {sys_id}**: {metric} = {value} - invalid value detected")
                    recommendations.add("Investigate computational errors in metric calculation")
        
        if issues:
            lines.append("### ğŸš¨ Issues Detected:\n")
            lines.extend(issues)
            lines.append("\n### ğŸ’¡ Recommendations:\n")
            for rec in sorted(recommendations):
                lines.append(f"- {rec}")
        else:
            lines.append("### âœ… No Major Issues Detected\n")
            lines.append("All systems are performing within acceptable ranges.")
        
        # æ·»åŠ é€šç”¨å»ºè®®
        lines.append("\n### ğŸ“‹ General Recommendations:\n")
        lines.append("- Monitor system-specific trends across multiple epochs")
        lines.append("- Consider per-system hyperparameter optimization")
        lines.append("- Implement early stopping based on system-level performance")
        lines.append("- Use ensemble methods to leverage strengths of different systems")
        
        return lines
    
    def _format_detailed_data(self, system_metrics: Dict[str, Dict[str, float]]) -> List[str]:
        """æ ¼å¼åŒ–è¯¦ç»†æ•°æ®ä¸ºå¯å¯¼å‡ºæ ¼å¼"""
        lines = []
        
        lines.append("### Raw Data (CSV Format)\n")
        lines.append("```csv")
        
        if system_metrics:
            # è·å–æ‰€æœ‰æŒ‡æ ‡å
            all_metrics = set()
            for metrics in system_metrics.values():
                all_metrics.update(metrics.keys())
            
            # åˆ›å»ºCSVå¤´éƒ¨
            header = "System_ID," + ",".join(sorted(all_metrics))
            lines.append(header)
            
            # æ·»åŠ æ•°æ®è¡Œ
            for sys_id, metrics in sorted(system_metrics.items()):
                row = [sys_id]
                for metric in sorted(all_metrics):
                    value = metrics.get(metric, 'N/A')
                    if isinstance(value, float):
                        row.append(f"{value:.6f}")
                    else:
                        row.append(str(value))
                lines.append(",".join(row))
        
        lines.append("```")
        return lines
    
    def _calculate_system_scores(self, system_metrics: Dict[str, Dict[str, float]]) -> Dict[str, float]:
        """è®¡ç®—ç³»ç»Ÿç»¼åˆå¾—åˆ†"""
        system_scores = {}
        
        for sys_id, metrics in system_metrics.items():
            valid_scores = []
            
            for metric, value in metrics.items():
                if isinstance(value, (int, float)) and not (np.isnan(value) or np.isinf(value)):
                    # å¯¹ä¸åŒç±»å‹çš„æŒ‡æ ‡è¿›è¡Œå½’ä¸€åŒ–
                    if 'auroc' in metric.lower() or 'acc' in metric.lower() or 'f1' in metric.lower():
                        # è¿™äº›æŒ‡æ ‡è¶Šå¤§è¶Šå¥½ï¼ŒèŒƒå›´[0,1]
                        normalized = max(0, min(1, value))
                    elif 'loss' in metric.lower() or 'mae' in metric.lower() or 'mse' in metric.lower():
                        # è¿™äº›æŒ‡æ ‡è¶Šå°è¶Šå¥½ï¼Œä½¿ç”¨å€’æ•°
                        if value > 0:
                            normalized = 1 / (1 + value)  # èŒƒå›´[0,1]
                        else:
                            normalized = 0
                    elif 'r2' in metric.lower():
                        # RÂ²ç†è®ºä¸Šå¯ä»¥æ˜¯è´Ÿæ•°ï¼Œä½†é€šå¸¸æœŸæœ›æ­£å€¼
                        normalized = max(0, min(1, (value + 1) / 2))  # å°†[-1,1]æ˜ å°„åˆ°[0,1]
                    else:
                        # å…¶ä»–æŒ‡æ ‡å‡è®¾åœ¨[0,1]èŒƒå›´å†…
                        normalized = max(0, min(1, value))
                    
                    valid_scores.append(normalized)
            
            if valid_scores:
                system_scores[sys_id] = sum(valid_scores) / len(valid_scores)
        
        return system_scores
    
    def _detect_metric_anomalies(self, system_metrics: Dict[str, Dict[str, float]]) -> List[str]:
        """æ£€æµ‹æŒ‡æ ‡å¼‚å¸¸"""
        anomalies = []
        
        for sys_id, metrics in system_metrics.items():
            for metric, value in metrics.items():
                if self._is_metric_anomaly(metric, value):
                    anomalies.append(f"{sys_id}_{metric}")
        
        return anomalies
    
    def _is_metric_anomaly(self, metric: str, value: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¼‚å¸¸æŒ‡æ ‡"""
        if not isinstance(value, (int, float)):
            return True
        
        if np.isnan(value) or np.isinf(value):
            return True
        
        metric_lower = metric.lower()
        
        # æ£€æŸ¥å„ç§å¼‚å¸¸æ¨¡å¼
        if 'auroc' in metric_lower and value < 0.5:
            return True
        if 'r2' in metric_lower and value < -1:
            return True
        if ('acc' in metric_lower or 'f1' in metric_lower) and value < 0.3:
            return True
        if ('loss' in metric_lower or 'mae' in metric_lower or 'mse' in metric_lower) and value > 10:
            return True
        
        return False
    
    def _get_metric_status(self, metric: str, value: float) -> str:
        """è·å–æŒ‡æ ‡çŠ¶æ€"""
        if self._is_metric_anomaly(metric, value):
            return "âš ï¸"
        elif 'auroc' in metric.lower() or 'acc' in metric.lower() or 'f1' in metric.lower():
            return "âœ…" if value > 0.7 else "ğŸŸ¡" if value > 0.5 else "ğŸ”´"
        elif 'r2' in metric.lower():
            return "âœ…" if value > 0.5 else "ğŸŸ¡" if value > 0 else "ğŸ”´"
        else:
            return "âœ…"


if __name__ == '__main__':
    """å•å…ƒæµ‹è¯•"""
    print("=== Testing MetricsMarkdownReporter ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    system_metrics = {
        'system_1': {
            'classification_acc': 0.95,
            'classification_f1': 0.93,
            'anomaly_auroc': 0.87,
            'rul_mae': 0.45,
            'signal_r2': 0.78
        },
        'system_5': {
            'classification_acc': 0.85,
            'classification_f1': 0.82,
            'anomaly_auroc': 0.02,  # å¼‚å¸¸å€¼
            'rul_mae': 1.23,
            'signal_r2': -0.45
        },
        'system_13': {
            'classification_acc': 0.98,
            'classification_f1': 0.97,
            'anomaly_auroc': 0.91,
            'rul_mae': 0.34,
            'signal_r2': 0.89
        }
    }
    
    global_metrics = {
        'test_classification_acc': 0.926,
        'test_anomaly_auroc': 0.600,
        'test_rul_mae': 0.674,
        'test_signal_r2': 0.407
    }
    
    # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
    reporter = MetricsMarkdownReporter(save_dir="test_reports")
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = reporter.generate_report(
        system_metrics=system_metrics,
        global_metrics=global_metrics,
        phase='test',
        experiment_name='test_experiment',
        config_info={
            'model': 'M_01_ISFM',
            'tasks': 'classification, anomaly_detection, rul_prediction, signal_prediction',
            'batch_size': 32
        }
    )
    
    print(f"âœ… Test report generated: {report_path}")
    print("\nâœ“ MetricsMarkdownReporter test completed!")