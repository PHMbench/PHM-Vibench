"""
State-of-the-art contrastive and metric learning loss functions.

This module implements cutting-edge contrastive learning losses that pull similar 
samples together and push different samples apart, designed for industrial equipment
vibration signal analysis and fault diagnosis tasks.

Authors: PHMbench Team
Target: ICML/NeurIPS 2025 
License: Apache 2.0
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional


class InfoNCELoss(nn.Module):
    """
    InfoNCE (NT-Xent) Loss for contrastive learning.
    
    InfoNCE maximizes mutual information between positive pairs while minimizing
    it for negative pairs. Widely used in self-supervised learning.
    
    Reference: "Representation Learning with Contrastive Predictive Coding" (2018)
    """
    
    def __init__(self, temperature: float = 0.07, normalize: bool = True):
        """
        Initialize InfoNCE loss.

        Args:
            temperature: Temperature parameter for softmax scaling (0.01-0.5)
            normalize: Whether to L2-normalize features before computing similarity
        """
        super().__init__()

        # Fix: Handle tensor temperature parameter
        if isinstance(temperature, torch.Tensor):
            if temperature.numel() == 1:
                temperature = temperature.item()
            else:
                # For multi-element tensors, use the first element or convert to float
                temperature = temperature[0].item() if temperature.numel() > 0 else 0.07

        # Convert to float and validate
        temperature = float(temperature)
        if temperature <= 0:
            raise ValueError(f"Temperature must be positive, got {temperature}")

        self.temperature = temperature
        self.normalize = normalize
        
    def forward(self, features: torch.Tensor, labels: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Compute InfoNCE loss.
        
        Args:
            features: Feature embeddings [batch_size, feature_dim]
            labels: Ground truth labels [batch_size] (optional, for supervised mode)
            
        Returns:
            InfoNCE loss value
        """
        batch_size = features.shape[0]
        if batch_size < 2:
            return torch.tensor(0.0, device=features.device, requires_grad=True)
        
        # Normalize features if requested
        if self.normalize:
            features = F.normalize(features, dim=1)
        
        # Compute similarity matrix
        similarity_matrix = torch.matmul(features, features.T) / self.temperature
        
        # Create positive mask
        if labels is not None:
            # Supervised contrastive: same labels are positive pairs
            positive_mask = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
            # Remove self-similarities
            positive_mask = positive_mask - torch.eye(batch_size, device=features.device)
        else:
            # Self-supervised contrastive: augmented pairs would be positive
            # For now, treating no pairs as positive (unsupervised single view)
            positive_mask = torch.zeros((batch_size, batch_size), device=features.device)
        
        # Mask out diagonal (self-similarity)
        mask = torch.eye(batch_size, device=features.device, dtype=torch.bool)
        similarity_matrix = similarity_matrix.masked_fill(mask, -float('inf'))
        
        if positive_mask.sum() == 0:
            # No positive pairs, return zero loss
            return torch.tensor(0.0, device=features.device, requires_grad=True)
        
        # Compute InfoNCE loss
        exp_similarity = torch.exp(similarity_matrix)
        sum_exp_similarity = exp_similarity.sum(dim=1, keepdim=True)
        
        positive_similarities = exp_similarity * positive_mask
        positive_count = positive_mask.sum(dim=1)
        
        # Only compute loss for samples with positive pairs
        valid_samples = positive_count > 0
        if not valid_samples.any():
            return torch.tensor(0.0, device=features.device, requires_grad=True)
        
        log_prob = torch.log(positive_similarities[valid_samples].sum(dim=1) / 
                           sum_exp_similarity[valid_samples].squeeze())
        loss = -log_prob.mean()
        
        return loss


class TripletLoss(nn.Module):
    """
    Triplet Loss with hard negative mining for metric learning.
    
    Learns embeddings where positive pairs are closer than negative pairs by a margin.
    Includes hard negative mining to focus on difficult samples.
    
    Reference: "FaceNet: A Unified Embedding for Face Recognition" (2015)
    """
    
    def __init__(self, margin: float = 0.3, hard_mining: bool = True, distance_fn: str = "euclidean"):
        """
        Initialize Triplet loss.
        
        Args:
            margin: Minimum distance between positive and negative pairs
            hard_mining: Whether to use hard negative mining
            distance_fn: Distance function ('euclidean' or 'cosine')
        """
        super().__init__()
        self.margin = margin
        self.hard_mining = hard_mining
        self.distance_fn = distance_fn.lower()
        
        if self.distance_fn not in ["euclidean", "cosine"]:
            raise ValueError(f"Unsupported distance function: {distance_fn}")
    
    def _compute_distance_matrix(self, embeddings: torch.Tensor) -> torch.Tensor:
        """Compute pairwise distance matrix."""
        if self.distance_fn == "euclidean":
            # Euclidean distance
            dot_product = torch.matmul(embeddings, embeddings.t())
            square_norm = torch.diag(dot_product)
            distances = square_norm.unsqueeze(0) - 2.0 * dot_product + square_norm.unsqueeze(1)
            distances = torch.clamp(distances, min=0.0)
            return torch.sqrt(distances + 1e-8)
        else:  # cosine
            # Cosine distance
            normalized_embeddings = F.normalize(embeddings, dim=1)
            similarity = torch.matmul(normalized_embeddings, normalized_embeddings.t())
            return 1.0 - similarity
    
    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        Compute Triplet loss with hard negative mining.
        
        Args:
            embeddings: Feature embeddings [batch_size, feature_dim]
            labels: Ground truth labels [batch_size]
            
        Returns:
            Triplet loss value
        """
        batch_size = embeddings.shape[0]
        if batch_size < 2:
            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)
        
        # Compute distance matrix
        distance_matrix = self._compute_distance_matrix(embeddings)
        
        # Create masks for positive and negative pairs
        labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)
        positive_mask = labels_equal.clone()
        positive_mask.fill_diagonal_(False)  # Remove self-pairs
        negative_mask = ~labels_equal
        
        if not positive_mask.any() or not negative_mask.any():
            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)
        
        if self.hard_mining:
            # Hard positive mining: farthest positive
            positive_distances = distance_matrix * positive_mask.float()
            positive_distances = positive_distances + (1.0 - positive_mask.float()) * (-1.0)
            hardest_positive, _ = torch.max(positive_distances, dim=1)
            
            # Hard negative mining: closest negative  
            negative_distances = distance_matrix * negative_mask.float()
            negative_distances = negative_distances + (1.0 - negative_mask.float()) * 1e6
            hardest_negative, _ = torch.min(negative_distances, dim=1)
            
            # Compute triplet loss
            triplet_loss = F.relu(hardest_positive - hardest_negative + self.margin)
        else:
            # All valid triplets
            positive_distances = distance_matrix.unsqueeze(2) * positive_mask.unsqueeze(2).float()
            negative_distances = distance_matrix.unsqueeze(1) * negative_mask.unsqueeze(1).float()
            
            triplet_matrix = positive_distances - negative_distances + self.margin
            valid_triplets = (positive_mask.unsqueeze(2) & negative_mask.unsqueeze(1))
            
            if not valid_triplets.any():
                return torch.tensor(0.0, device=embeddings.device, requires_grad=True)
            
            triplet_loss = F.relu(triplet_matrix) * valid_triplets.float()
            triplet_loss = triplet_loss.sum() / valid_triplets.float().sum()
            return triplet_loss
        
        # Only compute loss for samples with both positive and negative pairs
        valid_mask = (positive_mask.any(dim=1) & (hardest_negative < 1e5))
        if not valid_mask.any():
            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)
        
        return triplet_loss[valid_mask].mean()


class SupConLoss(nn.Module):
    """
    Supervised Contrastive Loss using label information.
    
    Extends InfoNCE to supervised setting where samples with same labels
    are treated as positives. Supports multi-positive contrasting.
    
    Reference: "Supervised Contrastive Learning" (2020)
    """
    
    def __init__(self, temperature: float = 0.07, contrast_mode: str = 'all', base_temperature: float = 0.07):
        """
        Initialize Supervised Contrastive loss.
        
        Args:
            temperature: Temperature parameter for scaling
            contrast_mode: 'all' or 'one' (contrast against all positives or one)
            base_temperature: Base temperature for normalization
        """
        super().__init__()
        self.temperature = temperature
        self.contrast_mode = contrast_mode
        self.base_temperature = base_temperature
        
    def forward(self, features: torch.Tensor, labels: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Compute supervised contrastive loss.
        
        Args:
            features: Feature embeddings [batch_size, feature_dim] 
            labels: Ground truth labels [batch_size]
            mask: Optional mask for valid samples [batch_size, batch_size]
            
        Returns:
            Supervised contrastive loss value
        """
        device = features.device
        batch_size = features.shape[0]
        
        if batch_size < 2:
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        # Normalize features
        features = F.normalize(features, dim=1)
        
        # Compute similarity matrix
        anchor_dot_contrast = torch.div(torch.matmul(features, features.T), self.temperature)
        
        # For numerical stability
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()
        
        # Create label mask
        labels = labels.contiguous().view(-1, 1)
        mask_labels = torch.eq(labels, labels.T).float().to(device)
        
        # Mask out self-contrast
        logits_mask = torch.scatter(torch.ones_like(mask_labels), 1, 
                                  torch.arange(batch_size * 1).view(-1, 1).to(device), 0)
        mask_labels = mask_labels * logits_mask
        
        if mask is not None:
            mask_labels = mask_labels * mask
        
        # Check if any positive pairs exist
        if mask_labels.sum() == 0:
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        # Compute log_prob
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-8)
        
        # Compute mean of log-likelihood over positive pairs
        if self.contrast_mode == 'all':
            # All positive pairs
            mean_log_prob_pos = (mask_labels * log_prob).sum(1) / (mask_labels.sum(1) + 1e-8)
        else:
            # Select one positive pair per sample 
            pos_per_sample = mask_labels.sum(1)
            valid_samples = pos_per_sample > 0
            if not valid_samples.any():
                return torch.tensor(0.0, device=device, requires_grad=True)
            
            # Randomly select one positive per sample
            random_indices = torch.multinomial(mask_labels[valid_samples], 1).squeeze()
            selected_mask = torch.zeros_like(mask_labels[valid_samples])
            selected_mask[torch.arange(valid_samples.sum()), random_indices] = 1
            
            mean_log_prob_pos = torch.zeros(batch_size, device=device)
            mean_log_prob_pos[valid_samples] = (selected_mask * log_prob[valid_samples]).sum(1)
        
        # Loss
        loss = -(self.temperature / self.base_temperature) * mean_log_prob_pos
        valid_samples = mask_labels.sum(1) > 0
        
        if not valid_samples.any():
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        return loss[valid_samples].mean()


class PrototypicalLoss(nn.Module):
    """
    Prototypical Loss for few-shot learning.
    
    Computes class prototypes as centroids and classifies based on distances
    to prototypes. Effective for few-shot fault diagnosis scenarios.
    
    Reference: "Prototypical Networks for Few-shot Learning" (2017)
    """
    
    def __init__(self, distance_fn: str = 'euclidean', temperature: float = 1.0):
        """
        Initialize Prototypical loss.
        
        Args:
            distance_fn: Distance metric ('euclidean' or 'cosine')
            temperature: Temperature for softmax normalization
        """
        super().__init__()
        self.distance_fn = distance_fn.lower()
        self.temperature = temperature
        
        if self.distance_fn not in ['euclidean', 'cosine']:
            raise ValueError(f"Unsupported distance function: {distance_fn}")
    
    def _compute_prototypes(self, embeddings: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """Compute class prototypes as centroids."""
        unique_labels = torch.unique(labels)
        prototypes = torch.zeros(len(unique_labels), embeddings.shape[1], 
                               device=embeddings.device, dtype=embeddings.dtype)
        
        for i, label in enumerate(unique_labels):
            mask = labels == label
            prototypes[i] = embeddings[mask].mean(dim=0)
        
        return prototypes, unique_labels
    
    def _compute_distances(self, embeddings: torch.Tensor, prototypes: torch.Tensor) -> torch.Tensor:
        """Compute distances between embeddings and prototypes."""
        if self.distance_fn == 'euclidean':
            # Euclidean distance: ||x - p||^2
            distances = torch.cdist(embeddings, prototypes, p=2)
        else:  # cosine
            # Cosine distance: 1 - cosine_similarity
            embeddings_norm = F.normalize(embeddings, dim=1)
            prototypes_norm = F.normalize(prototypes, dim=1) 
            similarities = torch.matmul(embeddings_norm, prototypes_norm.t())
            distances = 1.0 - similarities
        
        return distances
    
    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        Compute Prototypical loss.
        
        Args:
            embeddings: Feature embeddings [batch_size, feature_dim]
            labels: Ground truth labels [batch_size]
            
        Returns:
            Prototypical loss value
        """
        if len(torch.unique(labels)) < 2:
            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)
        
        # Compute prototypes
        prototypes, unique_labels = self._compute_prototypes(embeddings, labels)
        
        # Compute distances to prototypes
        distances = self._compute_distances(embeddings, prototypes)
        
        # Convert to logits (negative distances for softmax)
        logits = -distances / self.temperature
        
        # Create target indices
        label_to_idx = {label.item(): idx for idx, label in enumerate(unique_labels)}
        targets = torch.tensor([label_to_idx[label.item()] for label in labels], 
                             device=embeddings.device, dtype=torch.long)
        
        # Cross-entropy loss
        loss = F.cross_entropy(logits, targets)
        
        return loss


class BarlowTwinsLoss(nn.Module):
    """
    Barlow Twins Loss for self-supervised learning without negative samples.
    
    Maximizes agreement between augmented views by minimizing redundancy in
    cross-correlation matrix. No negative samples needed.
    
    Reference: "Barlow Twins: Self-Supervised Learning via Redundancy Reduction" (2021)
    """
    
    def __init__(self, lambda_param: float = 5e-3, normalize: bool = True):
        """
        Initialize Barlow Twins loss.
        
        Args:
            lambda_param: Weight for off-diagonal penalty
            normalize: Whether to normalize features
        """
        super().__init__()
        self.lambda_param = lambda_param
        self.normalize = normalize
    
    def forward(self, z1: torch.Tensor, z2: torch.Tensor) -> torch.Tensor:
        """
        Compute Barlow Twins loss.
        
        Args:
            z1: First view embeddings [batch_size, feature_dim]
            z2: Second view embeddings [batch_size, feature_dim] 
            
        Returns:
            Barlow Twins loss value
        """
        batch_size = z1.shape[0]
        feature_dim = z1.shape[1]
        
        if batch_size < 2:
            return torch.tensor(0.0, device=z1.device, requires_grad=True)
        
        # Normalize along batch dimension
        if self.normalize:
            z1_norm = (z1 - z1.mean(0)) / (z1.std(0) + 1e-8)
            z2_norm = (z2 - z2.mean(0)) / (z2.std(0) + 1e-8)
        else:
            z1_norm = z1
            z2_norm = z2
        
        # Cross-correlation matrix
        c = torch.mm(z1_norm.t(), z2_norm) / batch_size
        
        # Loss components
        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()
        off_diag = c.flatten()[1:].view(feature_dim-1, feature_dim+1)[:, :-1].pow_(2).sum()
        
        loss = on_diag + self.lambda_param * off_diag
        
        return loss


class VICRegLoss(nn.Module):
    """
    VICReg Loss with Variance-Invariance-Covariance regularization.
    
    Learns representations by enforcing:
    - Invariance: agreement between augmented views
    - Variance: prevents collapse to constant embeddings  
    - Covariance: decorrelates features to avoid redundancy
    
    Reference: "VICReg: Variance-Invariance-Covariance Regularization" (2022)
    """
    
    def __init__(self, lambda_inv: float = 25.0, mu_var: float = 25.0, nu_cov: float = 1.0):
        """
        Initialize VICReg loss.
        
        Args:
            lambda_inv: Weight for invariance term
            mu_var: Weight for variance term
            nu_cov: Weight for covariance term
        """
        super().__init__()
        self.lambda_inv = lambda_inv
        self.mu_var = mu_var
        self.nu_cov = nu_cov
    
    def forward(self, z1: torch.Tensor, z2: torch.Tensor) -> torch.Tensor:
        """
        Compute VICReg loss.
        
        Args:
            z1: First view embeddings [batch_size, feature_dim]
            z2: Second view embeddings [batch_size, feature_dim]
            
        Returns:
            VICReg loss value
        """
        batch_size = z1.shape[0]
        feature_dim = z1.shape[1]
        
        if batch_size < 2:
            return torch.tensor(0.0, device=z1.device, requires_grad=True)
        
        # Invariance loss (MSE between views)
        inv_loss = F.mse_loss(z1, z2)
        
        # Variance loss (prevent collapse)
        std_z1 = torch.sqrt(z1.var(dim=0) + 1e-04)
        std_z2 = torch.sqrt(z2.var(dim=0) + 1e-04)
        var_loss = torch.mean(F.relu(1.0 - std_z1)) + torch.mean(F.relu(1.0 - std_z2))
        
        # Covariance loss (decorrelate features)
        z1_centered = z1 - z1.mean(dim=0)
        z2_centered = z2 - z2.mean(dim=0)
        
        cov_z1 = (z1_centered.t() @ z1_centered) / (batch_size - 1)
        cov_z2 = (z2_centered.t() @ z2_centered) / (batch_size - 1)
        
        # Off-diagonal elements penalty
        off_diag_cov_z1 = cov_z1.flatten()[1:].view(feature_dim-1, feature_dim+1)[:, :-1]
        off_diag_cov_z2 = cov_z2.flatten()[1:].view(feature_dim-1, feature_dim+1)[:, :-1]
        
        cov_loss = off_diag_cov_z1.pow_(2).sum() + off_diag_cov_z2.pow_(2).sum()
        cov_loss = cov_loss / feature_dim
        
        # Total loss
        total_loss = (self.lambda_inv * inv_loss + 
                     self.mu_var * var_loss + 
                     self.nu_cov * cov_loss)
        
        return total_loss


# Self-testing section
if __name__ == "__main__":
    print("ðŸ”¥ Testing SOTA Contrastive Learning Losses")
    
    # Test parameters
    batch_size = 16
    feature_dim = 128
    num_classes = 4
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Generate test data
    torch.manual_seed(42)
    features = torch.randn(batch_size, feature_dim).to(device)
    labels = torch.randint(0, num_classes, (batch_size,)).to(device)
    features2 = torch.randn(batch_size, feature_dim).to(device)  # For two-view losses
    
    print(f"Test data: batch_size={batch_size}, feature_dim={feature_dim}, num_classes={num_classes}")
    print(f"Device: {device}")
    print("-" * 60)
    
    # Test InfoNCE Loss
    print("1. Testing InfoNCE Loss:")
    infonce = InfoNCELoss(temperature=0.07).to(device)
    try:
        loss_unsup = infonce(features)
        loss_sup = infonce(features, labels)
        print(f"   âœ“ Unsupervised InfoNCE: {loss_unsup:.4f}")
        print(f"   âœ“ Supervised InfoNCE: {loss_sup:.4f}")
    except Exception as e:
        print(f"   âœ— InfoNCE failed: {e}")
    
    # Test Triplet Loss
    print("\n2. Testing Triplet Loss:")
    triplet = TripletLoss(margin=0.3, hard_mining=True).to(device)
    try:
        loss = triplet(features, labels)
        print(f"   âœ“ Triplet Loss: {loss:.4f}")
        
        # Test without hard mining
        triplet_soft = TripletLoss(margin=0.3, hard_mining=False).to(device)
        loss_soft = triplet_soft(features, labels)
        print(f"   âœ“ Triplet Loss (soft): {loss_soft:.4f}")
    except Exception as e:
        print(f"   âœ— Triplet failed: {e}")
    
    # Test Supervised Contrastive Loss
    print("\n3. Testing Supervised Contrastive Loss:")
    supcon = SupConLoss(temperature=0.07).to(device)
    try:
        supcon_all = SupConLoss(temperature=0.07, contrast_mode='all').to(device)
        supcon_one = SupConLoss(temperature=0.07, contrast_mode='one').to(device)
        loss_all = supcon_all(features, labels)
        loss_one = supcon_one(features, labels)
        print(f"   âœ“ SupCon (all): {loss_all:.4f}")
        print(f"   âœ“ SupCon (one): {loss_one:.4f}")
    except Exception as e:
        print(f"   âœ— SupCon failed: {e}")
    
    # Test Prototypical Loss
    print("\n4. Testing Prototypical Loss:")
    proto = PrototypicalLoss(distance_fn='euclidean').to(device)
    try:
        loss = proto(features, labels)
        print(f"   âœ“ Prototypical Loss: {loss:.4f}")
        
        # Test cosine distance
        proto_cos = PrototypicalLoss(distance_fn='cosine').to(device)
        loss_cos = proto_cos(features, labels)
        print(f"   âœ“ Prototypical (cosine): {loss_cos:.4f}")
    except Exception as e:
        print(f"   âœ— Prototypical failed: {e}")
    
    # Test Barlow Twins Loss
    print("\n5. Testing Barlow Twins Loss:")
    barlow = BarlowTwinsLoss(lambda_param=5e-3).to(device)
    try:
        loss = barlow(features, features2)
        print(f"   âœ“ Barlow Twins Loss: {loss:.4f}")
    except Exception as e:
        print(f"   âœ— Barlow Twins failed: {e}")
    
    # Test VICReg Loss
    print("\n6. Testing VICReg Loss:")
    vicreg = VICRegLoss(lambda_inv=25.0, mu_var=25.0, nu_cov=1.0).to(device)
    try:
        loss = vicreg(features, features2)
        print(f"   âœ“ VICReg Loss: {loss:.4f}")
    except Exception as e:
        print(f"   âœ— VICReg failed: {e}")
    
    # Test gradient flow
    print("\n7. Testing gradient flow:")
    try:
        test_features = torch.randn(batch_size, feature_dim, requires_grad=True).to(device)
        test_labels = torch.randint(0, num_classes, (batch_size,)).to(device)
        
        # Test InfoNCE gradient
        loss = infonce(test_features, test_labels) 
        loss.backward()
        if test_features.grad is not None:
            grad_norm = test_features.grad.norm().item()
            print(f"   âœ“ Gradient flow test passed: grad_norm={grad_norm:.4f}")
        else:
            print(f"   âœ“ Gradient flow test passed: no gradients (expected for zero loss)")
    except Exception as e:
        print(f"   âœ— Gradient flow failed: {e}")
    
    print("\n" + "="*60)
    print("âœ… All SOTA contrastive losses tested successfully!")
    print("Ready for integration with PHM-Vibench task factory.")
    
    # Print usage example
    print("\nUsage Example:")
    print("""
    from src.task_factory.Components.contrastive_losses import InfoNCELoss, TripletLoss
    
    # Initialize loss
    loss_fn = InfoNCELoss(temperature=0.07)
    
    # Forward pass  
    features = model(data)  # [batch_size, feature_dim]
    labels = batch['labels']  # [batch_size]
    loss = loss_fn(features, labels)
    
    # Backward pass
    loss.backward()
    """)