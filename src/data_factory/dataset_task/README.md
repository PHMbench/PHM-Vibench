-----

# dataset_task Module

This folder hosts **task-oriented dataset wrappers**.  
They sit between the raw data readers (`reader/`) and the task modules (`src/task_factory/task/`), and are responsible for:

- Converting `H5DataDict` + `metadata` into `torch.utils.data.Dataset` objects.
- Shaping each batch so that it matches the expectations of the corresponding task in `task_factory`.
- Encapsulating task-specific windowing, normalization, and sampling strategies.

> ç®€å•ç†è§£ï¼šä¸åŒçš„ `task.type` / `task.name` å¯¹åº”ä¸åŒçš„ `LightningModule`ï¼Œ  
> è€Œè¿™é‡Œçš„ `dataset_task` åˆ™è´Ÿè´£æŠŠåº•å±‚ä¿¡å·æ•°æ®æ•´ç†æˆè¯¥ä»»åŠ¡æ‰€éœ€çš„ batch ç»“æ„ã€‚

-----

## ğŸ“‚ Directory Overview

Key files and directories:

| Path                        | Description |
| :-------------------------- | :---------- |
| `Default_dataset.py`        | Generic window-based dataset (sliding windows, normalization, optional noise). Most specialized datasets subclass this one. |
| `Dataset_cluster.py`        | Wraps per-ID datasets into an `IdIncludedDataset` cluster used by samplers and data factory. |
| `DG/Classification_dataset.py` | Dataset for domain generalization classification (`task.type: DG`, `task.name: classification`). |
| `CDDG/classification_dataset.py` | Dataset for cross-dataset domain generalization classification (`task.type: CDDG`). |
| `Pretrain/Classification_dataset.py` | Dataset for pretraining tasks that still use supervised labels or masked prediction (`task.type: pretrain`). |
| `FS/Classification_dataset.py` | Window-based few-shot dataset (per-sample view) for FS tasks. |
| `FS/Episode_dataset.py`     | Episode-style few-shot dataset (support/query episodic batch). |
| `GFS/Classification_dataset.py` | Dataset for generalized few-shot classification (`task.type: GFS`). |
| `ID/Classification_dataset.py` | Dataset for ID-style tasks, aligned with `ID_task`. |
| `ID_dataset.py`             | ID-centric dataset used by `id_data_factory`, focusing on raw ID access. |

At runtime, `data_factory` chooses the dataset class via:

```python
mod = importlib.import_module(
    f"src.data_factory.dataset_task.{task_type}.{task_name}_dataset"
)
dataset_cls = mod.set_dataset
```

So the key mapping is driven by the same `task.type` / `task.name` pair that `task_factory` uses.

-----

## ğŸ”— Mapping: task.type / task.name â†’ dataset_task

The following CSV captures the recommended mappings between tasks and dataset wrappers.  
Each row corresponds to one combination of `task.type` and `task.name`, and the dataset under `dataset_task/` that is intended to feed that task.

```csv
id,task.type,task.name,path,args,batch_format,test_status
1,DG,classification,dataset_task/DG/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","{'x','y','file_id',...}",
2,CDDG,classification,dataset_task/CDDG/classification_dataset.py,"(data, metadata, args_data, args_task, mode)","{'x','y','file_id','domain_id',...}",
3,pretrain,classification,dataset_task/Pretrain/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","{'x','y','file_id',...}",
4,pretrain,hse_contrastive,dataset_task/Pretrain/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","{'x','y','file_id','domain_id',...}",
5,pretrain,masked_reconstruction,dataset_task/Pretrain/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","{'x','mask','file_id',...}",
6,pretrain,prediction,dataset_task/Pretrain/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","{'x','y','file_id',...}",
7,pretrain,classification_prediction,dataset_task/Pretrain/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","{'x','y','file_id',...}",
8,FS,prototypical_network,dataset_task/FS/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","per-sample few-shot view; sampler builds episodes",
9,FS,matching_network,dataset_task/FS/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","per-sample few-shot view; sampler builds episodes",
10,FS,knn_feature,dataset_task/FS/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","per-sample feature view for kNN",
11,FS,finetuning,dataset_task/FS/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","standard supervised few-shot finetuning batches",
12,GFS,classification,dataset_task/GFS/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","{'x','y','base/novel flags',...}",
13,GFS,matching,dataset_task/GFS/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","GFS-style episodes / batches",
14,Default_task,Default_task,dataset_task/Default_dataset.py,"(data, metadata, args_data, args_task, mode)","Default windows: {'x','y'}",
15,Default_task,ID_task,dataset_task/ID/Classification_dataset.py,"(data, metadata, args_data, args_task, mode)","ID-based windows: {'x','y','file_id',...}",
```

- `args` åˆ—ä¸ºå½“å‰å®ç°ä¸­çš„æ„é€ å‡½æ•°ç­¾åï¼Œä¾¿äºä½ åœ¨é˜…è¯»ä»£ç æ—¶å¿«é€Ÿå¯¹é½ã€‚
- `batch_format` åªç»™å‡ºä¸€ä¸ªç®€è¦çš„ç»“æ„æç¤ºï¼›è¯¦ç»†å­—æ®µè¯·æŸ¥å¯¹åº” task çš„ READMEï¼Œä¾‹å¦‚ï¼š
  - `src/task_factory/task/pretrain/README.md`
  - `src/task_factory/task/FS/README.md`
  - `src/task_factory/task/GFS/README.md`
- `test_status` ç•™ç©ºï¼Œæ–¹ä¾¿ä½ æ‰‹åŠ¨ç»´æŠ¤æ¯ä¸ªç»„åˆçš„æµ‹è¯•ç»“æœï¼ˆå¦‚ `passed` / `failed` / `not_tested`ï¼‰ã€‚

-----

## ğŸ§© How it works with `data_factory`

1. `data_factory` ä¾æ® `args_task` è¿‡æ»¤å‡ºéœ€è¦çš„ `Id`ï¼ˆ`search_dataset_id` / `search_ids_for_task`ï¼‰ã€‚
2. é€šè¿‡ reader å’Œç¼“å­˜ (`H5DataDict`) å‡†å¤‡å¥½åŸå§‹ä¿¡å·çŸ©é˜µã€‚
3. æ ¹æ® `task.type` / `task.name` å¯¼å…¥ä¸Šè¡¨å¯¹åº”çš„ `set_dataset` å¹¶å®ä¾‹åŒ–ï¼š

```python
dataset_cls = set_dataset  # imported from dataset_task/{task.type}/{task.name}_dataset.py
train_dataset[id] = dataset_cls({id: self.data[id]}, self.target_metadata, self.args_data, self.args_task, 'train')
```

4. ä½¿ç”¨ `IdIncludedDataset` + è‡ªå®šä¹‰ sampler ç»„åˆæˆæœ€ç»ˆçš„ `DataLoader`ï¼Œä¾› `task_factory` æ„å»ºçš„ä»»åŠ¡æ¨¡å—æ¶ˆè´¹ã€‚

-----

## ğŸ†• Adding a New dataset_task

When you introduce a new `task.type` / `task.name` pair on the task side:

1. Decide the batch structure that the new task expects.
2. Implement a new dataset under `dataset_task/{task.type}/{task.name}_dataset.py` exposing `set_dataset`.
3. Ensure its `__init__` signature follows the existing pattern:  
   `(data, metadata, args_data, args_task, mode="train")`.
4. Add a new row into:
   - `src/task_factory/task_type_name_mapping.csv`
   - `src/data_factory/dataset_task/dataset_task_mapping.csv` (see below)

This keeps the config â†’ task â†’ dataset pipeline explicit and traceable.

-----

