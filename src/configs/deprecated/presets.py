"""
PHM-Vibenché…ç½®é¢„è®¾é›†åˆ
======================

æä¾›é¢å‘ä¸åŒä½¿ç”¨åœºæ™¯çš„é…ç½®æ¨¡æ¿ï¼š
- ğŸš€ quickstart: 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹
- ğŸ—ï¸ basic: åŸºç¡€ç ”ç©¶é…ç½®
- ğŸ§  isfm: ISFMåŸºç¡€æ¨¡å‹é…ç½® 
- ğŸ”¬ research: æ·±åº¦ç ”ç©¶é…ç½®
- ğŸ­ production: ç”Ÿäº§ç¯å¢ƒé…ç½®
- ğŸ“Š benchmark: åŸºå‡†æµ‹è¯•é…ç½®

ä½¿ç”¨æ–¹å¼ï¼š
    from src.configs.presets import get_preset_config
    
    config = get_preset_config("quickstart", 
                              model__d_model=256,
                              trainer__num_epochs=100)

ä½œè€…: PHM-Vibench Team
"""

from typing import Dict, Any, Optional
from .config_schema import (
    PHMConfig, 
    EnvironmentConfig,
    DataConfig, 
    ModelConfig,
    TaskConfig,
    TrainerConfig
)


# ==================== é¢„è®¾é…ç½®å®šä¹‰ ====================

def get_quickstart_config(**overrides) -> PHMConfig:
    """
    å¿«é€Ÿå¼€å§‹é…ç½® - 5åˆ†é’Ÿä¸Šæ‰‹PHM-Vibench
    
    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ç®€å•çš„ResNet1Dæ¨¡å‹
    - å°æ•°æ®é›†å’ŒçŸ­è®­ç»ƒæ—¶é—´
    - ç¦ç”¨å¤æ‚åŠŸèƒ½ï¼Œä¸“æ³¨æ ¸å¿ƒæµç¨‹
    """
    return PHMConfig(
        environment=EnvironmentConfig(
            experiment_name="quickstart_demo",
            project="phm_quickstart", 
            notes="å¿«é€Ÿå¼€å§‹ç¤ºä¾‹ï¼Œå±•ç¤ºåŸºæœ¬åŠŸèƒ½",
            seed=42,
            iterations=1,
            wandb=False,
            swanlab=False,
            WANDB_MODE="disabled"
        ),
        data=DataConfig(
            data_dir="./data",
            metadata_file="metadata_dummy.csv",  # ä½¿ç”¨dummyæ•°æ®
            batch_size=16,  # å°æ‰¹æ¬¡ç”¨äºå¿«é€Ÿè¿è¡Œ
            num_workers=2,
            pin_memory=True,
            train_ratio=0.7,
            normalization=True,
            window_size=512,  # è¾ƒå°çª—å£
            stride=256
        ),
        model=ModelConfig(
            name="ResNet1D",
            type="CNN",
            input_dim=1,
            num_classes=4,
            depth=18,
            in_channels=1,
            dropout=0.1
        ),
        task=TaskConfig(
            name="classification",
            type="DG",
            target_system_id=[1],
            epochs=10,  # å¿«é€Ÿè®­ç»ƒ
            lr=0.001,
            optimizer="adam",
            loss="CE",
            metrics=["acc"],
            early_stopping=True,
            es_patience=5
        ),
        trainer=TrainerConfig(
            name="Default_trainer",
            num_epochs=10,
            gpus=1,
            device="auto",
            early_stopping=True,
            patience=5,
            wandb=False,
            mixed_precision=False  # ç®€åŒ–è®¾ç½®
        ),
        **overrides
    )


def get_basic_config(**overrides) -> PHMConfig:
    """
    åŸºç¡€é…ç½® - æ ‡å‡†ç ”ç©¶è®¾ç½®
    
    ç‰¹ç‚¹ï¼š
    - å¹³è¡¡çš„å‚æ•°è®¾ç½®
    - é€‚ä¸­çš„è®­ç»ƒæ—¶é—´
    - åŒ…å«å¸¸ç”¨åŠŸèƒ½
    """
    return PHMConfig(
        environment=EnvironmentConfig(
            experiment_name="basic_experiment",
            project="phm_basic",
            notes="åŸºç¡€å®éªŒé…ç½®",
            seed=42,
            iterations=1,
            wandb=True,  # å¯ç”¨å®éªŒè·Ÿè¸ª
            WANDB_MODE="online"
        ),
        data=DataConfig(
            data_dir="./data",
            metadata_file="metadata.xlsx",
            batch_size=32,
            num_workers=4,
            pin_memory=True,
            persistent_workers=True,
            train_ratio=0.7,
            normalization=True,
            window_size=1024,
            stride=512
        ),
        model=ModelConfig(
            name="ResNet1D", 
            type="CNN",
            input_dim=1,
            num_classes=10,
            depth=18,
            in_channels=1,
            dropout=0.1
        ),
        task=TaskConfig(
            name="classification",
            type="DG", 
            epochs=50,
            lr=0.001,
            weight_decay=0.0001,
            optimizer="adam",
            loss="CE",
            metrics=["acc", "f1"],
            scheduler=True,
            scheduler_type="step",
            step_size=20,
            gamma=0.5,
            early_stopping=True,
            es_patience=10
        ),
        trainer=TrainerConfig(
            name="Default_trainer",
            num_epochs=50,
            gpus=1,
            device="auto",
            mixed_precision=True,  # å¯ç”¨æ··åˆç²¾åº¦
            gradient_clip_val=1.0,
            early_stopping=True,
            patience=10,
            wandb=True,
            save_top_k=3
        ),
        **overrides
    )


def get_isfm_config(**overrides) -> PHMConfig:
    """
    ISFMåŸºç¡€æ¨¡å‹é…ç½® - å·¥ä¸šä¿¡å·åŸºç¡€æ¨¡å‹
    
    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ISFMæ¶æ„
    - Transformeréª¨å¹²ç½‘ç»œ
    - å¤šä»»åŠ¡å¤´æ”¯æŒ
    """
    return PHMConfig(
        environment=EnvironmentConfig(
            experiment_name="isfm_experiment",
            project="phm_isfm",
            notes="ISFMåŸºç¡€æ¨¡å‹å®éªŒ",
            seed=42,
            iterations=1,
            wandb=True,
            WANDB_MODE="online"
        ),
        data=DataConfig(
            data_dir="./data",
            metadata_file="metadata.xlsx", 
            batch_size=32,
            num_workers=8,
            pin_memory=True,
            persistent_workers=True,
            train_ratio=0.7,
            normalization=True,
            window_size=1024,
            stride=512
        ),
        model=ModelConfig(
            name="M_01_ISFM",  # æ¨èçš„ISFMç‰ˆæœ¬
            type="ISFM",
            
            # ISFMç»„ä»¶
            embedding="E_01_HSE",
            backbone="B_08_PatchTST", 
            task_head="H_01_Linear_cla",
            
            # æ¨¡å‹å‚æ•°
            input_dim=1,
            d_model=128,
            num_heads=8,
            num_layers=6,
            d_ff=512,
            dropout=0.1,
            
            # Patchå‚æ•°
            patch_size_L=16,
            patch_size_C=1,
            num_patches=64,
            output_dim=128
        ),
        task=TaskConfig(
            name="classification",
            type="DG",
            epochs=100,  # ISFMé€šå¸¸éœ€è¦æ›´å¤šè®­ç»ƒ
            lr=0.0001,   # è¾ƒå°å­¦ä¹ ç‡
            weight_decay=0.0001,
            optimizer="adam",
            loss="CE",
            metrics=["acc", "f1", "precision", "recall"],
            scheduler=True,
            scheduler_type="cosine",
            early_stopping=True,
            es_patience=20
        ),
        trainer=TrainerConfig(
            name="Default_trainer",
            num_epochs=100,
            gpus=1,
            device="auto",
            mixed_precision=True,
            gradient_clip_val=1.0,
            accumulate_grad_batches=1,
            early_stopping=True,
            patience=20,
            wandb=True,
            save_top_k=5,
            monitor_metric="val_acc",
            mode="max"
        ),
        **overrides
    )


def get_research_config(**overrides) -> PHMConfig:
    """
    ç ”ç©¶é…ç½® - æ·±åº¦ç ”ç©¶è®¾ç½®
    
    ç‰¹ç‚¹ï¼š
    - å¤šæ¬¡è¿è¡Œç”¨äºç»Ÿè®¡
    - å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—
    - é«˜çº§åŠŸèƒ½å¯ç”¨
    """
    return PHMConfig(
        environment=EnvironmentConfig(
            experiment_name="research_experiment",
            project="phm_research",
            notes="æ·±åº¦ç ”ç©¶å®éªŒï¼Œå¤šæ¬¡è¿è¡Œç»Ÿè®¡ç»“æœ",
            seed=42,
            iterations=5,  # å¤šæ¬¡è¿è¡Œ
            wandb=True,
            swanlab=True,  # åŒé‡ç›‘æ§
            WANDB_MODE="online"
        ),
        data=DataConfig(
            data_dir="./data",
            metadata_file="metadata.xlsx",
            batch_size=64,  # æ›´å¤§æ‰¹æ¬¡
            num_workers=8,
            pin_memory=True,
            persistent_workers=True,
            train_ratio=0.7,
            val_ratio=0.15,
            normalization="standardization",
            window_size=2048,
            stride=1024
        ),
        model=ModelConfig(
            name="M_02_ISFM",  # ä½¿ç”¨å¢å¼ºç‰ˆISFM
            type="ISFM",
            
            embedding="E_02_HSE_v2", 
            backbone="B_08_PatchTST",
            task_head="H_09_multiple_task",  # å¤šä»»åŠ¡å¤´
            
            d_model=256,    # æ›´å¤§æ¨¡å‹
            num_heads=16,
            num_layers=12,
            d_ff=1024,
            dropout=0.1,
            
            patch_size_L=32,
            num_patches=64,
            output_dim=256
        ),
        task=TaskConfig(
            name="classification",
            type="CDDG",  # è·¨æ•°æ®é›†æ³›åŒ–
            epochs=200,
            lr=0.0001,
            weight_decay=0.0001,
            optimizer="adamw",  # æ›´å¥½çš„ä¼˜åŒ–å™¨
            loss="CE",
            metrics=["acc", "f1", "precision", "recall", "auc"],
            scheduler=True,
            scheduler_type="cosine",
            early_stopping=True,
            es_patience=30
        ),
        trainer=TrainerConfig(
            name="Default_trainer",
            num_epochs=200,
            gpus=1,
            device="auto",
            mixed_precision=True,
            gradient_clip_val=0.5,
            accumulate_grad_batches=2,
            early_stopping=True,
            patience=30,
            wandb=True,
            save_top_k=10,
            log_every_n_steps=20,
            val_check_interval=0.5,  # æ›´é¢‘ç¹éªŒè¯
            profiler="simple"  # æ€§èƒ½åˆ†æ
        ),
        **overrides
    )


def get_production_config(**overrides) -> PHMConfig:
    """
    ç”Ÿäº§é…ç½® - ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–
    
    ç‰¹ç‚¹ï¼š
    - æ€§èƒ½ä¼˜åŒ–è®¾ç½®
    - ç¨³å®šæ€§ä¼˜å…ˆ
    - èµ„æºé«˜æ•ˆåˆ©ç”¨
    """
    return PHMConfig(
        environment=EnvironmentConfig(
            experiment_name="production_experiment",
            project="phm_production",
            notes="ç”Ÿäº§ç¯å¢ƒé…ç½®ï¼Œä¼˜åŒ–æ€§èƒ½å’Œç¨³å®šæ€§",
            seed=42,
            iterations=1,
            wandb=False,  # ç”Ÿäº§ç¯å¢ƒé€šå¸¸ä¸éœ€è¦
            WANDB_MODE="disabled"
        ),
        data=DataConfig(
            data_dir="./data",
            metadata_file="metadata.xlsx",
            batch_size=128,  # å¤§æ‰¹æ¬¡æé«˜æ•ˆç‡
            num_workers=16,  # å¤šè¿›ç¨‹åŠ è½½
            pin_memory=True,
            persistent_workers=True,
            train_ratio=0.8,  # æ›´å¤šè®­ç»ƒæ•°æ®
            normalization=True,
            window_size=1024,
            stride=512
        ),
        model=ModelConfig(
            name="M_01_ISFM",  # ç¨³å®šç‰ˆæœ¬
            type="ISFM",
            
            embedding="E_01_HSE",
            backbone="B_04_Dlinear",  # é«˜æ•ˆéª¨å¹²
            task_head="H_01_Linear_cla",
            
            d_model=128,  # å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
            num_heads=8,
            num_layers=6,
            dropout=0.0,  # ç”Ÿäº§ç¯å¢ƒä¸éœ€è¦dropout
            
            patch_size_L=16,
            num_patches=32,
            output_dim=128
        ),
        task=TaskConfig(
            name="classification",
            type="DG",
            epochs=50,
            lr=0.001,
            weight_decay=0.0001,
            optimizer="adam",
            loss="CE",
            metrics=["acc"],  # ç®€åŒ–æŒ‡æ ‡
            scheduler=False,  # ç®€åŒ–è°ƒåº¦
            early_stopping=False,  # å®Œæ•´è®­ç»ƒ
            shuffle=True
        ),
        trainer=TrainerConfig(
            name="Default_trainer",
            num_epochs=50,
            gpus=1,
            device="auto",
            mixed_precision=True,  # æ€§èƒ½ä¼˜åŒ–
            gradient_clip_val=None,  # ç®€åŒ–è®¾ç½®
            accumulate_grad_batches=1,
            early_stopping=False,
            wandb=False,
            save_top_k=1,  # åªä¿å­˜æœ€ä½³æ¨¡å‹
            enable_progress_bar=False,  # å‡å°‘è¾“å‡º
            log_every_n_steps=100
        ),
        **overrides
    )


def get_benchmark_config(**overrides) -> PHMConfig:
    """
    åŸºå‡†æµ‹è¯•é…ç½® - æ ‡å‡†è¯„ä¼°è®¾ç½®
    
    ç‰¹ç‚¹ï¼š
    - æ ‡å‡†åŒ–å‚æ•°
    - å…¬å¹³æ¯”è¾ƒè®¾ç½®
    - å¤šæŒ‡æ ‡è¯„ä¼°
    """
    return PHMConfig(
        environment=EnvironmentConfig(
            experiment_name="benchmark_experiment", 
            project="phm_benchmark",
            notes="åŸºå‡†æµ‹è¯•é…ç½®ï¼Œç”¨äºæ¨¡å‹å¯¹æ¯”",
            seed=42,
            iterations=3,  # å¤šæ¬¡è¿è¡Œæ±‚å‡å€¼
            wandb=True,
            WANDB_MODE="online"
        ),
        data=DataConfig(
            data_dir="./data",
            metadata_file="metadata.xlsx",
            batch_size=32,  # æ ‡å‡†æ‰¹æ¬¡
            num_workers=4,
            pin_memory=True,
            train_ratio=0.7,
            val_ratio=0.15,
            normalization="standardization",  # æ ‡å‡†å½’ä¸€åŒ–
            window_size=1024,
            stride=512
        ),
        model=ModelConfig(
            # æ¨¡å‹å‚æ•°å°†è¢«è¦†ç›–ï¼Œè¿™é‡Œæä¾›é»˜è®¤å€¼
            name="ResNet1D",
            type="CNN",
            input_dim=1,
            dropout=0.1
        ),
        task=TaskConfig(
            name="classification",
            type="DG",
            epochs=100,  # å……åˆ†è®­ç»ƒ
            lr=0.001,
            weight_decay=0.0001,
            optimizer="adam",
            loss="CE",
            metrics=["acc", "f1", "precision", "recall", "auc"],  # å®Œæ•´æŒ‡æ ‡
            scheduler=True,
            scheduler_type="step",
            step_size=30,
            gamma=0.1,
            early_stopping=True,
            es_patience=20
        ),
        trainer=TrainerConfig(
            name="Default_trainer",
            num_epochs=100,
            gpus=1,
            device="auto",
            mixed_precision=True,
            gradient_clip_val=1.0,
            early_stopping=True,
            patience=20,
            wandb=True,
            save_top_k=5,
            monitor_metric="val_f1",
            mode="max"
        ),
        **overrides
    )


# ==================== å¤šä»»åŠ¡å’Œç‰¹æ®Šé…ç½® ====================

def get_multitask_config(**overrides) -> PHMConfig:
    """å¤šä»»åŠ¡å­¦ä¹ é…ç½®"""
    base_config = get_isfm_config()
    
    # æ›´æ–°ä¸ºå¤šä»»åŠ¡è®¾ç½®
    multitask_overrides = {
        'environment__experiment_name': 'multitask_experiment',
        'environment__notes': 'å¤šä»»åŠ¡å­¦ä¹ å®éªŒ',
        'model__task_head': 'H_09_multiple_task',
        'task__name': 'multitask',
        'task__type': 'Multitask', 
        'task__task_list': ['classification', 'prediction'],
        'task__loss_weights': {'classification': 1.0, 'prediction': 0.5},
        'trainer__num_epochs': 150
    }
    multitask_overrides.update(overrides)
    
    return PHMConfig(**{**base_config.dict(), **_flatten_dict(multitask_overrides)})


def get_fewshot_config(**overrides) -> PHMConfig:
    """å°‘æ ·æœ¬å­¦ä¹ é…ç½®"""  
    base_config = get_basic_config()
    
    fewshot_overrides = {
        'environment__experiment_name': 'fewshot_experiment',
        'environment__notes': 'å°‘æ ·æœ¬å­¦ä¹ å®éªŒ',
        'model__name': 'ProtoNet',
        'model__type': 'FewShot',
        'task__name': 'classification',
        'task__type': 'FS',
        'task__num_support': 5,
        'task__num_query': 15,
        'task__num_episodes': 1000,
        'task__epochs': 200,
        'trainer__num_epochs': 200
    }
    fewshot_overrides.update(overrides)
    
    return PHMConfig(**{**base_config.dict(), **_flatten_dict(fewshot_overrides)})


# ==================== é¢„è®¾ç®¡ç† ====================

PRESET_CONFIGS = {
    'quickstart': get_quickstart_config,
    'basic': get_basic_config,
    'isfm': get_isfm_config,
    'research': get_research_config,
    'production': get_production_config,
    'benchmark': get_benchmark_config,
    'multitask': get_multitask_config,
    'fewshot': get_fewshot_config
}


def get_preset_config(preset_name: str, **overrides) -> PHMConfig:
    """
    è·å–é¢„è®¾é…ç½®
    
    Args:
        preset_name: é¢„è®¾åç§°
        **overrides: è¦†ç›–å‚æ•°ï¼ˆæ”¯æŒåŒä¸‹åˆ’çº¿è¯­æ³•ï¼‰
        
    Returns:
        PHMConfig: é…ç½®å¯¹è±¡
    """
    if preset_name not in PRESET_CONFIGS:
        available = list(PRESET_CONFIGS.keys())
        raise ValueError(f"æœªçŸ¥é¢„è®¾ '{preset_name}'ï¼Œå¯ç”¨é¢„è®¾: {available}")
    
    config_func = PRESET_CONFIGS[preset_name]
    return config_func(**overrides)


def list_presets() -> Dict[str, str]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨é¢„è®¾åŠå…¶æè¿°"""
    descriptions = {
        'quickstart': 'ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹ï¼Œä½¿ç”¨ç®€å•æ¨¡å‹å’Œå°æ•°æ®é›†',
        'basic': 'ğŸ—ï¸ åŸºç¡€ç ”ç©¶é…ç½®ï¼Œå¹³è¡¡çš„å‚æ•°è®¾ç½®',
        'isfm': 'ğŸ§  ISFMåŸºç¡€æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨Transformeræ¶æ„',
        'research': 'ğŸ”¬ æ·±åº¦ç ”ç©¶é…ç½®ï¼Œå¤šæ¬¡è¿è¡Œå’Œå®Œæ•´ç›‘æ§', 
        'production': 'ğŸ­ ç”Ÿäº§ç¯å¢ƒé…ç½®ï¼Œæ€§èƒ½å’Œç¨³å®šæ€§ä¼˜åŒ–',
        'benchmark': 'ğŸ“Š åŸºå‡†æµ‹è¯•é…ç½®ï¼Œæ ‡å‡†åŒ–è¯„ä¼°è®¾ç½®',
        'multitask': 'ğŸ¯ å¤šä»»åŠ¡å­¦ä¹ é…ç½®ï¼ŒåŒæ—¶è®­ç»ƒå¤šä¸ªä»»åŠ¡',
        'fewshot': 'ğŸª å°‘æ ·æœ¬å­¦ä¹ é…ç½®ï¼ŒåŸå‹ç½‘ç»œæ¶æ„'
    }
    return descriptions


def create_custom_preset(name: str, base_preset: str = 'basic', **overrides) -> PHMConfig:
    """
    åˆ›å»ºè‡ªå®šä¹‰é¢„è®¾
    
    Args:
        name: è‡ªå®šä¹‰é¢„è®¾åç§°
        base_preset: åŸºç¡€é¢„è®¾åç§°
        **overrides: è¦†ç›–å‚æ•°
        
    Returns:
        PHMConfig: è‡ªå®šä¹‰é…ç½®å¯¹è±¡
    """
    base_config = get_preset_config(base_preset)
    custom_config = PHMConfig(**{**base_config.dict(), **_flatten_dict(overrides)})
    
    # æ›´æ–°å®éªŒåç§°
    custom_config.environment.experiment_name = name
    custom_config.environment.notes = f"åŸºäº {base_preset} çš„è‡ªå®šä¹‰é…ç½®"
    
    return custom_config


# ==================== è¾…åŠ©å‡½æ•° ====================

def _flatten_dict(d: Dict[str, Any], prefix: str = "") -> Dict[str, Any]:
    """å°†åµŒå¥—å­—å…¸æ‰å¹³åŒ–ä¸ºåŒä¸‹åˆ’çº¿æ ¼å¼"""
    flattened = {}
    for key, value in d.items():
        new_key = f"{prefix}__{key}" if prefix else key
        if isinstance(value, dict) and not key.endswith('_'):
            flattened.update(_flatten_dict(value, new_key))
        else:
            flattened[new_key] = value
    return flattened


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    # åˆ—å‡ºæ‰€æœ‰é¢„è®¾
    print("ğŸ“‹ å¯ç”¨é…ç½®é¢„è®¾:")
    for name, desc in list_presets().items():
        print(f"  {name}: {desc}")
    
    # åˆ›å»ºå¿«é€Ÿå¼€å§‹é…ç½®
    print(f"\nğŸš€ åˆ›å»ºå¿«é€Ÿå¼€å§‹é…ç½®:")
    quickstart = get_preset_config("quickstart")
    print(f"  å®éªŒå: {quickstart.environment.experiment_name}")
    print(f"  æ¨¡å‹: {quickstart.model.type}.{quickstart.model.name}")
    print(f"  æ‰¹æ¬¡å¤§å°: {quickstart.data.batch_size}")
    
    # åˆ›å»ºè‡ªå®šä¹‰ISFMé…ç½®
    print(f"\nğŸ§  åˆ›å»ºè‡ªå®šä¹‰ISFMé…ç½®:")
    custom_isfm = get_preset_config("isfm", 
                                   model__d_model=256,
                                   trainer__num_epochs=150)
    print(f"  æ¨¡å‹ç»´åº¦: {custom_isfm.model.d_model}")
    print(f"  è®­ç»ƒè½®æ•°: {custom_isfm.trainer.num_epochs}")
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    print(f"\nğŸ’¾ ä¿å­˜é…ç½®æ–‡ä»¶:")
    quickstart.save_yaml("quickstart_config.yaml", minimal=True)
    print("  å·²ä¿å­˜: quickstart_config.yaml")