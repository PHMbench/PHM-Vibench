from pytorch_lightning import LightningModule, Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
import torch
try: 
    import wandb
except ImportError:
    print("[WARNING] wandb æœªå®‰è£…")
    wandb = None
try:
    import swanlab
    from swanlab.plugin.notification import LarkCallback
    from swanlab.plugin.notification import SlackCallback
except ImportError:
    print("[WARNING] swanlab æœªå®‰è£…")
    swanlab = None
import numpy as np


def load_best_model_checkpoint(model: LightningModule, trainer: Trainer) -> LightningModule:
    """
    åŠ è½½è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚

    å‚æ•°:
    - model: è¦åŠ è½½æ£€æŸ¥ç‚¹æƒé‡çš„æ¨¡å‹å®ä¾‹ã€‚
    - trainer: ç”¨äºè®­ç»ƒæ¨¡å‹çš„è®­ç»ƒå™¨å®ä¾‹ã€‚

    è¿”å›:
    - åŠ è½½äº†æœ€ä½³æ£€æŸ¥ç‚¹æƒé‡çš„æ¨¡å‹å®ä¾‹ã€‚
    """
    # ä»trainerçš„callbacksä¸­æ‰¾åˆ°ModelCheckpointå®ä¾‹ï¼Œå¹¶è·å–best_model_path
    model_checkpoint = None
    for callback in trainer.callbacks:
        if isinstance(callback, ModelCheckpoint):
            model_checkpoint = callback
            break

    if model_checkpoint is None:
        raise ValueError("ModelCheckpoint callback not found in trainer's callbacks.")

    best_model_path = model_checkpoint.best_model_path
    print(f"Best model path: {best_model_path}")

    # ç¡®ä¿æœ€ä½³æ¨¡å‹è·¯å¾„ä¸æ˜¯ç©ºçš„
    if not best_model_path:
        print("No best model path found. Please check if the training process saved checkpoints.")
    else:
    # åŠ è½½æœ€ä½³æ£€æŸ¥ç‚¹
    # pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
    # 	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
    # 	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
    # 	WeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.
        state_dict = torch.load(best_model_path,weights_only =False)
        model.load_state_dict(state_dict['state_dict'])
    return model


def init_lab(args_environment, cli_args, experiment_name):
    """
    Initializes wandb and swanlab loggers based on environment configuration.

    Args:
        args_environment: Namespace containing environment configurations (e.g., wandb, swanlab flags, project name, notes).
        cli_args: Namespace containing command-line arguments (e.g., notes).
        experiment_name: The name for the current experiment run.
    """
    use_wandb = getattr(args_environment, 'wandb', False)
    use_swanlab = getattr(args_environment, 'swanlab', False)

    # Initialize WandB
    if wandb and wandb.run is None: # Check if wandb module is available and not already initialized
        if use_wandb:
            project_name = getattr(args_environment, 'project', 'vbench')
            notes = f'Task Notes:{getattr(cli_args, "notes", "")}\nConfig Notes:{getattr(args_environment, "notes", "")}'
            wandb.init(project=project_name,
                        name=experiment_name,
                        notes=notes.strip())
            print(f"[INFO] WandB initialized for project '{project_name}', experiment '{experiment_name}'.")
        else:
            wandb.init(mode='disabled')
            print("[INFO] WandB disabled by configuration.")
    elif use_wandb and wandb is None:
        print("[WARNING] WandB is configured to be used, but the 'wandb' library is not installed.")


    # Initialize SwanLab
    if swanlab and swanlab.run is None: # Check if swanlab module is available and not already initialized
        if use_swanlab:
            project_name = getattr(args_environment, 'project', 'vbench')
            notes = f'N1:{getattr(cli_args, "notes", "")}\n_N2:{getattr(args_environment, "notes", "")}'
            swanlab.init(
                workspace = getattr(args_environment, 'workspace', 'PHMbench'), # SwanLab uses 'workspace'
                project=project_name, # Assuming swanlab uses 'project' similar to wandb
                experiment_name= notes, # experiment_name, 
                description=notes.strip() # Swanlab uses 'description' for notes
                # logdir= # Optional: specify log directory if needed
            )
            print(f"[INFO] SwanLab initialized for project '{project_name}', experiment '{experiment_name}'.")
        else:
            swanlab.init(mode='disabled')
            print("[INFO] SwanLab disabled by configuration.")
    elif use_swanlab and swanlab is None:
        print("[WARNING] SwanLab is configured to be used, but the 'swanlab' library is not installed.")

def close_lab():
    """
    Closes the WandB and SwanLab loggers if they are initialized.
    """
    if wandb and wandb.run:
        wandb.finish()
        print("[INFO] WandB logger closed.")
    if swanlab and swanlab.run:
        try:
            swanlab.finish()
        except Exception as e:
            print(f"[INFO] SwanLab is not used: {e}")
        print("[INFO] SwanLab logger closed.")

def get_num_classes(metadata, dataset_id=None):
    """
    è·å–æ•°æ®é›†ç±»åˆ«æ•°ã€‚

    Args:
        metadata: å…ƒæ•°æ®å¯¹è±¡
        dataset_id: å¯é€‰ï¼ŒæŒ‡å®šæ•°æ®é›†IDæ—¶è¿”å›è¯¥æ•°æ®é›†çš„ç±»åˆ«æ•°(int)ï¼Œå¦åˆ™è¿”å›æ‰€æœ‰æ•°æ®é›†çš„æ˜ å°„(dict)

    Returns:
        int: å½“æŒ‡å®šdataset_idæ—¶ï¼Œè¿”å›è¯¥æ•°æ®é›†çš„ç±»åˆ«æ•°
        dict: å½“æœªæŒ‡å®šdataset_idæ—¶ï¼Œè¿”å›{dataset_id: num_classes}æ˜ å°„

    Raises:
        ValueError: å½“æŒ‡å®šçš„dataset_idä¸å­˜åœ¨æ—¶
    """
    df = metadata.df if hasattr(metadata, 'df') else metadata

    if dataset_id is not None:
        # è¿”å›ç‰¹å®šæ•°æ®é›†çš„ç±»åˆ«æ•°(int)
        dataset_data = df[df['Dataset_id'] == dataset_id]
        if len(dataset_data) == 0:
            raise ValueError(f"Dataset_id {dataset_id} not found in metadata")
        return int(max(dataset_data['Label']) + 1)
    else:
        # è¿”å›æ‰€æœ‰æ•°æ®é›†çš„ç±»åˆ«æ•°æ˜ å°„(dict) - ä¿æŒåŸæœ‰æ ¼å¼
        num_classes = {}
        for key in np.unique(df['Dataset_id']):
            num = max(df[df['Dataset_id'] == key]['Label']) + 1
            num_classes[str(key)] = int(num)  # ä¿æŒåŸæœ‰çš„æ•´å‹key
        return num_classes


def get_num_channels(metadata, dataset_id=None):
    """
    è·å–æ•°æ®é›†é€šé“æ•°ã€‚

    Args:
        metadata: å…ƒæ•°æ®å¯¹è±¡
        dataset_id: å¯é€‰ï¼ŒæŒ‡å®šæ•°æ®é›†IDæ—¶è¿”å›è¯¥æ•°æ®é›†çš„é€šé“æ•°(int)ï¼Œå¦åˆ™è¿”å›æ‰€æœ‰æ•°æ®é›†çš„æ˜ å°„(dict)

    Returns:
        int: å½“æŒ‡å®šdataset_idæ—¶ï¼Œè¿”å›è¯¥æ•°æ®é›†çš„é€šé“æ•°
        dict: å½“æœªæŒ‡å®šdataset_idæ—¶ï¼Œè¿”å›{dataset_id: num_channels}æ˜ å°„

    Raises:
        ValueError: å½“æŒ‡å®šçš„dataset_idä¸å­˜åœ¨æ—¶
    """
    df = metadata.df if hasattr(metadata, 'df') else metadata

    if dataset_id is not None:
        # è¿”å›ç‰¹å®šæ•°æ®é›†çš„é€šé“æ•°(int)
        dataset_data = df[df['Dataset_id'] == dataset_id]
        if len(dataset_data) == 0:
            raise ValueError(f"Dataset_id {dataset_id} not found in metadata")
        return int(max(dataset_data['Channel']))
    else:
        # è¿”å›æ‰€æœ‰æ•°æ®é›†çš„é€šé“æ•°æ˜ å°„(dict)
        num_channels = {}
        for key in np.unique(df['Dataset_id']):
            num_channels[key] = int(max(df[df['Dataset_id'] == key]['Channel']))
        return num_channels
