#!/usr/bin/env python3
"""
Test Flow pretraining integration with Pipeline_02_pretrain_fewshot.
Verifies the complete two-stage workflow: pretraining â†’ few-shot learning.
"""

import torch
import torch.nn as nn
import pytorch_lightning as pl
from argparse import Namespace
import numpy as np
import tempfile
import os

# Import Flow components
from src.model_factory.ISFM.M_04_ISFM_Flow import Model as FlowModel
from src.task_factory.task.pretrain.flow_pretrain import FlowPretrainTask


def create_pipeline_mock_config():
    """Create mock configuration for Pipeline_02 testing."""
    
    # Pretraining stage configuration
    pretrain_config = {
        'data': {
            'batch_size': 4,
            'window_size': 256,  # Smaller for quick testing
            'num_channels': 1
        },
        'model': {
            'type': "ISFM",
            'name': "M_04_ISFM_Flow",
            'sequence_length': 256,
            'channels': 1,
            'hidden_dim': 64,
            'time_dim': 16,
            'condition_dim': 16,
            'use_conditional': True,
            'sigma_min': 0.001,
            'sigma_max': 1.0
        },
        'task': {
            'name': "flow_pretrain",
            'type': "pretrain",
            'loss': "CE",
            'metrics': ["acc"],
            'num_steps': 10,  # Fewer steps for testing
            'flow_lr': 1e-3,
            'use_contrastive': False,
            'enable_visualization': False,
            'track_memory': False,
            'track_gradients': False,
            'lr': 1e-3,
            'weight_decay': 1e-5
        },
        'trainer': {
            'gpus': 0,  # CPU only for testing
            'precision': 32,
            'gradient_clip_val': 1.0,
            'max_epochs': 1,
            'log_every_n_steps': 10
        },
        'environment': {
            'seed': 42
        }
    }
    
    # Few-shot stage configuration
    fewshot_config = {
        'data': {
            'batch_size': 4,
            'window_size': 256,
            'num_channels': 1,
            'n_support': 2,
            'n_query': 3
        },
        'model': {
            'type': "ISFM", 
            'name': "M_04_ISFM_Flow",  # Same as pretraining
            'load_pretrained': True
        },
        'task': {
            'name': "few_shot",
            'type': "FS",
            'loss': "CE",
            'metrics': ["acc"],
            'lr': 1e-4,  # Lower LR for few-shot fine-tuning
            'weight_decay': 1e-6
        },
        'trainer': {
            'gpus': 0,
            'precision': 32,
            'max_epochs': 1
        },
        'environment': {
            'seed': 42
        }
    }
    
    return pretrain_config, fewshot_config


def config_to_namespace(config_dict):
    """Convert nested config dict to nested namespaces."""
    result = Namespace()
    for key, value in config_dict.items():
        if isinstance(value, dict):
            setattr(result, key, config_to_namespace(value))
        else:
            setattr(result, key, value)
    return result


def create_mock_metadata():
    """Create enhanced mock metadata for pipeline testing."""
    class MockMetadata:
        def __init__(self):
            import pandas as pd
            self.df = pd.DataFrame({
                'Dataset_id': [1, 2, 3, 4, 5, 6],
                'Domain_id': [1, 1, 2, 2, 3, 3], 
                'file_id': ['file1', 'file2', 'file3', 'file4', 'file5', 'file6'],
                'Label': [0, 1, 0, 1, 0, 1]
            })
            
            # Enhanced mapping for pipeline testing
            self._file_mapping = {}
            for i in range(20):  # Support more files
                for j in range(8):
                    file_id = f'pipeline_file_{i}_{j}'
                    self._file_mapping[file_id] = {
                        'Domain_id': (i % 3) + 1, 
                        'Dataset_id': (i % 4) + 1, 
                        'Label': j % 2
                    }
        
        def items(self):
            return {
                'item1': {'Name': 'test_dataset', 'Label': 0},
                'item2': {'Name': 'test_dataset', 'Label': 1},
                'item3': {'Name': 'test_dataset', 'Label': 2}, 
                'item4': {'Name': 'test_dataset', 'Label': 3}
            }.items()
            
        def __contains__(self, key):
            return key in self._file_mapping or key in ['file1', 'file2', 'file3']
            
        def __getitem__(self, key):
            return self._file_mapping.get(key, {'Domain_id': 1, 'Dataset_id': 1, 'Label': 0})
    
    return MockMetadata()


def create_pipeline_data(config, stage="pretrain"):
    """Create data for pipeline stages."""
    batch_size = config['data']['batch_size']
    seq_len = config['data']['window_size']
    
    # Generate different data for different stages
    if stage == "pretrain":
        # Pretraining data - focus on diverse patterns
        t = torch.linspace(0, 1, seq_len)
        signals = []
        file_ids = []
        
        for i in range(batch_size):
            # Create varied pretraining signals
            freq = 20 + i * 10  # Different frequencies
            signal = torch.sin(2 * np.pi * freq * t) + 0.1 * torch.randn(seq_len)
            signals.append(signal.unsqueeze(-1))
            file_ids.append(f'pipeline_file_pretrain_{i}')
        
        x = torch.stack(signals)
        
    else:  # few-shot stage
        # Few-shot data - more focused patterns for classification
        t = torch.linspace(0, 1, seq_len)
        signals = []
        file_ids = []
        
        for i in range(batch_size):
            # Create class-specific patterns for few-shot
            if i % 2 == 0:
                # Class 0: Low frequency pattern
                signal = torch.sin(2 * np.pi * 15 * t) + 0.05 * torch.randn(seq_len)
            else:
                # Class 1: High frequency pattern  
                signal = torch.sin(2 * np.pi * 80 * t) + 0.05 * torch.randn(seq_len)
            
            signals.append(signal.unsqueeze(-1))
            file_ids.append(f'pipeline_file_fewshot_{i}')
        
        x = torch.stack(signals)
    
    return {'x': x, 'file_id': file_ids}


def test_pipeline02_integration():
    """Test complete Pipeline_02_pretrain_fewshot integration."""
    print("ğŸš€ å¼€å§‹Pipeline_02_pretrain_fewshoté›†æˆæµ‹è¯•...")
    
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        # Get pipeline configurations
        pretrain_config, fewshot_config = create_pipeline_mock_config()
        metadata = create_mock_metadata()
        
        print("ğŸ“‹ Pipelineé…ç½®:")
        print(f"   é¢„è®­ç»ƒé…ç½®: {pretrain_config['model']['name']}")
        print(f"   Few-shoté…ç½®: {fewshot_config['model']['name']}")
        
        # === STAGE 1: PRETRAINING ===
        print("\nğŸ“¦ é˜¶æ®µ1: Flowé¢„è®­ç»ƒ...")
        
        # Convert to namespaces
        args_data = config_to_namespace(pretrain_config['data'])
        args_model = config_to_namespace(pretrain_config['model']) 
        args_task = config_to_namespace(pretrain_config['task'])
        args_trainer = config_to_namespace(pretrain_config['trainer'])
        args_environment = config_to_namespace(pretrain_config['environment'])
        
        # Create pretraining model and task
        pretrain_model = FlowModel(args_model, metadata)
        pretrain_task = FlowPretrainTask(
            network=pretrain_model,
            args_data=args_data,
            args_model=args_model,
            args_task=args_task,
            args_trainer=args_trainer,
            args_environment=args_environment,
            metadata=metadata
        )
        
        print(f"   âœ… é¢„è®­ç»ƒæ¨¡å‹å‚æ•°: {sum(p.numel() for p in pretrain_model.parameters()):,}")
        
        # Test pretraining training step
        pretrain_data = create_pipeline_data(pretrain_config, "pretrain")
        pretrain_task.train()
        pretrain_loss = pretrain_task.training_step(pretrain_data, 0)
        print(f"   âœ… é¢„è®­ç»ƒæŸå¤±: {pretrain_loss.item():.4f}")
        
        # Test checkpoint saving capability
        print("ğŸ’¾ æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜...")
        with tempfile.NamedTemporaryFile(suffix='.pt', delete=False) as f:
            checkpoint_path = f.name
        
        try:
            # Test save_pretrained_state method
            save_info = pretrain_task.save_pretrained_state(checkpoint_path)
            print(f"   âœ… æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {save_info['success']}")
            print(f"   ğŸ“Š ä¿å­˜è½®æ¬¡: {save_info['saved_epoch']}")
            print(f"   ğŸ“Š æ¨¡å‹æ¶æ„: {save_info['model_architecture']}")
            
        except Exception as e:
            print(f"   âš ï¸  æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
            checkpoint_path = None
        
        # === STAGE 2: FEW-SHOT LEARNING ===
        print("\nğŸ¯ é˜¶æ®µ2: Few-shotå­¦ä¹ ...")
        
        if checkpoint_path and os.path.exists(checkpoint_path):
            # Convert few-shot config to namespaces
            fs_args_data = config_to_namespace(fewshot_config['data'])
            fs_args_model = config_to_namespace(fewshot_config['model'])
            fs_args_task = config_to_namespace(fewshot_config['task'])
            fs_args_trainer = config_to_namespace(fewshot_config['trainer'])
            fs_args_environment = config_to_namespace(fewshot_config['environment'])
            
            # Create few-shot model (same architecture as pretraining)
            fewshot_model = FlowModel(args_model, metadata)  # Use original model config
            fewshot_task = FlowPretrainTask(
                network=fewshot_model,
                args_data=fs_args_data,
                args_model=args_model,  # Keep original model config
                args_task=fs_args_task,
                args_trainer=fs_args_trainer,
                args_environment=fs_args_environment,
                metadata=metadata
            )
            
            # Test checkpoint loading
            print("ğŸ“‚ æµ‹è¯•æ£€æŸ¥ç‚¹åŠ è½½...")
            try:
                load_info = fewshot_task.load_pretrained_state(checkpoint_path)
                print(f"   âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: {load_info['success']}")
                print(f"   ğŸ“Š æºè®­ç»ƒè½®æ¬¡: {load_info['source_epoch']}")
                print(f"   ğŸ“Š ç¼ºå¤±é”®: {load_info['missing_keys']}")
                print(f"   ğŸ“Š æ„å¤–é”®: {load_info['unexpected_keys']}")
                
                # Test few-shot preparation
                print("ğŸ”§ å‡†å¤‡Few-shotè½¬ç§»...")
                transfer_info = fewshot_task.prepare_for_fewshot_transfer()
                print(f"   âœ… å†»ç»“å±‚æ•°: {len(transfer_info['frozen_layers'])}")
                print(f"   âœ… å¯è®­ç»ƒå±‚æ•°: {len(transfer_info['unfrozen_layers'])}")
                print(f"   ğŸ“Š å»ºè®®å­¦ä¹ ç‡ç¼©æ”¾: {transfer_info['transfer_recommendations']['suggested_lr_reduction']}")
                
                # Test few-shot training
                fewshot_data = create_pipeline_data(fewshot_config, "fewshot")
                fewshot_task.train()
                fewshot_loss = fewshot_task.training_step(fewshot_data, 0)
                print(f"   âœ… Few-shotè®­ç»ƒæŸå¤±: {fewshot_loss.item():.4f}")
                
                # Compare pretraining vs few-shot performance
                print("ğŸ“Š æ€§èƒ½å¯¹æ¯”:")
                print(f"   é¢„è®­ç»ƒæŸå¤±: {pretrain_loss.item():.4f}")
                print(f"   Few-shotæŸå¤±: {fewshot_loss.item():.4f}")
                improvement = pretrain_loss.item() - fewshot_loss.item()
                print(f"   æ€§èƒ½æ”¹è¿›: {improvement:.4f}")
                
            except Exception as e:
                print(f"   âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
                return False
                
        else:
            print("   âš ï¸  è·³è¿‡Few-shotæµ‹è¯• - æ— æ£€æŸ¥ç‚¹æ–‡ä»¶")
        
        # === PIPELINE COMPATIBILITY VALIDATION ===
        print("\nğŸ” Pipelineå…¼å®¹æ€§éªŒè¯...")
        
        # Test pipeline checkpoint callback
        checkpoint_callback = pretrain_task.get_pipeline_checkpoint_callback()
        print(f"   âœ… æ£€æŸ¥ç‚¹å›è°ƒ: {type(checkpoint_callback).__name__}")
        print(f"   ğŸ“Š ç›‘æ§æŒ‡æ ‡: {checkpoint_callback.monitor}")
        
        # Test feature extraction capability  
        print("ğŸ“Š ç‰¹å¾æå–æµ‹è¯•...")
        class MockDataLoader:
            def __init__(self, data):
                self.data = [data]
            def __iter__(self):
                return iter(self.data)
        
        mock_loader = MockDataLoader(pretrain_data)
        features = pretrain_task.extract_feature_representations(mock_loader)
        print(f"   âœ… ç‰¹å¾æå–å®Œæˆ: {len(features)} ç±»ç‰¹å¾")
        
        # Final compatibility check
        compatibility = pretrain_task.validate_pipeline_compatibility()
        print(f"   ğŸ¯ æœ€ç»ˆå…¼å®¹æ€§: {compatibility['status']}")
        
        # Cleanup
        if checkpoint_path and os.path.exists(checkpoint_path):
            os.unlink(checkpoint_path)
        
        print("\nğŸ‰ Pipeline_02_pretrain_fewshoté›†æˆæµ‹è¯•å®Œå…¨é€šè¿‡ï¼")
        print("=" * 60)
        print("âœ… é¢„è®­ç»ƒé˜¶æ®µ: æ¨¡å‹è®­ç»ƒå’Œæ£€æŸ¥ç‚¹ä¿å­˜")
        print("âœ… è½¬ç§»é˜¶æ®µ: æ£€æŸ¥ç‚¹åŠ è½½å’ŒçŠ¶æ€æ¢å¤") 
        print("âœ… Few-shoté˜¶æ®µ: æ¨¡å‹å¾®è°ƒå’Œæ€§èƒ½éªŒè¯")
        print("âœ… Pipelineå…¼å®¹æ€§: å®Œæ•´å·¥ä½œæµéªŒè¯")
        print("ğŸš€ Flow + Pipeline_02 é›†æˆå°±ç»ªï¼")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Pipeline_02é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_pipeline02_integration()
    if success:
        print("\nâœ¨ Pipeline_02_pretrain_fewshoté›†æˆæµ‹è¯•é€šè¿‡ï¼")
        exit(0)
    else:
        print("\nğŸ’¥ æµ‹è¯•å¤±è´¥ - è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        exit(1)