# PHMåŸºç¡€æ¨¡å‹æ¥å£è§„èŒƒ

> ğŸ“ **è§„èŒƒæŒ‡å—** - PHMåŸºç¡€æ¨¡å‹å¼€å‘è€…å¿…è¯»ï¼Œå®šä¹‰æ ‡å‡†æ¥å£å’Œæœ€ä½³å®è·µ

## ğŸ¯ æ–‡æ¡£ç›®çš„

æœ¬æ–‡æ¡£ä¸ºPHMåŸºç¡€æ¨¡å‹å¼€å‘è€…æä¾›ï¼š
- âœ… **æ ‡å‡†æ¥å£è§„èŒƒ** - å¿…é¡»éµå¾ªçš„APIå®šä¹‰
- ğŸ—ï¸ **å®ç°æŒ‡å¯¼** - å…·ä½“å¼€å‘æ­¥éª¤å’Œæ³¨æ„äº‹é¡¹  
- ğŸ”§ **è°ƒè¯•æŠ€å·§** - å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
- ğŸ“Š **æœ€ä½³å®è·µ** - æ€§èƒ½ä¼˜åŒ–å’Œä»£ç è§„èŒƒ

## ğŸ“‹ æ ¸å¿ƒæ¥å£è§„èŒƒ

### åŸºç¡€æ¨¡å‹æ¥å£

**æ‰€æœ‰æ¨¡å‹å¿…é¡»éµå¾ªä»¥ä¸‹æ¥å£è§„èŒƒï¼š**

```python
import torch.nn as nn

class Model(nn.Module):
    """
    PHMåŸºç¡€æ¨¡å‹æ ‡å‡†æ¥å£
    
    æ³¨æ„ï¼šç±»åå¿…é¡»æ˜¯ 'Model'ï¼Œè¿™æ˜¯æ¡†æ¶çº¦å®š
    """
    
    def __init__(self, args_m, metadata=None):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            args_m (Namespace): æ¨¡å‹é…ç½®å‚æ•°
                - åŒ…å«æ‰€æœ‰é…ç½®æ–‡ä»¶ä¸­ model èŠ‚çš„å‚æ•°
                - å¯é€šè¿‡ args_m.parameter_name è®¿é—®
                
            metadata (MetadataAccessor): æ•°æ®é›†å…ƒä¿¡æ¯è®¿é—®å™¨
                - ç”¨äºè·å–æ•°æ®é›†ç›¸å…³ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
                - è®¿é—®æ–¹å¼ï¼šmetadata[file_id]['field_name']
        
        å¿…é¡»è®¾ç½®çš„å±æ€§ï¼š
            - æ¨¡å‹çš„æ‰€æœ‰ç½‘ç»œå±‚å’Œå‚æ•°
            - ä»args_mä¸­æå–å¿…è¦çš„é…ç½®å‚æ•°
        """
        super(Model, self).__init__()
        
        # âœ… æ­£ç¡®ç¤ºä¾‹ï¼šæå–é…ç½®å‚æ•°
        self.input_dim = getattr(args_m, 'input_dim', 1)
        self.num_classes = getattr(args_m, 'num_classes', 10)
        self.dropout = getattr(args_m, 'dropout', 0.1)
        
        # âœ… ä¿å­˜å¼•ç”¨ç”¨äºåç»­ä½¿ç”¨
        self.metadata = metadata
        self.args_m = args_m
    
    def forward(self, x, file_id=None, task_id=None, return_feature=False):
        """
        å‰å‘ä¼ æ’­ - æ ¸å¿ƒæ¥å£
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡
                - æ ‡å‡†å½¢çŠ¶: (batch_size, sequence_length, channels)
                - ä¹Ÿæ”¯æŒ: (batch_size, channels, sequence_length)
                
            file_id (str/int, optional): æ ·æœ¬æ–‡ä»¶ID
                - ç”¨äºä»metadataè·å–æ ·æœ¬ç‰¹å®šä¿¡æ¯
                - ä¾‹å¦‚ï¼šé‡‡æ ·ç‡ã€æ•°æ®é›†IDç­‰
                
            task_id (str, optional): ä»»åŠ¡ç±»å‹æ ‡è¯†
                - 'classification': åˆ†ç±»ä»»åŠ¡
                - 'prediction': é¢„æµ‹ä»»åŠ¡
                - 'regression': å›å½’ä»»åŠ¡
                
            return_feature (bool): æ˜¯å¦è¿”å›ç‰¹å¾è€Œéæœ€ç»ˆè¾“å‡º
                - True: è¿”å›ä¸­é—´ç‰¹å¾è¡¨ç¤º
                - False: è¿”å›ä»»åŠ¡ç›¸å…³çš„æœ€ç»ˆè¾“å‡º
        
        Returns:
            torch.Tensor: æ¨¡å‹è¾“å‡º
                - åˆ†ç±»ä»»åŠ¡: (batch_size, num_classes)
                - é¢„æµ‹ä»»åŠ¡: (batch_size, pred_length, channels)
                - ç‰¹å¾è¾“å‡º: (batch_size, feature_dim)
        """
        # ä½ çš„å‰å‘ä¼ æ’­å®ç°
        pass
```

### ä¼ ç»Ÿæ¨¡å‹å®ç°ç¤ºä¾‹

```python
# src/model_factory/CNN/MyResNet.py
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, args_m, metadata=None):
        super(Model, self).__init__()
        
        # ä»é…ç½®æå–å‚æ•°
        self.input_channels = getattr(args_m, 'input_channels', 1)
        self.num_classes = getattr(args_m, 'num_classes', 10)
        self.dropout_rate = getattr(args_m, 'dropout', 0.1)
        
        # æ„å»ºç½‘ç»œå±‚
        self.conv1 = nn.Conv1d(self.input_channels, 64, kernel_size=7)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=5) 
        self.conv3 = nn.Conv1d(128, 256, kernel_size=3)
        
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.dropout = nn.Dropout(self.dropout_rate)
        self.classifier = nn.Linear(256, self.num_classes)
        
    def forward(self, x, file_id=None, task_id=None, return_feature=False):
        # è¾“å…¥å½¢çŠ¶å¤„ç†: (B, L, C) -> (B, C, L)
        if x.dim() == 3 and x.shape[-1] < x.shape[1]:
            x = x.transpose(1, 2)
        
        # ç‰¹å¾æå–
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))  
        x = F.relu(self.conv3(x))
        
        # å…¨å±€æ± åŒ–
        features = self.pool(x).squeeze(-1)  # (B, 256)
        
        # æ ¹æ®éœ€æ±‚è¿”å›ç‰¹å¾æˆ–åˆ†ç±»ç»“æœ
        if return_feature:
            return features
        
        x = self.dropout(features)
        output = self.classifier(x)
        
        return output
```

## ğŸ—ï¸ ISFMåŸºç¡€æ¨¡å‹è§„èŒƒ

### ISFMæ¨¡å‹æ¶æ„

ISFM (Industrial Signal Foundation Model) é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼š

```
Input Signal â†’ Embedding â†’ Backbone â†’ Task Head â†’ Output
     â†“           â†“          â†“          â†“         â†“
   (B,L,C)   Hierarchical  Transformer  Linear   Task-specific
             Signal Embed    / CNN       Head     Output
```

### ISFMç‰ˆæœ¬è¯´æ˜

| ç‰ˆæœ¬ | æ–‡ä»¶å | ç‰¹ç‚¹ | æ¨èç”¨é€” | çŠ¶æ€ |
|-----|--------|------|----------|------|
| **M_01** | `M_01_ISFM.py` | åŸºç¡€ç‰ˆæœ¬ï¼Œæœ€å°åŠŸèƒ½é›† | âœ… **æ–°æ‰‹æ¨è** | ç¨³å®š |
| **M_02** | `M_02_ISFM.py` | å¢å¼ºç‰ˆï¼Œæ”¯æŒå¤šé€šé“å’Œç³»ç»Ÿæ„ŸçŸ¥ | ğŸš€ **ç”Ÿäº§æ¨è** | ç¨³å®š |
| **M_03** | `M_03_ISFM.py` | å®éªŒç‰ˆæœ¬ï¼ŒåŠŸèƒ½ä¸å®Œæ•´ | âŒ **ä¸æ¨èä½¿ç”¨** | æœ‰Bug |

### ISFMæ ‡å‡†æ¥å£

```python
class Model(nn.Module):
    """ISFMåŸºç¡€æ¨¡å‹æ ‡å‡†å®ç°"""
    
    def __init__(self, args_m, metadata):
        super(Model, self).__init__()
        self.metadata = metadata
        self.args_m = args_m
        
        # æ„å»ºä¸‰å¤§ç»„ä»¶
        self.embedding = Embedding_dict[args_m.embedding](args_m)
        self.backbone = Backbone_dict[args_m.backbone](args_m)
        self.task_head = TaskHead_dict[args_m.task_head](args_m)
    
    def _embed(self, x, file_id=None):
        """
        Step 1: ä¿¡å·åµŒå…¥
        
        Args:
            x: åŸå§‹ä¿¡å· (B, L, C)
            file_id: ç”¨äºè·å–é‡‡æ ·ç‡ç­‰ä¿¡æ¯
            
        Returns:
            embedded_x: åµŒå…¥åçš„ä¿¡å·
            context_info: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        """
        # è·å–ä¿¡å·ç›¸å…³å…ƒä¿¡æ¯
        if file_id is not None and self.args_m.embedding in ('E_01_HSE', 'E_02_HSE_v2'):
            fs = self.metadata[file_id]['Sample_rate']
            system_id = self.metadata[file_id]['Dataset_id']
            return self.embedding(x, system_id, fs)
        else:
            return self.embedding(x)
    
    def _encode(self, x, context=None):
        """
        Step 2: ç‰¹å¾ç¼–ç 
        
        Args:
            x: åµŒå…¥åçš„ä¿¡å·
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            encoded_features: ç¼–ç åçš„ç‰¹å¾
        """
        return self.backbone(x, context)
    
    def _head(self, x, file_id=None, task_id=None, return_feature=False):
        """
        Step 3: ä»»åŠ¡è¾“å‡º
        
        Args:
            x: ç¼–ç åçš„ç‰¹å¾
            file_id: æ ·æœ¬ID
            task_id: ä»»åŠ¡ç±»å‹
            return_feature: æ˜¯å¦è¿”å›ç‰¹å¾
            
        Returns:
            task_output: ä»»åŠ¡ç›¸å…³è¾“å‡º
        """
        system_id = None
        if file_id is not None:
            system_id = self.metadata[file_id]['Dataset_id']
            
        return self.task_head(
            x, 
            system_id=system_id, 
            task_id=task_id,
            return_feature=return_feature
        )
    
    def forward(self, x, file_id=None, task_id=None, return_feature=False):
        """å®Œæ•´å‰å‘ä¼ æ’­æµç¨‹"""
        # è®°å½•è¾“å…¥å½¢çŠ¶ä¾›åç»­ä½¿ç”¨
        self.shape = x.shape
        
        # ä¸‰æ­¥å¤„ç†æµç¨‹
        x, context = self._embed(x, file_id)      # Step 1
        x = self._encode(x, context)              # Step 2  
        x = self._head(x, file_id, task_id, return_feature)  # Step 3
        
        return x
```

## ğŸ”§ æ¨¡å‹ç»„ä»¶å¼€å‘

### 1. åµŒå…¥å±‚å¼€å‘ (E_XX)

```python
# src/model_factory/ISFM/embedding/E_XX_YourEmbedding.py
import torch.nn as nn

class E_XX_YourEmbedding(nn.Module):
    """è‡ªå®šä¹‰ä¿¡å·åµŒå…¥å±‚"""
    
    def __init__(self, configs):
        super().__init__()
        self.patch_size = configs.patch_size_L
        self.embed_dim = configs.output_dim
        
        # ä½ çš„åµŒå…¥ç½‘ç»œ
        self.projection = nn.Linear(self.patch_size, self.embed_dim)
        
    def forward(self, x, system_id=None, fs=None):
        """
        Args:
            x: è¾“å…¥ä¿¡å· (B, L, C)
            system_id: ç³»ç»ŸIDï¼ˆå¯é€‰ï¼Œç”¨äºç³»ç»Ÿæ„ŸçŸ¥ï¼‰
            fs: é‡‡æ ·é¢‘ç‡ï¼ˆå¯é€‰ï¼Œç”¨äºé¢‘ç‡æ„ŸçŸ¥ï¼‰
            
        Returns:
            embedded_signal: åµŒå…¥åä¿¡å· (B, N_patches, embed_dim)
            context_info: ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        # å®ç°ä½ çš„åµŒå…¥é€»è¾‘
        batch_size, seq_len, channels = x.shape
        
        # Patchåˆ†å‰²ç¤ºä¾‹
        n_patches = seq_len // self.patch_size
        x_patches = x[:, :n_patches * self.patch_size, :].reshape(
            batch_size, n_patches, self.patch_size, channels
        )
        
        # æŠ•å½±åˆ°åµŒå…¥ç©ºé—´
        embedded = self.projection(x_patches.mean(dim=-1))
        
        return embedded, None
```

### 2. éª¨å¹²ç½‘ç»œå¼€å‘ (B_XX)

```python
# src/model_factory/ISFM/backbone/B_XX_YourBackbone.py
import torch.nn as nn

class B_XX_YourBackbone(nn.Module):
    """è‡ªå®šä¹‰éª¨å¹²ç½‘ç»œ"""
    
    def __init__(self, configs):
        super().__init__()
        self.d_model = configs.d_model
        self.num_layers = configs.num_layers
        
        # æ„å»ºä½ çš„ç½‘ç»œæ¶æ„
        self.layers = nn.ModuleList([
            YourTransformerLayer(self.d_model) 
            for _ in range(self.num_layers)
        ])
        
    def forward(self, x, context=None):
        """
        Args:
            x: åµŒå…¥åä¿¡å· (B, N_patches, embed_dim)
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            processed_features: å¤„ç†åç‰¹å¾ (B, N_patches, d_model)
        """
        for layer in self.layers:
            x = layer(x, context)
        return x
```

### 3. ä»»åŠ¡å¤´å¼€å‘ (H_XX)

```python
# src/model_factory/ISFM/task_head/H_XX_YourHead.py
import torch.nn as nn

class H_XX_YourHead(nn.Module):
    """è‡ªå®šä¹‰ä»»åŠ¡å¤´"""
    
    def __init__(self, configs):
        super().__init__()
        self.d_model = configs.d_model
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹æ„å»ºä¸åŒè¾“å‡ºå±‚
        if hasattr(configs, 'num_classes'):
            self.classifier = nn.Linear(self.d_model, configs.num_classes)
        
    def forward(self, x, system_id=None, task_id=None, return_feature=False):
        """
        Args:
            x: éª¨å¹²ç½‘ç»œè¾“å‡º (B, N_patches, d_model)
            system_id: ç³»ç»ŸIDï¼ˆç”¨äºå¤šç³»ç»Ÿä»»åŠ¡ï¼‰
            task_id: ä»»åŠ¡ç±»å‹
            return_feature: æ˜¯å¦è¿”å›ç‰¹å¾
            
        Returns:
            task_output: ä»»åŠ¡ç›¸å…³è¾“å‡º
        """
        # å…¨å±€ç‰¹å¾èšåˆ
        if x.dim() == 3:
            features = x.mean(dim=1)  # (B, d_model)
        else:
            features = x
            
        if return_feature:
            return features
            
        # æ ¹æ®ä»»åŠ¡ç±»å‹è¾“å‡º
        if task_id == 'classification':
            return self.classifier(features)
        elif task_id == 'prediction':
            # å®ç°é¢„æµ‹ä»»åŠ¡é€»è¾‘
            return self.predictor(features)
        else:
            return features
```

## âš™ï¸ é…ç½®å‚æ•°è§„èŒƒ

### æ ‡å‡†å‚æ•°å‘½å

```yaml
model:
  # === åŸºç¡€å‚æ•° ===
  name: "ModelName"           # æ¨¡å‹åç§°ï¼ˆå¿…éœ€ï¼‰
  type: "ModelType"           # æ¨¡å‹ç±»å‹ï¼ˆå¿…éœ€ï¼‰
  
  # === ç½‘ç»œç»“æ„å‚æ•° ===
  input_dim: 1                # è¾“å…¥é€šé“æ•°
  d_model: 128               # æ¨¡å‹éšè—ç»´åº¦
  num_layers: 6              # ç½‘ç»œå±‚æ•°
  num_heads: 8               # æ³¨æ„åŠ›å¤´æ•°
  d_ff: 256                  # å‰é¦ˆç½‘ç»œç»´åº¦
  
  # === è®­ç»ƒå‚æ•° ===
  dropout: 0.1               # Dropoutæ¦‚ç‡
  activation: "relu"         # æ¿€æ´»å‡½æ•°
  
  # === ä»»åŠ¡å‚æ•° ===
  num_classes: 10            # åˆ†ç±»ç±»åˆ«æ•°
  pred_length: 96            # é¢„æµ‹é•¿åº¦
  
  # === ISFMä¸“ç”¨å‚æ•° ===
  embedding: "E_01_HSE"      # åµŒå…¥å±‚ç±»å‹
  backbone: "B_08_PatchTST"  # éª¨å¹²ç½‘ç»œç±»å‹  
  task_head: "H_01_Linear_cla" # ä»»åŠ¡å¤´ç±»å‹
  
  # Patchç›¸å…³å‚æ•°
  patch_size_L: 16           # Patché•¿åº¦
  patch_size_C: 1            # Patché€šé“
  num_patches: 64            # Patchæ•°é‡
  output_dim: 128            # è¾“å‡ºç»´åº¦
```

### å‚æ•°è®¿é—®æœ€ä½³å®è·µ

```python
def __init__(self, args_m, metadata=None):
    super().__init__()
    
    # âœ… æ¨èï¼šä½¿ç”¨ getattr æä¾›é»˜è®¤å€¼
    self.input_dim = getattr(args_m, 'input_dim', 1)
    self.dropout = getattr(args_m, 'dropout', 0.1)
    
    # âœ… æ¨èï¼šå‚æ•°éªŒè¯
    assert self.input_dim > 0, "input_dim must be positive"
    assert 0 <= self.dropout <= 1, "dropout must be in [0, 1]"
    
    # âŒ é¿å…ï¼šç›´æ¥è®¿é—®å¯èƒ½ä¸å­˜åœ¨çš„å±æ€§
    # self.some_param = args_m.some_param  # å¯èƒ½æŠ¥é”™
    
    # âœ… æ¨èï¼šå¤„ç†å¤æ‚å‚æ•°
    if hasattr(args_m, 'layer_sizes'):
        self.layer_sizes = args_m.layer_sizes
    else:
        self.layer_sizes = [128, 256, 128]  # é»˜è®¤å€¼
```

## ğŸš€ æ¨¡å‹æ³¨å†Œå’Œä½¿ç”¨

### 1. æ³¨å†Œæ–°æ¨¡å‹

```python
# æ–¹æ³•1ï¼šåœ¨ __init__.py ä¸­æ³¨å†Œ
# src/model_factory/YourType/__init__.py
from .YourModel import Model as YourModel

# æ–¹æ³•2ï¼šä½¿ç”¨æ³¨å†Œè£…é¥°å™¨ï¼ˆæ¨èï¼‰
from ...utils.registry import Registry
from ..model_factory import register_model

@register_model("YourType", "YourModel")
class Model(nn.Module):
    # ä½ çš„å®ç°
```

### 2. é…ç½®æ–‡ä»¶ä½¿ç”¨

```yaml
# configs/your_experiment.yaml
model:
  name: "YourModel"
  type: "YourType"
  
  # ä½ çš„è‡ªå®šä¹‰å‚æ•°
  custom_param1: 128
  custom_param2: true
  custom_layers: [64, 128, 256]
```

### 3. ç¼–ç¨‹æ–¹å¼ä½¿ç”¨

```python
from src.model_factory.model_factory import model_factory
from argparse import Namespace

# åˆ›å»ºé…ç½®
args_model = Namespace(
    name="YourModel",
    type="YourType", 
    input_dim=1,
    num_classes=4
)

# å®ä¾‹åŒ–æ¨¡å‹
model = model_factory(args_model, metadata)
```

## ğŸ§ª æµ‹è¯•å’Œè°ƒè¯•

### å•å…ƒæµ‹è¯•æ¨¡æ¿

```python
# åœ¨ä½ çš„æ¨¡å‹æ–‡ä»¶æœ«å°¾æ·»åŠ 
if __name__ == '__main__':
    import torch
    from argparse import Namespace
    
    def test_model():
        """æµ‹è¯•æ¨¡å‹åŸºæœ¬åŠŸèƒ½"""
        # åˆ›å»ºé…ç½®
        args_m = Namespace(
            input_dim=1,
            num_classes=4,
            d_model=64,
            dropout=0.1
        )
        
        # åˆ›å»ºæ¨¡å‹
        model = Model(args_m)
        model.eval()
        
        # æµ‹è¯•ä¸åŒè¾“å…¥å½¢çŠ¶
        batch_sizes = [1, 4, 16]
        seq_lengths = [512, 1024, 2048]
        channels = [1, 3]
        
        for B in batch_sizes:
            for L in seq_lengths:
                for C in channels:
                    x = torch.randn(B, L, C)
                    
                    # æµ‹è¯•å‰å‘ä¼ æ’­
                    with torch.no_grad():
                        output = model(x)
                        print(f"Input: {x.shape} -> Output: {output.shape}")
                    
                    # éªŒè¯è¾“å‡ºå½¢çŠ¶
                    assert output.shape[0] == B, "Batch size mismatch"
                    assert output.shape[1] == args_m.num_classes, "Class number mismatch"
        
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    
    # è¿è¡Œæµ‹è¯•
    test_model()
```

### å¸¸è§é”™è¯¯å’Œè°ƒè¯•

#### 1. å½¢çŠ¶é”™è¯¯

```python
# âŒ å¸¸è§é”™è¯¯ï¼šæœªå¤„ç†è¾“å…¥å½¢çŠ¶å˜åŒ–
def forward(self, x):
    return self.conv1d(x)  # å‡è®¾è¾“å…¥æ˜¯ (B, C, L) ä½†å®é™…æ˜¯ (B, L, C)

# âœ… æ­£ç¡®å¤„ç†ï¼š
def forward(self, x):
    # æ£€æŸ¥å¹¶è½¬æ¢è¾“å…¥å½¢çŠ¶
    if x.dim() == 3 and x.shape[-1] < x.shape[1]:
        x = x.transpose(1, 2)  # (B, L, C) -> (B, C, L)
    return self.conv1d(x)
```

#### 2. å‚æ•°è®¿é—®é”™è¯¯

```python
# âŒ å¯èƒ½å‡ºé”™ï¼š
self.param = args_m.param  # å¦‚æœ param ä¸å­˜åœ¨ä¼šæŠ¥é”™

# âœ… å®‰å…¨è®¿é—®ï¼š
self.param = getattr(args_m, 'param', default_value)
```

#### 3. ISFMç»„ä»¶é”™è¯¯

```python
# âŒ å¸¸è§é”™è¯¯ï¼šå¿˜è®°å¤„ç†å¯é€‰å‚æ•°
def forward(self, x, file_id=None, task_id=None):
    system_id = self.metadata[file_id]['Dataset_id']  # file_id å¯èƒ½æ˜¯ None

# âœ… æ­£ç¡®å¤„ç†ï¼š
def forward(self, x, file_id=None, task_id=None):
    system_id = None
    if file_id is not None:
        system_id = self.metadata[file_id]['Dataset_id']
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å†…å­˜ä¼˜åŒ–

```python
# ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜
import torch.utils.checkpoint as checkpoint

def forward(self, x):
    # å¯¹äºå¤§æ¨¡å‹ï¼Œä½¿ç”¨æ£€æŸ¥ç‚¹
    x = checkpoint.checkpoint(self.large_layer, x)
    return x

# åŠæ—¶é‡Šæ”¾ä¸­é—´å˜é‡
def forward(self, x):
    intermediate = self.layer1(x)
    output = self.layer2(intermediate)
    del intermediate  # æ˜¾å¼é‡Šæ”¾å†…å­˜
    return output
```

### 2. è®¡ç®—ä¼˜åŒ–

```python
# ä½¿ç”¨ torch.jit.script ç¼–è¯‘åŠ é€Ÿ
@torch.jit.script
def efficient_computation(x: torch.Tensor) -> torch.Tensor:
    return torch.nn.functional.relu(x)

# é¢„è®¡ç®—å¸¸ç”¨å€¼
def __init__(self, args_m, metadata=None):
    super().__init__()
    # é¢„è®¡ç®—ä½ç½®ç¼–ç ç­‰
    self.register_buffer('pos_encoding', self._create_pos_encoding())
```

### 3. å¹¶è¡ŒåŒ–

```python
# åˆ©ç”¨å¤šGPUå¹¶è¡Œ
def forward(self, x):
    if self.training and torch.cuda.device_count() > 1:
        # æ¨¡å‹å¹¶è¡Œæˆ–æ•°æ®å¹¶è¡Œä¼˜åŒ–
        pass
    return x
```

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### âœ… æ¨èåšæ³•

1. **æ¥å£è§„èŒƒ**ï¼šä¸¥æ ¼éµå¾ª `forward(x, file_id, task_id, return_feature)` æ¥å£
2. **å‚æ•°å¤„ç†**ï¼šä½¿ç”¨ `getattr()` å®‰å…¨è®¿é—®é…ç½®å‚æ•°
3. **å½¢çŠ¶å¤„ç†**ï¼šå§‹ç»ˆéªŒè¯å’Œå¤„ç†è¾“å…¥å¼ é‡å½¢çŠ¶
4. **é”™è¯¯å¤„ç†**ï¼šå¯¹Noneå€¼å’Œè¾¹ç•Œæƒ…å†µè¿›è¡Œæ£€æŸ¥
5. **æµ‹è¯•ä»£ç **ï¼šåœ¨ `if __name__ == '__main__':` ä¸­æ·»åŠ å•å…ƒæµ‹è¯•
6. **æ–‡æ¡£æ³¨é‡Š**ï¼šä¸ºå…³é”®æ–¹æ³•æ·»åŠ æ¸…æ™°çš„docstring

### âŒ é¿å…äº‹é¡¹

1. **ç¡¬ç¼–ç å‚æ•°**ï¼šé¿å…åœ¨ä»£ç ä¸­ç¡¬ç¼–ç æ•°å€¼
2. **å½¢çŠ¶å‡è®¾**ï¼šä¸è¦å‡è®¾ç‰¹å®šçš„è¾“å…¥å¼ é‡å½¢çŠ¶
3. **ç›´æ¥å±æ€§è®¿é—®**ï¼šé¿å… `args_m.param` è€Œä¸æ£€æŸ¥å­˜åœ¨æ€§
4. **å†…å­˜æ³„æ¼**ï¼šåŠæ—¶é‡Šæ”¾å¤§å‹ä¸­é—´å˜é‡
5. **æ¥å£ä¸ä¸€è‡´**ï¼šä¸è¦æ”¹å˜æ ‡å‡†æ¥å£ç­¾å

---

ğŸ‰ **ç°åœ¨ä½ å·²ç»æŒæ¡äº†PHMåŸºç¡€æ¨¡å‹å¼€å‘çš„å…¨éƒ¨è¦ç‚¹ï¼**

ç»§ç»­é˜…è¯»ï¼š
- ğŸ“Š [QUICKSTART.md](QUICKSTART.md) - å¿«é€Ÿä¸Šæ‰‹æŒ‡å—
- ğŸ“ˆ [DATA_GUIDE.md](DATA_GUIDE.md) - æ•°æ®ç³»ç»Ÿè¯¦è§£  
- ğŸ¯ [TASK_GUIDE.md](TASK_GUIDE.md) - ä»»åŠ¡ç±»å‹è¯´æ˜